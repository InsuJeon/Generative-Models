{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cover](complements/cover.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from dataset import DatasetFromFolder\n",
    "from model import Generator, Discriminator\n",
    "\n",
    "import utils\n",
    "import argparse\n",
    "import os, itertools\n",
    "from logger import Logger\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Argument Parsing & Directories Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=1, beta1=0.5, beta2=0.999, crop_size=256, dataset='horse2zebra', decay_epoch=100, fliplr=True, input_size=256, lambdaA=10, lambdaB=10, lrD=0.0002, lrG=0.0002, ndf=64, ngf=32, num_epochs=30, num_resnet=6, resize_scale=286)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "#Data Set Parameter\n",
    "parser.add_argument('--dataset', required=False, default='horse2zebra', help='input dataset')\n",
    "parser.add_argument('--batch_size', type=int, default=1, help='train batch size')\n",
    "parser.add_argument('--input_size', type=int, default=256, help='input size')\n",
    "parser.add_argument('--resize_scale', type=int, default=286, help='resize scale (0 is false)')\n",
    "parser.add_argument('--crop_size', type=int, default=256, help='crop size (0 is false)')\n",
    "parser.add_argument('--fliplr', type=bool, default=True, help='random fliplr True of False')\n",
    "\n",
    "#Model Parameters \n",
    "parser.add_argument('--ngf', type=int, default=32) # number of generator filters\n",
    "parser.add_argument('--ndf', type=int, default=64) # number of discriminator filters\n",
    "parser.add_argument('--num_resnet', type=int, default=6, help='number of resnet blocks in generator')\n",
    "\n",
    "#Learning Parameters\n",
    "parser.add_argument('--num_epochs', type=int, default=30, help='number of train epochs')\n",
    "parser.add_argument('--decay_epoch', type=int, default=100, help='start decaying learning rate after this number')\n",
    "parser.add_argument('--lrG', type=float, default=0.0002, help='learning rate for generator, default=0.0002')\n",
    "parser.add_argument('--lrD', type=float, default=0.0002, help='learning rate for discriminator, default=0.0002')\n",
    "parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for Adam optimizer')\n",
    "parser.add_argument('--beta2', type=float, default=0.999, help='beta2 for Adam optimizer')\n",
    "parser.add_argument('--lambdaA', type=float, default=10, help='lambdaA for cycle loss')\n",
    "parser.add_argument('--lambdaB', type=float, default=10, help='lambdaB for cycle loss')\n",
    "params = parser.parse_args([])\n",
    "print(params)\n",
    "\n",
    "# Directories for loading data and saving results\n",
    "data_dir = 'data/' + params.dataset + '/'\n",
    "save_dir = params.dataset + '_results/'\n",
    "model_dir = params.dataset + '_model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(data_dir) : \n",
    "    os.makedirs(data_dir)\n",
    "    file = 'ZIP_FILE.zip'\n",
    "    url = 'https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/' + params.dataset + '.zip'\n",
    "    print(url)\n",
    "    !wget -N $url -O $file\n",
    "    !unzip $file -d 'data/'\n",
    "    !rm $file\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "\n",
    "# Set the logger\n",
    "D_A_log_dir = save_dir + 'D_A_logs'\n",
    "D_B_log_dir = save_dir + 'D_B_logs'\n",
    "if not os.path.exists(D_A_log_dir):\n",
    "    os.mkdir(D_A_log_dir)\n",
    "D_A_logger = Logger(D_A_log_dir)\n",
    "if not os.path.exists(D_B_log_dir):\n",
    "    os.mkdir(D_B_log_dir)\n",
    "D_B_logger = Logger(D_B_log_dir)\n",
    "\n",
    "G_A_log_dir = save_dir + 'G_A_logs'\n",
    "G_B_log_dir = save_dir + 'G_B_logs'\n",
    "if not os.path.exists(G_A_log_dir):\n",
    "    os.mkdir(G_A_log_dir)\n",
    "G_A_logger = Logger(G_A_log_dir)\n",
    "if not os.path.exists(G_B_log_dir):\n",
    "    os.mkdir(G_B_log_dir)\n",
    "G_B_logger = Logger(G_B_log_dir)\n",
    "\n",
    "cycle_A_log_dir = save_dir + 'cycle_A_logs'\n",
    "cycle_B_log_dir = save_dir + 'cycle_B_logs'\n",
    "if not os.path.exists(cycle_A_log_dir):\n",
    "    os.mkdir(cycle_A_log_dir)\n",
    "cycle_A_logger = Logger(cycle_A_log_dir)\n",
    "if not os.path.exists(cycle_B_log_dir):\n",
    "    os.mkdir(cycle_B_log_dir)\n",
    "cycle_B_logger = Logger(cycle_B_log_dir)\n",
    "\n",
    "img_log_dir = save_dir + 'img_logs'\n",
    "if not os.path.exists(img_log_dir):\n",
    "    os.mkdir(img_log_dir)\n",
    "img_logger = Logger(img_log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data pre-processing\n",
    "transform = transforms.Compose([transforms.Resize(params.input_size),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])\n",
    "\n",
    "# Train data\n",
    "train_data_A = DatasetFromFolder(data_dir, subfolder='trainA', transform=transform,\n",
    "                                 resize_scale=params.resize_scale, crop_size=params.crop_size, fliplr=params.fliplr)\n",
    "train_data_loader_A = torch.utils.data.DataLoader(dataset=train_data_A,\n",
    "                                                  batch_size=params.batch_size,\n",
    "                                                  shuffle=True)\n",
    "train_data_B = DatasetFromFolder(data_dir, subfolder='trainB', transform=transform,\n",
    "                                 resize_scale=params.resize_scale, crop_size=params.crop_size, fliplr=params.fliplr)\n",
    "train_data_loader_B = torch.utils.data.DataLoader(dataset=train_data_B,\n",
    "                                                  batch_size=params.batch_size,\n",
    "                                                  shuffle=True)\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# Test data\n",
    "test_data_A = DatasetFromFolder(data_dir, subfolder='testA', transform=transform)\n",
    "test_data_loader_A = torch.utils.data.DataLoader(dataset=test_data_A,\n",
    "                                                 batch_size=params.batch_size,\n",
    "                                                 shuffle=False)\n",
    "test_data_B = DatasetFromFolder(data_dir, subfolder='testB', transform=transform)\n",
    "test_data_loader_B = torch.utils.data.DataLoader(dataset=test_data_B,\n",
    "                                                 batch_size=params.batch_size,\n",
    "                                                 shuffle=False)\n",
    "\n",
    "# Get specific test images\n",
    "test_real_A_data = train_data_A.__getitem__(11).unsqueeze(0)  # Convert to 4d tensor (BxNxHxW)\n",
    "test_real_B_data = train_data_B.__getitem__(91).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models & Optimizers & Criterions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Models\n",
    "G_A = Generator(3, params.ngf, 3, params.num_resnet) # arguments : input_dim, num_filter, output_dim, num_resnet\n",
    "G_B = Generator(3, params.ngf, 3, params.num_resnet)\n",
    "D_A = Discriminator(3, params.ndf, 1)               # arguments : input_dim, num_filter, output_dim\n",
    "D_B = Discriminator(3, params.ndf, 1) \n",
    "\n",
    "G_A.normal_weight_init(mean=0.0, std=0.02)\n",
    "G_B.normal_weight_init(mean=0.0, std=0.02)\n",
    "D_A.normal_weight_init(mean=0.0, std=0.02)\n",
    "D_B.normal_weight_init(mean=0.0, std=0.02)\n",
    "\n",
    "G_A.cuda()\n",
    "G_B.cuda()\n",
    "D_A.cuda()\n",
    "D_B.cuda()\n",
    "\n",
    "# optimizers\n",
    "G_optimizer = torch.optim.Adam(itertools.chain(G_A.parameters(), G_B.parameters()), lr=params.lrG, betas=(params.beta1, params.beta2))\n",
    "D_A_optimizer = torch.optim.Adam(D_A.parameters(), lr=params.lrD, betas=(params.beta1, params.beta2))\n",
    "D_B_optimizer = torch.optim.Adam(D_B.parameters(), lr=params.lrD, betas=(params.beta1, params.beta2))\n",
    "\n",
    "# Loss function\n",
    "MSE_loss = torch.nn.MSELoss().cuda()\n",
    "L1_loss = torch.nn.L1Loss().cuda()\n",
    "\n",
    "# # Training GAN\n",
    "D_A_avg_losses = []\n",
    "D_B_avg_losses = []\n",
    "G_A_avg_losses = []\n",
    "G_B_avg_losses = []\n",
    "cycle_A_avg_losses = []\n",
    "cycle_B_avg_losses = []\n",
    "\n",
    "# Generated image pool\n",
    "num_pool = 50\n",
    "fake_A_pool = utils.ImagePool(num_pool)\n",
    "fake_B_pool = utils.ImagePool(num_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "<br><br>\n",
    "## Concepts\n",
    "![concept](complements/concept.jpg)\n",
    "<br>\n",
    "## GAN Losses\n",
    "![gan_loss](complements/gan_loss.JPG)\n",
    "<br>\n",
    "## Cycle Consistency Losses\n",
    "![cycle_consistency_loss](complements/cycle_consistency.JPG)\n",
    "<br>\n",
    "## Full Objectives = Gan Losses + Cycle Consistency Losses \n",
    "![full_objectives](complements/full_objectives.JPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Step [1/1067], D_A_loss: 0.5212, D_B_loss: 0.7074, G_A_loss: 1.2753, G_B_loss: 0.8926\n",
      "Epoch [1/30], Step [11/1067], D_A_loss: 0.3092, D_B_loss: 0.3224, G_A_loss: 0.3540, G_B_loss: 0.3521\n",
      "Epoch [1/30], Step [21/1067], D_A_loss: 0.2910, D_B_loss: 0.2677, G_A_loss: 0.3569, G_B_loss: 0.3252\n",
      "Epoch [1/30], Step [31/1067], D_A_loss: 0.2243, D_B_loss: 0.2649, G_A_loss: 0.2590, G_B_loss: 0.3433\n",
      "Epoch [1/30], Step [41/1067], D_A_loss: 0.3040, D_B_loss: 0.2783, G_A_loss: 0.5191, G_B_loss: 0.3537\n",
      "Epoch [1/30], Step [51/1067], D_A_loss: 0.2116, D_B_loss: 0.3354, G_A_loss: 0.3843, G_B_loss: 0.4440\n",
      "Epoch [1/30], Step [61/1067], D_A_loss: 0.2431, D_B_loss: 0.3156, G_A_loss: 0.2724, G_B_loss: 0.2262\n",
      "Epoch [1/30], Step [71/1067], D_A_loss: 0.1964, D_B_loss: 0.2071, G_A_loss: 0.2339, G_B_loss: 0.2805\n",
      "Epoch [1/30], Step [81/1067], D_A_loss: 0.2187, D_B_loss: 0.1790, G_A_loss: 0.4944, G_B_loss: 0.2333\n",
      "Epoch [1/30], Step [91/1067], D_A_loss: 0.4833, D_B_loss: 0.2413, G_A_loss: 0.4898, G_B_loss: 0.8418\n",
      "Epoch [1/30], Step [101/1067], D_A_loss: 0.2906, D_B_loss: 0.4131, G_A_loss: 0.5293, G_B_loss: 0.2745\n",
      "Epoch [1/30], Step [111/1067], D_A_loss: 0.2050, D_B_loss: 0.4927, G_A_loss: 0.4121, G_B_loss: 0.2680\n",
      "Epoch [1/30], Step [121/1067], D_A_loss: 0.2630, D_B_loss: 0.2253, G_A_loss: 0.2992, G_B_loss: 0.3844\n",
      "Epoch [1/30], Step [131/1067], D_A_loss: 0.2777, D_B_loss: 0.2679, G_A_loss: 0.2326, G_B_loss: 0.4144\n",
      "Epoch [1/30], Step [141/1067], D_A_loss: 0.2401, D_B_loss: 0.1620, G_A_loss: 0.3877, G_B_loss: 0.5178\n",
      "Epoch [1/30], Step [151/1067], D_A_loss: 0.2574, D_B_loss: 0.1745, G_A_loss: 0.3878, G_B_loss: 0.3998\n",
      "Epoch [1/30], Step [161/1067], D_A_loss: 0.1980, D_B_loss: 0.1912, G_A_loss: 0.5791, G_B_loss: 0.1500\n",
      "Epoch [1/30], Step [171/1067], D_A_loss: 0.2991, D_B_loss: 0.2290, G_A_loss: 0.2207, G_B_loss: 0.3724\n",
      "Epoch [1/30], Step [181/1067], D_A_loss: 0.2186, D_B_loss: 0.1933, G_A_loss: 0.5954, G_B_loss: 0.5063\n",
      "Epoch [1/30], Step [191/1067], D_A_loss: 0.2063, D_B_loss: 0.1760, G_A_loss: 0.4230, G_B_loss: 0.3036\n",
      "Epoch [1/30], Step [201/1067], D_A_loss: 0.1744, D_B_loss: 0.1846, G_A_loss: 0.1366, G_B_loss: 0.3734\n",
      "Epoch [1/30], Step [211/1067], D_A_loss: 0.1598, D_B_loss: 0.2716, G_A_loss: 0.2073, G_B_loss: 0.2103\n",
      "Epoch [1/30], Step [221/1067], D_A_loss: 0.2183, D_B_loss: 0.2382, G_A_loss: 0.2757, G_B_loss: 0.2891\n",
      "Epoch [1/30], Step [231/1067], D_A_loss: 0.2564, D_B_loss: 0.2690, G_A_loss: 0.4523, G_B_loss: 0.4811\n",
      "Epoch [1/30], Step [241/1067], D_A_loss: 0.1532, D_B_loss: 0.1532, G_A_loss: 0.6126, G_B_loss: 0.3168\n",
      "Epoch [1/30], Step [251/1067], D_A_loss: 0.2591, D_B_loss: 0.2232, G_A_loss: 0.4836, G_B_loss: 0.3167\n",
      "Epoch [1/30], Step [261/1067], D_A_loss: 0.1574, D_B_loss: 0.0990, G_A_loss: 0.3268, G_B_loss: 0.2616\n",
      "Epoch [1/30], Step [271/1067], D_A_loss: 0.1937, D_B_loss: 0.1463, G_A_loss: 0.2655, G_B_loss: 0.2245\n",
      "Epoch [1/30], Step [281/1067], D_A_loss: 0.3224, D_B_loss: 0.2518, G_A_loss: 0.2139, G_B_loss: 0.9424\n",
      "Epoch [1/30], Step [291/1067], D_A_loss: 0.1931, D_B_loss: 0.2478, G_A_loss: 0.5445, G_B_loss: 0.2011\n",
      "Epoch [1/30], Step [301/1067], D_A_loss: 0.3707, D_B_loss: 0.2416, G_A_loss: 0.2862, G_B_loss: 0.2367\n",
      "Epoch [1/30], Step [311/1067], D_A_loss: 0.1529, D_B_loss: 0.2615, G_A_loss: 0.2374, G_B_loss: 0.3300\n",
      "Epoch [1/30], Step [321/1067], D_A_loss: 0.3128, D_B_loss: 0.1370, G_A_loss: 0.3901, G_B_loss: 0.5463\n",
      "Epoch [1/30], Step [331/1067], D_A_loss: 0.3323, D_B_loss: 0.3169, G_A_loss: 0.0963, G_B_loss: 0.3166\n",
      "Epoch [1/30], Step [341/1067], D_A_loss: 0.2732, D_B_loss: 0.2210, G_A_loss: 0.4658, G_B_loss: 0.3758\n",
      "Epoch [1/30], Step [351/1067], D_A_loss: 0.1592, D_B_loss: 0.1089, G_A_loss: 0.4865, G_B_loss: 0.3258\n",
      "Epoch [1/30], Step [361/1067], D_A_loss: 0.2880, D_B_loss: 0.1794, G_A_loss: 0.4158, G_B_loss: 0.4074\n",
      "Epoch [1/30], Step [371/1067], D_A_loss: 0.2336, D_B_loss: 0.1896, G_A_loss: 0.2872, G_B_loss: 0.3427\n",
      "Epoch [2/30], Step [131/1067], D_A_loss: 0.0653, D_B_loss: 0.1989, G_A_loss: 0.6180, G_B_loss: 0.1185\n",
      "Epoch [2/30], Step [141/1067], D_A_loss: 0.0738, D_B_loss: 0.0630, G_A_loss: 0.2175, G_B_loss: 0.1890\n",
      "Epoch [2/30], Step [151/1067], D_A_loss: 0.2421, D_B_loss: 0.3052, G_A_loss: 0.0865, G_B_loss: 0.1304\n",
      "Epoch [2/30], Step [161/1067], D_A_loss: 0.2243, D_B_loss: 0.0691, G_A_loss: 0.4703, G_B_loss: 0.2846\n",
      "Epoch [2/30], Step [171/1067], D_A_loss: 0.0319, D_B_loss: 0.1188, G_A_loss: 0.4679, G_B_loss: 0.3744\n",
      "Epoch [2/30], Step [181/1067], D_A_loss: 0.0437, D_B_loss: 0.2141, G_A_loss: 0.3206, G_B_loss: 0.0924\n",
      "Epoch [2/30], Step [191/1067], D_A_loss: 0.0475, D_B_loss: 0.3019, G_A_loss: 0.2765, G_B_loss: 0.6951\n",
      "Epoch [2/30], Step [201/1067], D_A_loss: 0.1846, D_B_loss: 0.2740, G_A_loss: 0.1076, G_B_loss: 0.5943\n",
      "Epoch [2/30], Step [211/1067], D_A_loss: 0.1158, D_B_loss: 0.2341, G_A_loss: 0.3314, G_B_loss: 0.2570\n",
      "Epoch [2/30], Step [221/1067], D_A_loss: 0.2549, D_B_loss: 0.1243, G_A_loss: 0.3926, G_B_loss: 0.1904\n",
      "Epoch [2/30], Step [231/1067], D_A_loss: 0.2436, D_B_loss: 0.1385, G_A_loss: 0.5219, G_B_loss: 0.6865\n",
      "Epoch [2/30], Step [241/1067], D_A_loss: 0.0905, D_B_loss: 0.0290, G_A_loss: 0.2744, G_B_loss: 0.2399\n",
      "Epoch [2/30], Step [251/1067], D_A_loss: 0.0492, D_B_loss: 0.0237, G_A_loss: 0.2431, G_B_loss: 0.1454\n",
      "Epoch [2/30], Step [261/1067], D_A_loss: 0.1791, D_B_loss: 0.4288, G_A_loss: 1.2292, G_B_loss: 0.4835\n",
      "Epoch [2/30], Step [271/1067], D_A_loss: 0.4143, D_B_loss: 0.2736, G_A_loss: 0.1256, G_B_loss: 1.2083\n",
      "Epoch [2/30], Step [281/1067], D_A_loss: 0.2452, D_B_loss: 0.0606, G_A_loss: 0.8218, G_B_loss: 0.5368\n",
      "Epoch [2/30], Step [291/1067], D_A_loss: 0.2398, D_B_loss: 0.2059, G_A_loss: 0.2623, G_B_loss: 0.1664\n",
      "Epoch [2/30], Step [301/1067], D_A_loss: 0.0832, D_B_loss: 0.1024, G_A_loss: 0.6494, G_B_loss: 0.3720\n",
      "Epoch [2/30], Step [311/1067], D_A_loss: 0.4913, D_B_loss: 0.0729, G_A_loss: 0.4688, G_B_loss: 1.1279\n",
      "Epoch [2/30], Step [321/1067], D_A_loss: 0.0773, D_B_loss: 0.1047, G_A_loss: 0.5724, G_B_loss: 0.9972\n",
      "Epoch [2/30], Step [331/1067], D_A_loss: 0.0440, D_B_loss: 0.1006, G_A_loss: 0.4590, G_B_loss: 0.5082\n",
      "Epoch [2/30], Step [341/1067], D_A_loss: 0.3449, D_B_loss: 0.1424, G_A_loss: 0.2755, G_B_loss: 0.1753\n",
      "Epoch [2/30], Step [351/1067], D_A_loss: 0.1295, D_B_loss: 0.0712, G_A_loss: 0.1680, G_B_loss: 0.4864\n",
      "Epoch [2/30], Step [361/1067], D_A_loss: 0.1245, D_B_loss: 0.3143, G_A_loss: 0.6732, G_B_loss: 0.4880\n",
      "Epoch [2/30], Step [371/1067], D_A_loss: 0.2685, D_B_loss: 0.1097, G_A_loss: 0.1905, G_B_loss: 0.1386\n",
      "Epoch [2/30], Step [381/1067], D_A_loss: 0.0665, D_B_loss: 0.1255, G_A_loss: 0.6340, G_B_loss: 0.5631\n",
      "Epoch [2/30], Step [391/1067], D_A_loss: 0.0774, D_B_loss: 0.0629, G_A_loss: 0.3239, G_B_loss: 0.3453\n",
      "Epoch [2/30], Step [401/1067], D_A_loss: 0.0766, D_B_loss: 0.0442, G_A_loss: 0.7873, G_B_loss: 0.6501\n",
      "Epoch [2/30], Step [411/1067], D_A_loss: 0.1330, D_B_loss: 0.3099, G_A_loss: 0.1297, G_B_loss: 0.2709\n",
      "Epoch [2/30], Step [421/1067], D_A_loss: 0.2484, D_B_loss: 0.1433, G_A_loss: 0.7607, G_B_loss: 0.1341\n",
      "Epoch [2/30], Step [431/1067], D_A_loss: 0.0453, D_B_loss: 0.2216, G_A_loss: 0.2807, G_B_loss: 0.3786\n",
      "Epoch [2/30], Step [441/1067], D_A_loss: 0.1431, D_B_loss: 0.0804, G_A_loss: 0.4875, G_B_loss: 0.3884\n",
      "Epoch [2/30], Step [451/1067], D_A_loss: 0.2398, D_B_loss: 0.1797, G_A_loss: 0.5739, G_B_loss: 0.5294\n",
      "Epoch [2/30], Step [461/1067], D_A_loss: 0.0306, D_B_loss: 0.2012, G_A_loss: 0.1596, G_B_loss: 0.5517\n",
      "Epoch [2/30], Step [471/1067], D_A_loss: 0.1546, D_B_loss: 0.0510, G_A_loss: 0.5183, G_B_loss: 0.2260\n",
      "Epoch [2/30], Step [481/1067], D_A_loss: 0.0461, D_B_loss: 0.1677, G_A_loss: 0.2459, G_B_loss: 0.2380\n",
      "Epoch [2/30], Step [491/1067], D_A_loss: 0.1724, D_B_loss: 0.2014, G_A_loss: 0.1943, G_B_loss: 0.3600\n",
      "Epoch [2/30], Step [501/1067], D_A_loss: 0.3163, D_B_loss: 0.2697, G_A_loss: 0.1688, G_B_loss: 0.7500\n",
      "Epoch [2/30], Step [511/1067], D_A_loss: 0.4109, D_B_loss: 0.0539, G_A_loss: 0.2135, G_B_loss: 0.0582\n",
      "Epoch [2/30], Step [521/1067], D_A_loss: 0.2608, D_B_loss: 0.1599, G_A_loss: 0.2394, G_B_loss: 0.3701\n",
      "Epoch [2/30], Step [531/1067], D_A_loss: 0.1184, D_B_loss: 0.2005, G_A_loss: 0.5271, G_B_loss: 0.6620\n",
      "Epoch [2/30], Step [541/1067], D_A_loss: 0.0975, D_B_loss: 0.1541, G_A_loss: 0.2630, G_B_loss: 0.7020\n",
      "Epoch [2/30], Step [551/1067], D_A_loss: 0.2408, D_B_loss: 0.1699, G_A_loss: 0.5369, G_B_loss: 0.1791\n",
      "Epoch [2/30], Step [561/1067], D_A_loss: 0.1442, D_B_loss: 0.0934, G_A_loss: 0.3942, G_B_loss: 0.2796\n",
      "Epoch [2/30], Step [571/1067], D_A_loss: 0.0904, D_B_loss: 0.0804, G_A_loss: 0.7529, G_B_loss: 0.3824\n",
      "Epoch [2/30], Step [581/1067], D_A_loss: 0.0692, D_B_loss: 0.1605, G_A_loss: 0.5077, G_B_loss: 0.2944\n",
      "Epoch [2/30], Step [591/1067], D_A_loss: 0.0263, D_B_loss: 0.4511, G_A_loss: 0.1755, G_B_loss: 0.1682\n",
      "Epoch [2/30], Step [601/1067], D_A_loss: 0.0666, D_B_loss: 0.0520, G_A_loss: 0.2187, G_B_loss: 0.1842\n",
      "Epoch [2/30], Step [611/1067], D_A_loss: 0.2046, D_B_loss: 0.0780, G_A_loss: 0.0589, G_B_loss: 0.6288\n",
      "Epoch [2/30], Step [621/1067], D_A_loss: 0.0785, D_B_loss: 0.0777, G_A_loss: 0.2306, G_B_loss: 0.2680\n",
      "Epoch [2/30], Step [631/1067], D_A_loss: 0.1663, D_B_loss: 0.0709, G_A_loss: 0.7834, G_B_loss: 0.2133\n",
      "Epoch [2/30], Step [641/1067], D_A_loss: 0.0549, D_B_loss: 0.2829, G_A_loss: 0.6295, G_B_loss: 0.1982\n",
      "Epoch [2/30], Step [651/1067], D_A_loss: 0.0488, D_B_loss: 0.1122, G_A_loss: 0.5636, G_B_loss: 0.5955\n",
      "Epoch [2/30], Step [661/1067], D_A_loss: 0.0435, D_B_loss: 0.0856, G_A_loss: 0.4546, G_B_loss: 0.4982\n",
      "Epoch [2/30], Step [671/1067], D_A_loss: 0.0452, D_B_loss: 0.2253, G_A_loss: 0.8815, G_B_loss: 0.1760\n",
      "Epoch [2/30], Step [681/1067], D_A_loss: 0.3054, D_B_loss: 0.0753, G_A_loss: 0.4343, G_B_loss: 0.0678\n",
      "Epoch [2/30], Step [691/1067], D_A_loss: 0.0809, D_B_loss: 0.1104, G_A_loss: 0.8433, G_B_loss: 0.6476\n",
      "Epoch [2/30], Step [701/1067], D_A_loss: 0.4997, D_B_loss: 0.1717, G_A_loss: 0.2233, G_B_loss: 0.0852\n",
      "Epoch [2/30], Step [711/1067], D_A_loss: 0.1977, D_B_loss: 0.1825, G_A_loss: 0.6166, G_B_loss: 0.1272\n",
      "Epoch [2/30], Step [721/1067], D_A_loss: 0.3337, D_B_loss: 0.2563, G_A_loss: 0.6553, G_B_loss: 0.0733\n",
      "Epoch [2/30], Step [731/1067], D_A_loss: 0.4034, D_B_loss: 0.1168, G_A_loss: 0.8568, G_B_loss: 0.0621\n",
      "Epoch [2/30], Step [741/1067], D_A_loss: 0.2173, D_B_loss: 0.1768, G_A_loss: 0.2798, G_B_loss: 0.2542\n",
      "Epoch [2/30], Step [751/1067], D_A_loss: 0.1324, D_B_loss: 0.1871, G_A_loss: 0.2653, G_B_loss: 0.6288\n",
      "Epoch [2/30], Step [761/1067], D_A_loss: 0.1783, D_B_loss: 0.0841, G_A_loss: 0.4143, G_B_loss: 0.2825\n",
      "Epoch [2/30], Step [771/1067], D_A_loss: 0.1688, D_B_loss: 0.1636, G_A_loss: 0.2335, G_B_loss: 0.4223\n",
      "Epoch [2/30], Step [781/1067], D_A_loss: 0.1542, D_B_loss: 0.2177, G_A_loss: 0.9820, G_B_loss: 0.3156\n",
      "Epoch [2/30], Step [791/1067], D_A_loss: 0.1959, D_B_loss: 0.0293, G_A_loss: 0.1759, G_B_loss: 0.2315\n",
      "Epoch [2/30], Step [801/1067], D_A_loss: 0.5167, D_B_loss: 0.2024, G_A_loss: 0.2841, G_B_loss: 0.0732\n",
      "Epoch [2/30], Step [811/1067], D_A_loss: 0.3137, D_B_loss: 0.0872, G_A_loss: 0.6422, G_B_loss: 0.5216\n",
      "Epoch [2/30], Step [821/1067], D_A_loss: 0.1872, D_B_loss: 0.0535, G_A_loss: 0.1457, G_B_loss: 0.2133\n",
      "Epoch [2/30], Step [831/1067], D_A_loss: 0.1388, D_B_loss: 0.1207, G_A_loss: 0.3986, G_B_loss: 0.5108\n",
      "Epoch [2/30], Step [841/1067], D_A_loss: 0.1497, D_B_loss: 0.1085, G_A_loss: 0.4961, G_B_loss: 0.3236\n",
      "Epoch [2/30], Step [851/1067], D_A_loss: 0.0908, D_B_loss: 0.0518, G_A_loss: 0.3803, G_B_loss: 0.2950\n",
      "Epoch [2/30], Step [861/1067], D_A_loss: 0.0517, D_B_loss: 0.1373, G_A_loss: 0.4134, G_B_loss: 0.2388\n",
      "Epoch [2/30], Step [871/1067], D_A_loss: 0.0953, D_B_loss: 0.0522, G_A_loss: 0.5214, G_B_loss: 0.6448\n",
      "Epoch [2/30], Step [881/1067], D_A_loss: 0.0969, D_B_loss: 0.0760, G_A_loss: 0.6563, G_B_loss: 0.2814\n",
      "Epoch [2/30], Step [891/1067], D_A_loss: 0.3346, D_B_loss: 0.1181, G_A_loss: 0.1115, G_B_loss: 0.2516\n",
      "Epoch [2/30], Step [901/1067], D_A_loss: 0.1493, D_B_loss: 0.2159, G_A_loss: 0.2508, G_B_loss: 0.2006\n",
      "Epoch [2/30], Step [911/1067], D_A_loss: 0.0802, D_B_loss: 0.1036, G_A_loss: 0.8081, G_B_loss: 0.5010\n",
      "Epoch [2/30], Step [921/1067], D_A_loss: 0.1737, D_B_loss: 0.0329, G_A_loss: 0.7888, G_B_loss: 0.5161\n",
      "Epoch [2/30], Step [931/1067], D_A_loss: 0.2415, D_B_loss: 0.1833, G_A_loss: 0.3697, G_B_loss: 0.1830\n",
      "Epoch [2/30], Step [941/1067], D_A_loss: 0.1754, D_B_loss: 0.4811, G_A_loss: 0.3091, G_B_loss: 0.6261\n",
      "Epoch [2/30], Step [951/1067], D_A_loss: 0.1282, D_B_loss: 0.0824, G_A_loss: 1.0791, G_B_loss: 0.5632\n",
      "Epoch [2/30], Step [961/1067], D_A_loss: 0.4357, D_B_loss: 0.1545, G_A_loss: 0.6217, G_B_loss: 1.1950\n",
      "Epoch [2/30], Step [971/1067], D_A_loss: 0.1948, D_B_loss: 0.3667, G_A_loss: 0.5562, G_B_loss: 0.3129\n",
      "Epoch [2/30], Step [981/1067], D_A_loss: 0.2253, D_B_loss: 0.0185, G_A_loss: 0.1151, G_B_loss: 0.1421\n",
      "Epoch [2/30], Step [991/1067], D_A_loss: 0.2101, D_B_loss: 0.0716, G_A_loss: 0.5544, G_B_loss: 0.4881\n",
      "Epoch [2/30], Step [1001/1067], D_A_loss: 0.0472, D_B_loss: 0.1598, G_A_loss: 0.2341, G_B_loss: 0.2524\n",
      "Epoch [2/30], Step [1011/1067], D_A_loss: 0.2419, D_B_loss: 0.0683, G_A_loss: 0.5667, G_B_loss: 0.3538\n",
      "Epoch [2/30], Step [1021/1067], D_A_loss: 0.1545, D_B_loss: 0.0521, G_A_loss: 0.1635, G_B_loss: 0.4247\n",
      "Epoch [2/30], Step [1031/1067], D_A_loss: 0.0594, D_B_loss: 0.2194, G_A_loss: 0.4481, G_B_loss: 0.1391\n",
      "Epoch [2/30], Step [1041/1067], D_A_loss: 0.0769, D_B_loss: 0.2637, G_A_loss: 0.2796, G_B_loss: 0.2175\n",
      "Epoch [2/30], Step [1051/1067], D_A_loss: 0.1961, D_B_loss: 0.2889, G_A_loss: 0.1542, G_B_loss: 0.2412\n",
      "Epoch [2/30], Step [1061/1067], D_A_loss: 0.0794, D_B_loss: 0.0810, G_A_loss: 0.3931, G_B_loss: 0.4973\n",
      "Epoch [3/30], Step [1/1067], D_A_loss: 0.1199, D_B_loss: 0.1887, G_A_loss: 1.1122, G_B_loss: 0.3682\n",
      "Epoch [3/30], Step [11/1067], D_A_loss: 0.2228, D_B_loss: 0.1340, G_A_loss: 0.6483, G_B_loss: 0.1448\n",
      "Epoch [3/30], Step [21/1067], D_A_loss: 0.1610, D_B_loss: 0.1814, G_A_loss: 0.2157, G_B_loss: 0.2246\n",
      "Epoch [3/30], Step [31/1067], D_A_loss: 0.1614, D_B_loss: 0.0534, G_A_loss: 0.6966, G_B_loss: 0.4838\n",
      "Epoch [3/30], Step [41/1067], D_A_loss: 0.0892, D_B_loss: 0.1110, G_A_loss: 0.3583, G_B_loss: 0.5399\n",
      "Epoch [3/30], Step [51/1067], D_A_loss: 0.1034, D_B_loss: 0.0966, G_A_loss: 0.2363, G_B_loss: 0.5019\n",
      "Epoch [3/30], Step [61/1067], D_A_loss: 0.0699, D_B_loss: 0.0748, G_A_loss: 0.5112, G_B_loss: 0.5118\n",
      "Epoch [3/30], Step [71/1067], D_A_loss: 0.1094, D_B_loss: 0.2092, G_A_loss: 0.2254, G_B_loss: 0.7222\n",
      "Epoch [3/30], Step [81/1067], D_A_loss: 0.0528, D_B_loss: 0.2968, G_A_loss: 0.8880, G_B_loss: 0.4585\n",
      "Epoch [3/30], Step [91/1067], D_A_loss: 0.1653, D_B_loss: 0.2622, G_A_loss: 0.4296, G_B_loss: 0.2735\n",
      "Epoch [3/30], Step [101/1067], D_A_loss: 0.3495, D_B_loss: 0.1848, G_A_loss: 0.7500, G_B_loss: 0.7203\n",
      "Epoch [3/30], Step [111/1067], D_A_loss: 0.0793, D_B_loss: 0.1686, G_A_loss: 0.2595, G_B_loss: 0.6819\n",
      "Epoch [3/30], Step [121/1067], D_A_loss: 0.3161, D_B_loss: 0.0701, G_A_loss: 0.5162, G_B_loss: 0.3811\n",
      "Epoch [3/30], Step [131/1067], D_A_loss: 0.5505, D_B_loss: 0.0588, G_A_loss: 0.0451, G_B_loss: 1.3686\n",
      "Epoch [3/30], Step [141/1067], D_A_loss: 0.2211, D_B_loss: 0.1219, G_A_loss: 0.3420, G_B_loss: 0.5236\n",
      "Epoch [3/30], Step [151/1067], D_A_loss: 0.1540, D_B_loss: 0.1998, G_A_loss: 0.5126, G_B_loss: 0.4494\n",
      "Epoch [3/30], Step [161/1067], D_A_loss: 0.2510, D_B_loss: 0.0271, G_A_loss: 0.4826, G_B_loss: 0.4207\n",
      "Epoch [3/30], Step [171/1067], D_A_loss: 0.3697, D_B_loss: 0.0867, G_A_loss: 0.2017, G_B_loss: 0.0879\n",
      "Epoch [3/30], Step [181/1067], D_A_loss: 0.0761, D_B_loss: 0.2273, G_A_loss: 0.1748, G_B_loss: 0.3365\n",
      "Epoch [3/30], Step [191/1067], D_A_loss: 0.3891, D_B_loss: 0.1157, G_A_loss: 0.3200, G_B_loss: 0.9835\n",
      "Epoch [3/30], Step [201/1067], D_A_loss: 0.1660, D_B_loss: 0.0500, G_A_loss: 0.6580, G_B_loss: 0.3547\n",
      "Epoch [3/30], Step [211/1067], D_A_loss: 0.0505, D_B_loss: 0.2353, G_A_loss: 0.1556, G_B_loss: 0.5235\n",
      "Epoch [3/30], Step [221/1067], D_A_loss: 0.1117, D_B_loss: 0.2002, G_A_loss: 0.7954, G_B_loss: 0.4860\n",
      "Epoch [3/30], Step [231/1067], D_A_loss: 0.0862, D_B_loss: 0.0709, G_A_loss: 0.5120, G_B_loss: 0.4033\n",
      "Epoch [3/30], Step [241/1067], D_A_loss: 0.1156, D_B_loss: 0.0403, G_A_loss: 0.6289, G_B_loss: 0.4305\n",
      "Epoch [3/30], Step [251/1067], D_A_loss: 0.3287, D_B_loss: 0.3012, G_A_loss: 0.5955, G_B_loss: 0.0596\n",
      "Epoch [3/30], Step [261/1067], D_A_loss: 0.2642, D_B_loss: 0.0652, G_A_loss: 0.4898, G_B_loss: 1.2943\n",
      "Epoch [3/30], Step [271/1067], D_A_loss: 0.1782, D_B_loss: 0.1658, G_A_loss: 0.3223, G_B_loss: 0.7537\n",
      "Epoch [3/30], Step [281/1067], D_A_loss: 0.0940, D_B_loss: 0.1511, G_A_loss: 0.3585, G_B_loss: 0.6083\n",
      "Epoch [3/30], Step [291/1067], D_A_loss: 0.0230, D_B_loss: 0.0798, G_A_loss: 0.1795, G_B_loss: 0.2344\n",
      "Epoch [3/30], Step [301/1067], D_A_loss: 0.2155, D_B_loss: 0.1969, G_A_loss: 0.1988, G_B_loss: 0.3425\n",
      "Epoch [3/30], Step [311/1067], D_A_loss: 0.0368, D_B_loss: 0.1010, G_A_loss: 0.3800, G_B_loss: 0.6829\n",
      "Epoch [3/30], Step [321/1067], D_A_loss: 0.1982, D_B_loss: 0.0996, G_A_loss: 0.0371, G_B_loss: 0.4600\n",
      "Epoch [3/30], Step [331/1067], D_A_loss: 0.0792, D_B_loss: 0.0534, G_A_loss: 0.6380, G_B_loss: 0.8223\n",
      "Epoch [3/30], Step [341/1067], D_A_loss: 0.1129, D_B_loss: 0.1033, G_A_loss: 0.5651, G_B_loss: 0.7017\n",
      "Epoch [3/30], Step [351/1067], D_A_loss: 0.0401, D_B_loss: 0.0619, G_A_loss: 1.3517, G_B_loss: 0.3852\n",
      "Epoch [3/30], Step [361/1067], D_A_loss: 0.1507, D_B_loss: 0.1572, G_A_loss: 0.3191, G_B_loss: 0.5134\n",
      "Epoch [3/30], Step [371/1067], D_A_loss: 0.1532, D_B_loss: 0.2034, G_A_loss: 0.2511, G_B_loss: 0.1370\n",
      "Epoch [3/30], Step [381/1067], D_A_loss: 0.0706, D_B_loss: 0.1482, G_A_loss: 0.3484, G_B_loss: 0.4665\n",
      "Epoch [3/30], Step [391/1067], D_A_loss: 0.1018, D_B_loss: 0.0406, G_A_loss: 0.5452, G_B_loss: 0.4156\n",
      "Epoch [3/30], Step [401/1067], D_A_loss: 0.1950, D_B_loss: 0.4760, G_A_loss: 0.9449, G_B_loss: 0.1816\n",
      "Epoch [3/30], Step [411/1067], D_A_loss: 0.0368, D_B_loss: 0.1317, G_A_loss: 0.5716, G_B_loss: 0.3551\n",
      "Epoch [3/30], Step [421/1067], D_A_loss: 0.1132, D_B_loss: 0.1386, G_A_loss: 0.6516, G_B_loss: 0.4966\n",
      "Epoch [3/30], Step [431/1067], D_A_loss: 0.2501, D_B_loss: 0.1914, G_A_loss: 0.6446, G_B_loss: 0.4932\n",
      "Epoch [3/30], Step [441/1067], D_A_loss: 0.1356, D_B_loss: 0.0246, G_A_loss: 0.3183, G_B_loss: 0.2777\n",
      "Epoch [3/30], Step [451/1067], D_A_loss: 0.0812, D_B_loss: 0.1185, G_A_loss: 0.4541, G_B_loss: 0.2988\n",
      "Epoch [3/30], Step [461/1067], D_A_loss: 0.0879, D_B_loss: 0.0703, G_A_loss: 0.5670, G_B_loss: 0.5312\n",
      "Epoch [3/30], Step [471/1067], D_A_loss: 0.2311, D_B_loss: 0.0943, G_A_loss: 0.3759, G_B_loss: 0.6431\n",
      "Epoch [3/30], Step [481/1067], D_A_loss: 0.1025, D_B_loss: 0.1460, G_A_loss: 0.3544, G_B_loss: 0.2879\n",
      "Epoch [3/30], Step [491/1067], D_A_loss: 0.1026, D_B_loss: 0.1234, G_A_loss: 0.2904, G_B_loss: 0.6174\n",
      "Epoch [3/30], Step [501/1067], D_A_loss: 0.3021, D_B_loss: 0.1125, G_A_loss: 0.4016, G_B_loss: 0.5101\n",
      "Epoch [3/30], Step [511/1067], D_A_loss: 0.1307, D_B_loss: 0.0896, G_A_loss: 0.3600, G_B_loss: 0.3588\n",
      "Epoch [3/30], Step [521/1067], D_A_loss: 0.2313, D_B_loss: 0.0329, G_A_loss: 0.3014, G_B_loss: 0.4939\n",
      "Epoch [3/30], Step [531/1067], D_A_loss: 0.1095, D_B_loss: 0.0871, G_A_loss: 0.5152, G_B_loss: 0.3286\n",
      "Epoch [3/30], Step [541/1067], D_A_loss: 0.1813, D_B_loss: 0.0145, G_A_loss: 0.4742, G_B_loss: 0.1410\n",
      "Epoch [3/30], Step [551/1067], D_A_loss: 0.1187, D_B_loss: 0.0742, G_A_loss: 0.6379, G_B_loss: 0.0550\n",
      "Epoch [3/30], Step [561/1067], D_A_loss: 0.0478, D_B_loss: 0.0436, G_A_loss: 0.2186, G_B_loss: 0.7434\n",
      "Epoch [3/30], Step [571/1067], D_A_loss: 0.1938, D_B_loss: 0.0475, G_A_loss: 0.3927, G_B_loss: 0.6822\n",
      "Epoch [3/30], Step [581/1067], D_A_loss: 0.2399, D_B_loss: 0.0455, G_A_loss: 0.8484, G_B_loss: 0.2557\n",
      "Epoch [3/30], Step [591/1067], D_A_loss: 0.1057, D_B_loss: 0.1695, G_A_loss: 0.7409, G_B_loss: 0.1971\n",
      "Epoch [3/30], Step [601/1067], D_A_loss: 0.1080, D_B_loss: 0.0797, G_A_loss: 0.0589, G_B_loss: 0.3347\n",
      "Epoch [3/30], Step [611/1067], D_A_loss: 0.1343, D_B_loss: 0.0710, G_A_loss: 0.8552, G_B_loss: 0.5685\n",
      "Epoch [3/30], Step [621/1067], D_A_loss: 0.2166, D_B_loss: 0.1112, G_A_loss: 0.4491, G_B_loss: 0.1604\n",
      "Epoch [3/30], Step [631/1067], D_A_loss: 0.1955, D_B_loss: 0.1401, G_A_loss: 0.3051, G_B_loss: 0.2757\n",
      "Epoch [3/30], Step [641/1067], D_A_loss: 0.3188, D_B_loss: 0.1487, G_A_loss: 0.3527, G_B_loss: 0.7101\n",
      "Epoch [3/30], Step [651/1067], D_A_loss: 0.1237, D_B_loss: 0.1547, G_A_loss: 0.6424, G_B_loss: 0.1635\n",
      "Epoch [3/30], Step [661/1067], D_A_loss: 0.1692, D_B_loss: 0.1027, G_A_loss: 0.2982, G_B_loss: 0.4286\n",
      "Epoch [3/30], Step [671/1067], D_A_loss: 0.0641, D_B_loss: 0.1164, G_A_loss: 0.3325, G_B_loss: 0.2836\n",
      "Epoch [3/30], Step [681/1067], D_A_loss: 0.2552, D_B_loss: 0.2116, G_A_loss: 0.2157, G_B_loss: 1.0182\n",
      "Epoch [3/30], Step [691/1067], D_A_loss: 0.1770, D_B_loss: 0.0534, G_A_loss: 0.5744, G_B_loss: 0.2555\n",
      "Epoch [3/30], Step [701/1067], D_A_loss: 0.0661, D_B_loss: 0.1240, G_A_loss: 0.4118, G_B_loss: 0.4861\n",
      "Epoch [3/30], Step [711/1067], D_A_loss: 0.1305, D_B_loss: 0.2207, G_A_loss: 0.1666, G_B_loss: 0.4479\n",
      "Epoch [3/30], Step [721/1067], D_A_loss: 0.2251, D_B_loss: 0.2091, G_A_loss: 0.1834, G_B_loss: 0.2830\n",
      "Epoch [3/30], Step [731/1067], D_A_loss: 0.1965, D_B_loss: 0.0455, G_A_loss: 0.2569, G_B_loss: 0.4066\n",
      "Epoch [3/30], Step [741/1067], D_A_loss: 0.1910, D_B_loss: 0.1826, G_A_loss: 0.3140, G_B_loss: 0.5562\n",
      "Epoch [3/30], Step [751/1067], D_A_loss: 0.2143, D_B_loss: 0.0291, G_A_loss: 0.3237, G_B_loss: 0.6897\n",
      "Epoch [3/30], Step [761/1067], D_A_loss: 0.2742, D_B_loss: 0.2542, G_A_loss: 0.2832, G_B_loss: 0.5733\n",
      "Epoch [3/30], Step [771/1067], D_A_loss: 0.1352, D_B_loss: 0.1071, G_A_loss: 0.4082, G_B_loss: 0.3638\n",
      "Epoch [3/30], Step [781/1067], D_A_loss: 0.2812, D_B_loss: 0.2429, G_A_loss: 0.4039, G_B_loss: 0.5884\n",
      "Epoch [3/30], Step [791/1067], D_A_loss: 0.2278, D_B_loss: 0.2000, G_A_loss: 0.0627, G_B_loss: 0.1462\n",
      "Epoch [3/30], Step [801/1067], D_A_loss: 0.2036, D_B_loss: 0.1427, G_A_loss: 0.2515, G_B_loss: 0.5151\n",
      "Epoch [3/30], Step [811/1067], D_A_loss: 0.2137, D_B_loss: 0.1547, G_A_loss: 0.3435, G_B_loss: 0.2966\n",
      "Epoch [3/30], Step [821/1067], D_A_loss: 0.1554, D_B_loss: 0.1474, G_A_loss: 0.4700, G_B_loss: 0.6846\n",
      "Epoch [3/30], Step [831/1067], D_A_loss: 0.0937, D_B_loss: 0.1380, G_A_loss: 0.2822, G_B_loss: 0.1983\n",
      "Epoch [3/30], Step [841/1067], D_A_loss: 0.1587, D_B_loss: 0.1285, G_A_loss: 0.6663, G_B_loss: 0.2655\n",
      "Epoch [3/30], Step [851/1067], D_A_loss: 0.1107, D_B_loss: 0.1108, G_A_loss: 0.4994, G_B_loss: 0.7850\n",
      "Epoch [3/30], Step [861/1067], D_A_loss: 0.2461, D_B_loss: 0.0833, G_A_loss: 0.6593, G_B_loss: 0.3319\n",
      "Epoch [3/30], Step [871/1067], D_A_loss: 0.1804, D_B_loss: 0.0537, G_A_loss: 0.4489, G_B_loss: 0.3634\n",
      "Epoch [3/30], Step [881/1067], D_A_loss: 0.2242, D_B_loss: 0.1855, G_A_loss: 0.5726, G_B_loss: 0.1834\n",
      "Epoch [3/30], Step [891/1067], D_A_loss: 0.3079, D_B_loss: 0.0370, G_A_loss: 0.5680, G_B_loss: 0.1206\n",
      "Epoch [3/30], Step [901/1067], D_A_loss: 0.1336, D_B_loss: 0.1780, G_A_loss: 0.3019, G_B_loss: 0.5703\n",
      "Epoch [3/30], Step [911/1067], D_A_loss: 0.0912, D_B_loss: 0.0452, G_A_loss: 0.2344, G_B_loss: 0.1399\n",
      "Epoch [3/30], Step [921/1067], D_A_loss: 0.2037, D_B_loss: 0.1140, G_A_loss: 0.4473, G_B_loss: 0.4710\n",
      "Epoch [3/30], Step [931/1067], D_A_loss: 0.0282, D_B_loss: 0.0597, G_A_loss: 0.3789, G_B_loss: 0.8754\n",
      "Epoch [3/30], Step [941/1067], D_A_loss: 0.2267, D_B_loss: 0.1297, G_A_loss: 0.4624, G_B_loss: 0.1559\n",
      "Epoch [3/30], Step [951/1067], D_A_loss: 0.1547, D_B_loss: 0.2550, G_A_loss: 0.1724, G_B_loss: 0.6272\n",
      "Epoch [3/30], Step [961/1067], D_A_loss: 0.2145, D_B_loss: 0.2219, G_A_loss: 0.4549, G_B_loss: 0.1966\n",
      "Epoch [3/30], Step [971/1067], D_A_loss: 0.2320, D_B_loss: 0.0657, G_A_loss: 0.1705, G_B_loss: 0.2401\n",
      "Epoch [3/30], Step [981/1067], D_A_loss: 0.1976, D_B_loss: 0.1000, G_A_loss: 0.5434, G_B_loss: 0.9892\n",
      "Epoch [3/30], Step [991/1067], D_A_loss: 0.1413, D_B_loss: 0.1021, G_A_loss: 0.7703, G_B_loss: 0.6563\n",
      "Epoch [3/30], Step [1001/1067], D_A_loss: 0.2085, D_B_loss: 0.1749, G_A_loss: 1.0263, G_B_loss: 0.1913\n",
      "Epoch [3/30], Step [1011/1067], D_A_loss: 0.1710, D_B_loss: 0.1188, G_A_loss: 0.5374, G_B_loss: 0.3110\n",
      "Epoch [3/30], Step [1021/1067], D_A_loss: 0.1432, D_B_loss: 0.1283, G_A_loss: 0.6060, G_B_loss: 0.1965\n",
      "Epoch [3/30], Step [1031/1067], D_A_loss: 0.1015, D_B_loss: 0.0931, G_A_loss: 0.7117, G_B_loss: 0.4735\n",
      "Epoch [3/30], Step [1041/1067], D_A_loss: 0.0725, D_B_loss: 0.0528, G_A_loss: 0.7991, G_B_loss: 0.5664\n",
      "Epoch [3/30], Step [1051/1067], D_A_loss: 0.1320, D_B_loss: 0.2208, G_A_loss: 0.7292, G_B_loss: 0.2837\n",
      "Epoch [3/30], Step [1061/1067], D_A_loss: 0.1857, D_B_loss: 0.2885, G_A_loss: 0.5593, G_B_loss: 0.1344\n",
      "Epoch [4/30], Step [1/1067], D_A_loss: 0.3026, D_B_loss: 0.2719, G_A_loss: 0.2166, G_B_loss: 0.5667\n",
      "Epoch [4/30], Step [11/1067], D_A_loss: 0.1261, D_B_loss: 0.0551, G_A_loss: 0.7152, G_B_loss: 0.4043\n",
      "Epoch [4/30], Step [21/1067], D_A_loss: 0.0809, D_B_loss: 0.3801, G_A_loss: 0.8386, G_B_loss: 0.5158\n",
      "Epoch [4/30], Step [31/1067], D_A_loss: 0.1491, D_B_loss: 0.2018, G_A_loss: 0.9429, G_B_loss: 0.5072\n",
      "Epoch [4/30], Step [41/1067], D_A_loss: 0.1410, D_B_loss: 0.0570, G_A_loss: 0.3896, G_B_loss: 0.2641\n",
      "Epoch [4/30], Step [51/1067], D_A_loss: 0.1760, D_B_loss: 0.0728, G_A_loss: 0.4169, G_B_loss: 0.4963\n",
      "Epoch [4/30], Step [61/1067], D_A_loss: 0.2237, D_B_loss: 0.1254, G_A_loss: 0.2938, G_B_loss: 0.3879\n",
      "Epoch [4/30], Step [71/1067], D_A_loss: 0.2246, D_B_loss: 0.1334, G_A_loss: 0.2718, G_B_loss: 0.3804\n",
      "Epoch [4/30], Step [81/1067], D_A_loss: 0.1063, D_B_loss: 0.2893, G_A_loss: 0.6050, G_B_loss: 0.5411\n",
      "Epoch [4/30], Step [91/1067], D_A_loss: 0.3408, D_B_loss: 0.2770, G_A_loss: 0.1135, G_B_loss: 0.2533\n",
      "Epoch [4/30], Step [101/1067], D_A_loss: 0.1125, D_B_loss: 0.0998, G_A_loss: 0.8507, G_B_loss: 0.4375\n",
      "Epoch [4/30], Step [111/1067], D_A_loss: 0.0711, D_B_loss: 0.0986, G_A_loss: 0.4533, G_B_loss: 0.4988\n",
      "Epoch [4/30], Step [121/1067], D_A_loss: 0.1541, D_B_loss: 0.1412, G_A_loss: 0.3982, G_B_loss: 0.3316\n",
      "Epoch [4/30], Step [131/1067], D_A_loss: 0.0986, D_B_loss: 0.0674, G_A_loss: 0.4671, G_B_loss: 0.0581\n",
      "Epoch [4/30], Step [141/1067], D_A_loss: 0.1273, D_B_loss: 0.1049, G_A_loss: 0.3649, G_B_loss: 0.8047\n",
      "Epoch [4/30], Step [151/1067], D_A_loss: 0.2387, D_B_loss: 0.2663, G_A_loss: 0.7653, G_B_loss: 0.4257\n",
      "Epoch [4/30], Step [161/1067], D_A_loss: 0.2249, D_B_loss: 0.1207, G_A_loss: 0.4303, G_B_loss: 1.1079\n",
      "Epoch [4/30], Step [171/1067], D_A_loss: 0.0724, D_B_loss: 0.2087, G_A_loss: 0.3185, G_B_loss: 0.2855\n",
      "Epoch [4/30], Step [181/1067], D_A_loss: 0.2477, D_B_loss: 0.1605, G_A_loss: 0.3786, G_B_loss: 0.4333\n",
      "Epoch [4/30], Step [191/1067], D_A_loss: 0.2387, D_B_loss: 0.1211, G_A_loss: 0.5985, G_B_loss: 0.1615\n",
      "Epoch [4/30], Step [201/1067], D_A_loss: 0.0646, D_B_loss: 0.1064, G_A_loss: 0.4093, G_B_loss: 0.3255\n",
      "Epoch [4/30], Step [211/1067], D_A_loss: 0.1074, D_B_loss: 0.1351, G_A_loss: 0.4911, G_B_loss: 0.3181\n",
      "Epoch [4/30], Step [221/1067], D_A_loss: 0.1812, D_B_loss: 0.1136, G_A_loss: 0.6706, G_B_loss: 0.9688\n",
      "Epoch [4/30], Step [231/1067], D_A_loss: 0.2325, D_B_loss: 0.0958, G_A_loss: 0.3338, G_B_loss: 0.7208\n",
      "Epoch [4/30], Step [241/1067], D_A_loss: 0.1079, D_B_loss: 0.0288, G_A_loss: 0.3681, G_B_loss: 0.2844\n",
      "Epoch [4/30], Step [251/1067], D_A_loss: 0.1150, D_B_loss: 0.0789, G_A_loss: 0.4171, G_B_loss: 0.2938\n",
      "Epoch [4/30], Step [261/1067], D_A_loss: 0.0380, D_B_loss: 0.0536, G_A_loss: 0.6605, G_B_loss: 0.2863\n",
      "Epoch [4/30], Step [271/1067], D_A_loss: 0.1891, D_B_loss: 0.1852, G_A_loss: 0.1822, G_B_loss: 0.2079\n",
      "Epoch [4/30], Step [281/1067], D_A_loss: 0.0309, D_B_loss: 0.1417, G_A_loss: 0.2271, G_B_loss: 0.5073\n",
      "Epoch [4/30], Step [291/1067], D_A_loss: 0.3247, D_B_loss: 0.1320, G_A_loss: 0.6878, G_B_loss: 0.1753\n",
      "Epoch [4/30], Step [301/1067], D_A_loss: 0.1360, D_B_loss: 0.1323, G_A_loss: 0.3019, G_B_loss: 0.2783\n",
      "Epoch [4/30], Step [311/1067], D_A_loss: 0.0449, D_B_loss: 0.0495, G_A_loss: 0.3039, G_B_loss: 0.1681\n",
      "Epoch [4/30], Step [321/1067], D_A_loss: 0.0579, D_B_loss: 0.2013, G_A_loss: 0.2232, G_B_loss: 0.0897\n",
      "Epoch [4/30], Step [331/1067], D_A_loss: 0.1258, D_B_loss: 0.1600, G_A_loss: 0.5020, G_B_loss: 0.5346\n",
      "Epoch [4/30], Step [341/1067], D_A_loss: 0.0942, D_B_loss: 0.1909, G_A_loss: 0.6232, G_B_loss: 1.0555\n",
      "Epoch [4/30], Step [351/1067], D_A_loss: 0.0241, D_B_loss: 0.1737, G_A_loss: 0.5095, G_B_loss: 0.7904\n",
      "Epoch [4/30], Step [361/1067], D_A_loss: 0.0291, D_B_loss: 0.2188, G_A_loss: 0.0524, G_B_loss: 0.2461\n",
      "Epoch [4/30], Step [371/1067], D_A_loss: 0.0773, D_B_loss: 0.3152, G_A_loss: 0.3503, G_B_loss: 0.1468\n",
      "Epoch [4/30], Step [381/1067], D_A_loss: 0.7285, D_B_loss: 0.0679, G_A_loss: 0.4053, G_B_loss: 0.9202\n",
      "Epoch [4/30], Step [391/1067], D_A_loss: 0.1231, D_B_loss: 0.0450, G_A_loss: 0.6307, G_B_loss: 0.3360\n",
      "Epoch [4/30], Step [401/1067], D_A_loss: 0.2061, D_B_loss: 0.1738, G_A_loss: 0.5260, G_B_loss: 0.3069\n",
      "Epoch [4/30], Step [411/1067], D_A_loss: 0.3292, D_B_loss: 0.0984, G_A_loss: 0.6094, G_B_loss: 0.0919\n",
      "Epoch [4/30], Step [421/1067], D_A_loss: 0.0454, D_B_loss: 0.0301, G_A_loss: 0.7196, G_B_loss: 0.0882\n",
      "Epoch [4/30], Step [431/1067], D_A_loss: 0.1515, D_B_loss: 0.1159, G_A_loss: 0.3497, G_B_loss: 0.6928\n",
      "Epoch [4/30], Step [441/1067], D_A_loss: 0.1135, D_B_loss: 0.0605, G_A_loss: 0.4603, G_B_loss: 0.3078\n",
      "Epoch [4/30], Step [451/1067], D_A_loss: 0.0807, D_B_loss: 0.0428, G_A_loss: 0.6116, G_B_loss: 0.2963\n",
      "Epoch [4/30], Step [461/1067], D_A_loss: 0.1966, D_B_loss: 0.1977, G_A_loss: 0.2301, G_B_loss: 0.8820\n",
      "Epoch [4/30], Step [471/1067], D_A_loss: 0.1783, D_B_loss: 0.1694, G_A_loss: 0.9054, G_B_loss: 0.3467\n",
      "Epoch [4/30], Step [481/1067], D_A_loss: 0.1448, D_B_loss: 0.1363, G_A_loss: 0.6218, G_B_loss: 0.2848\n",
      "Epoch [4/30], Step [491/1067], D_A_loss: 0.0252, D_B_loss: 0.0681, G_A_loss: 0.5587, G_B_loss: 0.2021\n",
      "Epoch [4/30], Step [501/1067], D_A_loss: 0.1043, D_B_loss: 0.3196, G_A_loss: 0.1931, G_B_loss: 0.4005\n",
      "Epoch [4/30], Step [511/1067], D_A_loss: 0.1423, D_B_loss: 0.1210, G_A_loss: 0.7600, G_B_loss: 0.2929\n",
      "Epoch [4/30], Step [521/1067], D_A_loss: 0.1722, D_B_loss: 0.4185, G_A_loss: 0.6251, G_B_loss: 0.0973\n",
      "Epoch [4/30], Step [531/1067], D_A_loss: 0.0614, D_B_loss: 0.0808, G_A_loss: 0.6048, G_B_loss: 0.5936\n",
      "Epoch [4/30], Step [541/1067], D_A_loss: 0.0350, D_B_loss: 0.0752, G_A_loss: 0.1838, G_B_loss: 0.1281\n",
      "Epoch [4/30], Step [551/1067], D_A_loss: 0.1732, D_B_loss: 0.3766, G_A_loss: 0.2540, G_B_loss: 0.2833\n",
      "Epoch [4/30], Step [561/1067], D_A_loss: 0.3896, D_B_loss: 0.1811, G_A_loss: 0.2797, G_B_loss: 0.1563\n",
      "Epoch [4/30], Step [571/1067], D_A_loss: 0.0826, D_B_loss: 0.1086, G_A_loss: 0.3679, G_B_loss: 0.4154\n",
      "Epoch [4/30], Step [581/1067], D_A_loss: 0.0590, D_B_loss: 0.0968, G_A_loss: 0.4298, G_B_loss: 0.7324\n",
      "Epoch [4/30], Step [591/1067], D_A_loss: 0.1437, D_B_loss: 0.3678, G_A_loss: 0.0814, G_B_loss: 0.3899\n",
      "Epoch [4/30], Step [601/1067], D_A_loss: 0.0784, D_B_loss: 0.1650, G_A_loss: 0.4280, G_B_loss: 0.3650\n",
      "Epoch [4/30], Step [611/1067], D_A_loss: 0.0591, D_B_loss: 0.1546, G_A_loss: 0.7713, G_B_loss: 0.3743\n",
      "Epoch [4/30], Step [621/1067], D_A_loss: 0.0470, D_B_loss: 0.0412, G_A_loss: 0.6096, G_B_loss: 0.3537\n",
      "Epoch [4/30], Step [631/1067], D_A_loss: 0.0165, D_B_loss: 0.0588, G_A_loss: 0.5902, G_B_loss: 0.6802\n",
      "Epoch [4/30], Step [641/1067], D_A_loss: 0.1767, D_B_loss: 0.2223, G_A_loss: 0.8216, G_B_loss: 0.2016\n",
      "Epoch [4/30], Step [651/1067], D_A_loss: 0.1367, D_B_loss: 0.1460, G_A_loss: 0.4723, G_B_loss: 0.4839\n",
      "Epoch [4/30], Step [661/1067], D_A_loss: 0.1095, D_B_loss: 0.0433, G_A_loss: 0.5683, G_B_loss: 0.4561\n",
      "Epoch [4/30], Step [671/1067], D_A_loss: 0.1546, D_B_loss: 0.1409, G_A_loss: 0.2882, G_B_loss: 0.7782\n",
      "Epoch [4/30], Step [681/1067], D_A_loss: 0.1213, D_B_loss: 0.1484, G_A_loss: 0.7180, G_B_loss: 0.2993\n",
      "Epoch [4/30], Step [691/1067], D_A_loss: 0.2227, D_B_loss: 0.3027, G_A_loss: 0.3242, G_B_loss: 0.1901\n",
      "Epoch [4/30], Step [701/1067], D_A_loss: 0.1774, D_B_loss: 0.0581, G_A_loss: 0.1167, G_B_loss: 1.0966\n",
      "Epoch [4/30], Step [711/1067], D_A_loss: 0.0152, D_B_loss: 0.2976, G_A_loss: 0.6143, G_B_loss: 0.3065\n",
      "Epoch [4/30], Step [721/1067], D_A_loss: 0.0261, D_B_loss: 0.0711, G_A_loss: 0.3160, G_B_loss: 1.0100\n",
      "Epoch [4/30], Step [731/1067], D_A_loss: 0.0511, D_B_loss: 0.1134, G_A_loss: 0.6481, G_B_loss: 0.6147\n",
      "Epoch [4/30], Step [741/1067], D_A_loss: 0.1237, D_B_loss: 0.0450, G_A_loss: 0.5657, G_B_loss: 0.3085\n",
      "Epoch [4/30], Step [751/1067], D_A_loss: 0.1297, D_B_loss: 0.3311, G_A_loss: 0.1927, G_B_loss: 0.3572\n",
      "Epoch [4/30], Step [761/1067], D_A_loss: 0.6569, D_B_loss: 0.1729, G_A_loss: 0.2906, G_B_loss: 1.0046\n",
      "Epoch [4/30], Step [771/1067], D_A_loss: 0.0946, D_B_loss: 0.1965, G_A_loss: 0.1781, G_B_loss: 0.1134\n",
      "Epoch [4/30], Step [781/1067], D_A_loss: 0.4276, D_B_loss: 0.1924, G_A_loss: 0.5774, G_B_loss: 1.2567\n",
      "Epoch [4/30], Step [791/1067], D_A_loss: 0.3242, D_B_loss: 0.1752, G_A_loss: 0.2233, G_B_loss: 0.1279\n",
      "Epoch [4/30], Step [801/1067], D_A_loss: 0.1218, D_B_loss: 0.1571, G_A_loss: 0.9586, G_B_loss: 0.5595\n",
      "Epoch [4/30], Step [811/1067], D_A_loss: 0.1204, D_B_loss: 0.1316, G_A_loss: 0.4172, G_B_loss: 0.5232\n",
      "Epoch [4/30], Step [821/1067], D_A_loss: 0.1577, D_B_loss: 0.1265, G_A_loss: 0.3097, G_B_loss: 0.2048\n",
      "Epoch [4/30], Step [831/1067], D_A_loss: 0.1758, D_B_loss: 0.2264, G_A_loss: 0.8133, G_B_loss: 0.2083\n",
      "Epoch [4/30], Step [841/1067], D_A_loss: 0.1314, D_B_loss: 0.0841, G_A_loss: 0.4390, G_B_loss: 0.2926\n",
      "Epoch [4/30], Step [851/1067], D_A_loss: 0.4553, D_B_loss: 0.1040, G_A_loss: 0.4366, G_B_loss: 0.1885\n",
      "Epoch [4/30], Step [861/1067], D_A_loss: 0.1020, D_B_loss: 0.0816, G_A_loss: 0.5956, G_B_loss: 0.2354\n",
      "Epoch [4/30], Step [871/1067], D_A_loss: 0.1263, D_B_loss: 0.3396, G_A_loss: 0.9979, G_B_loss: 0.2907\n",
      "Epoch [4/30], Step [881/1067], D_A_loss: 0.0661, D_B_loss: 0.0928, G_A_loss: 0.5374, G_B_loss: 0.3571\n",
      "Epoch [4/30], Step [891/1067], D_A_loss: 0.1433, D_B_loss: 0.0764, G_A_loss: 0.2198, G_B_loss: 0.4226\n",
      "Epoch [4/30], Step [901/1067], D_A_loss: 0.1083, D_B_loss: 0.1641, G_A_loss: 0.4432, G_B_loss: 0.3845\n",
      "Epoch [4/30], Step [911/1067], D_A_loss: 0.0431, D_B_loss: 0.0440, G_A_loss: 0.4572, G_B_loss: 0.2585\n",
      "Epoch [4/30], Step [921/1067], D_A_loss: 0.2247, D_B_loss: 0.2332, G_A_loss: 0.1948, G_B_loss: 0.4938\n",
      "Epoch [4/30], Step [931/1067], D_A_loss: 0.2828, D_B_loss: 0.2724, G_A_loss: 0.3414, G_B_loss: 0.4449\n",
      "Epoch [4/30], Step [941/1067], D_A_loss: 0.0422, D_B_loss: 0.1358, G_A_loss: 0.4450, G_B_loss: 0.4216\n",
      "Epoch [4/30], Step [951/1067], D_A_loss: 0.0507, D_B_loss: 0.2469, G_A_loss: 0.1591, G_B_loss: 0.4050\n",
      "Epoch [4/30], Step [961/1067], D_A_loss: 0.1040, D_B_loss: 0.2603, G_A_loss: 0.4279, G_B_loss: 0.5089\n",
      "Epoch [4/30], Step [971/1067], D_A_loss: 0.1475, D_B_loss: 0.2181, G_A_loss: 0.1513, G_B_loss: 0.2504\n",
      "Epoch [4/30], Step [981/1067], D_A_loss: 0.0438, D_B_loss: 0.0370, G_A_loss: 0.4201, G_B_loss: 0.5822\n",
      "Epoch [4/30], Step [991/1067], D_A_loss: 0.1234, D_B_loss: 0.1598, G_A_loss: 1.1330, G_B_loss: 0.4154\n",
      "Epoch [4/30], Step [1001/1067], D_A_loss: 0.0702, D_B_loss: 0.1858, G_A_loss: 0.3373, G_B_loss: 0.2179\n",
      "Epoch [4/30], Step [1011/1067], D_A_loss: 0.2666, D_B_loss: 0.0923, G_A_loss: 0.6023, G_B_loss: 0.1276\n",
      "Epoch [4/30], Step [1021/1067], D_A_loss: 0.1858, D_B_loss: 0.2524, G_A_loss: 0.9524, G_B_loss: 0.2085\n",
      "Epoch [4/30], Step [1031/1067], D_A_loss: 0.1487, D_B_loss: 0.1114, G_A_loss: 0.4237, G_B_loss: 0.5013\n",
      "Epoch [4/30], Step [1041/1067], D_A_loss: 0.1265, D_B_loss: 0.0601, G_A_loss: 1.3114, G_B_loss: 0.3321\n",
      "Epoch [4/30], Step [1051/1067], D_A_loss: 0.0235, D_B_loss: 0.0500, G_A_loss: 0.3073, G_B_loss: 1.0750\n",
      "Epoch [4/30], Step [1061/1067], D_A_loss: 0.1484, D_B_loss: 0.1449, G_A_loss: 0.6857, G_B_loss: 0.3137\n",
      "Epoch [5/30], Step [1/1067], D_A_loss: 0.4083, D_B_loss: 0.3191, G_A_loss: 0.3657, G_B_loss: 0.0729\n",
      "Epoch [5/30], Step [11/1067], D_A_loss: 0.2273, D_B_loss: 0.0768, G_A_loss: 0.1001, G_B_loss: 0.6239\n",
      "Epoch [5/30], Step [21/1067], D_A_loss: 0.0771, D_B_loss: 0.1307, G_A_loss: 0.2605, G_B_loss: 0.3999\n",
      "Epoch [5/30], Step [31/1067], D_A_loss: 0.0398, D_B_loss: 0.2138, G_A_loss: 0.2224, G_B_loss: 0.5863\n",
      "Epoch [5/30], Step [41/1067], D_A_loss: 0.1776, D_B_loss: 0.1257, G_A_loss: 0.1648, G_B_loss: 0.0995\n",
      "Epoch [5/30], Step [51/1067], D_A_loss: 0.3996, D_B_loss: 0.1852, G_A_loss: 0.2279, G_B_loss: 0.0946\n",
      "Epoch [5/30], Step [61/1067], D_A_loss: 0.1008, D_B_loss: 0.1728, G_A_loss: 0.6538, G_B_loss: 0.3570\n",
      "Epoch [5/30], Step [71/1067], D_A_loss: 0.4273, D_B_loss: 0.1632, G_A_loss: 0.2548, G_B_loss: 0.4942\n",
      "Epoch [5/30], Step [81/1067], D_A_loss: 0.0197, D_B_loss: 0.1391, G_A_loss: 0.7299, G_B_loss: 0.4722\n",
      "Epoch [5/30], Step [91/1067], D_A_loss: 0.1255, D_B_loss: 0.0696, G_A_loss: 1.0289, G_B_loss: 0.4218\n",
      "Epoch [5/30], Step [101/1067], D_A_loss: 0.0262, D_B_loss: 0.1107, G_A_loss: 0.4568, G_B_loss: 0.3726\n",
      "Epoch [5/30], Step [111/1067], D_A_loss: 0.0702, D_B_loss: 0.1156, G_A_loss: 0.3965, G_B_loss: 0.7933\n",
      "Epoch [5/30], Step [121/1067], D_A_loss: 0.1516, D_B_loss: 0.1454, G_A_loss: 0.2466, G_B_loss: 0.3828\n",
      "Epoch [5/30], Step [131/1067], D_A_loss: 0.0882, D_B_loss: 0.0709, G_A_loss: 0.5049, G_B_loss: 0.5755\n",
      "Epoch [5/30], Step [141/1067], D_A_loss: 0.2007, D_B_loss: 0.1038, G_A_loss: 0.3399, G_B_loss: 0.7544\n",
      "Epoch [5/30], Step [151/1067], D_A_loss: 0.1731, D_B_loss: 0.0993, G_A_loss: 0.9467, G_B_loss: 0.3075\n",
      "Epoch [5/30], Step [161/1067], D_A_loss: 0.2044, D_B_loss: 0.0756, G_A_loss: 0.3300, G_B_loss: 0.2306\n",
      "Epoch [5/30], Step [171/1067], D_A_loss: 0.0487, D_B_loss: 0.0455, G_A_loss: 0.4485, G_B_loss: 0.4529\n",
      "Epoch [5/30], Step [181/1067], D_A_loss: 0.1861, D_B_loss: 0.3684, G_A_loss: 0.4176, G_B_loss: 0.7625\n",
      "Epoch [5/30], Step [191/1067], D_A_loss: 0.3378, D_B_loss: 0.1700, G_A_loss: 0.4119, G_B_loss: 0.0652\n",
      "Epoch [5/30], Step [201/1067], D_A_loss: 0.0576, D_B_loss: 0.0386, G_A_loss: 0.4561, G_B_loss: 0.5536\n",
      "Epoch [5/30], Step [211/1067], D_A_loss: 0.0843, D_B_loss: 0.2333, G_A_loss: 0.4834, G_B_loss: 0.5327\n",
      "Epoch [5/30], Step [221/1067], D_A_loss: 0.2112, D_B_loss: 0.2213, G_A_loss: 0.2121, G_B_loss: 0.5849\n",
      "Epoch [5/30], Step [231/1067], D_A_loss: 0.1438, D_B_loss: 0.0343, G_A_loss: 0.5845, G_B_loss: 0.3573\n",
      "Epoch [5/30], Step [241/1067], D_A_loss: 0.1795, D_B_loss: 0.1230, G_A_loss: 0.5706, G_B_loss: 0.2668\n",
      "Epoch [5/30], Step [251/1067], D_A_loss: 0.1412, D_B_loss: 0.1696, G_A_loss: 0.6442, G_B_loss: 0.4328\n",
      "Epoch [5/30], Step [261/1067], D_A_loss: 0.0593, D_B_loss: 0.0592, G_A_loss: 0.8890, G_B_loss: 0.5910\n",
      "Epoch [5/30], Step [271/1067], D_A_loss: 0.1487, D_B_loss: 0.0296, G_A_loss: 0.4567, G_B_loss: 0.3627\n",
      "Epoch [5/30], Step [281/1067], D_A_loss: 0.2481, D_B_loss: 0.1293, G_A_loss: 0.5255, G_B_loss: 0.1625\n",
      "Epoch [5/30], Step [291/1067], D_A_loss: 0.0506, D_B_loss: 0.4292, G_A_loss: 0.0292, G_B_loss: 0.6521\n",
      "Epoch [5/30], Step [301/1067], D_A_loss: 0.0498, D_B_loss: 0.1454, G_A_loss: 0.4481, G_B_loss: 0.6411\n",
      "Epoch [5/30], Step [311/1067], D_A_loss: 0.0512, D_B_loss: 0.0251, G_A_loss: 0.3241, G_B_loss: 0.4117\n",
      "Epoch [5/30], Step [321/1067], D_A_loss: 0.2666, D_B_loss: 0.1529, G_A_loss: 0.2675, G_B_loss: 0.9931\n",
      "Epoch [5/30], Step [331/1067], D_A_loss: 0.1528, D_B_loss: 0.1273, G_A_loss: 0.2968, G_B_loss: 0.2640\n",
      "Epoch [5/30], Step [341/1067], D_A_loss: 0.1426, D_B_loss: 0.1149, G_A_loss: 0.4931, G_B_loss: 0.5429\n",
      "Epoch [5/30], Step [351/1067], D_A_loss: 0.1695, D_B_loss: 0.0689, G_A_loss: 0.3609, G_B_loss: 0.4304\n",
      "Epoch [5/30], Step [361/1067], D_A_loss: 0.0651, D_B_loss: 0.2065, G_A_loss: 0.2751, G_B_loss: 0.5162\n",
      "Epoch [5/30], Step [371/1067], D_A_loss: 0.1051, D_B_loss: 0.0882, G_A_loss: 0.4469, G_B_loss: 0.4520\n",
      "Epoch [5/30], Step [381/1067], D_A_loss: 0.1254, D_B_loss: 0.1669, G_A_loss: 0.5261, G_B_loss: 0.5931\n",
      "Epoch [5/30], Step [391/1067], D_A_loss: 0.1513, D_B_loss: 0.1796, G_A_loss: 0.4501, G_B_loss: 1.0196\n",
      "Epoch [5/30], Step [401/1067], D_A_loss: 0.0567, D_B_loss: 0.1680, G_A_loss: 0.3572, G_B_loss: 0.6980\n",
      "Epoch [5/30], Step [411/1067], D_A_loss: 0.1430, D_B_loss: 0.1944, G_A_loss: 0.2001, G_B_loss: 0.2143\n",
      "Epoch [5/30], Step [421/1067], D_A_loss: 0.0869, D_B_loss: 0.0971, G_A_loss: 0.2547, G_B_loss: 0.0652\n",
      "Epoch [5/30], Step [431/1067], D_A_loss: 0.0281, D_B_loss: 0.2636, G_A_loss: 0.6470, G_B_loss: 0.4512\n",
      "Epoch [5/30], Step [441/1067], D_A_loss: 0.3055, D_B_loss: 0.1449, G_A_loss: 0.4115, G_B_loss: 1.0656\n",
      "Epoch [5/30], Step [451/1067], D_A_loss: 0.0971, D_B_loss: 0.1814, G_A_loss: 0.5134, G_B_loss: 0.0541\n",
      "Epoch [5/30], Step [461/1067], D_A_loss: 0.0556, D_B_loss: 0.2515, G_A_loss: 0.1656, G_B_loss: 0.4853\n",
      "Epoch [5/30], Step [471/1067], D_A_loss: 0.0447, D_B_loss: 0.2073, G_A_loss: 1.1157, G_B_loss: 0.5247\n",
      "Epoch [5/30], Step [481/1067], D_A_loss: 0.3178, D_B_loss: 0.2182, G_A_loss: 0.2624, G_B_loss: 0.1982\n",
      "Epoch [5/30], Step [491/1067], D_A_loss: 0.1116, D_B_loss: 0.1201, G_A_loss: 0.4126, G_B_loss: 0.5422\n",
      "Epoch [5/30], Step [501/1067], D_A_loss: 0.2339, D_B_loss: 0.1145, G_A_loss: 0.3180, G_B_loss: 0.1345\n",
      "Epoch [5/30], Step [511/1067], D_A_loss: 0.4331, D_B_loss: 0.2881, G_A_loss: 0.7257, G_B_loss: 0.4978\n",
      "Epoch [5/30], Step [521/1067], D_A_loss: 0.2342, D_B_loss: 0.0220, G_A_loss: 0.2739, G_B_loss: 0.2938\n",
      "Epoch [5/30], Step [531/1067], D_A_loss: 0.2986, D_B_loss: 0.1429, G_A_loss: 0.3133, G_B_loss: 0.4332\n",
      "Epoch [5/30], Step [541/1067], D_A_loss: 0.0231, D_B_loss: 0.0415, G_A_loss: 1.0964, G_B_loss: 0.2294\n",
      "Epoch [5/30], Step [551/1067], D_A_loss: 0.0789, D_B_loss: 0.0326, G_A_loss: 0.7749, G_B_loss: 0.1695\n",
      "Epoch [5/30], Step [561/1067], D_A_loss: 0.1444, D_B_loss: 0.1409, G_A_loss: 0.3612, G_B_loss: 0.2486\n",
      "Epoch [5/30], Step [571/1067], D_A_loss: 0.0977, D_B_loss: 0.3378, G_A_loss: 0.0660, G_B_loss: 0.5491\n",
      "Epoch [5/30], Step [581/1067], D_A_loss: 0.0260, D_B_loss: 0.0824, G_A_loss: 0.6336, G_B_loss: 1.0281\n",
      "Epoch [5/30], Step [591/1067], D_A_loss: 0.0546, D_B_loss: 0.1699, G_A_loss: 0.7806, G_B_loss: 0.3212\n",
      "Epoch [5/30], Step [601/1067], D_A_loss: 0.1929, D_B_loss: 0.0400, G_A_loss: 0.1950, G_B_loss: 0.5359\n",
      "Epoch [5/30], Step [611/1067], D_A_loss: 0.1262, D_B_loss: 0.1055, G_A_loss: 0.4248, G_B_loss: 0.1926\n",
      "Epoch [5/30], Step [621/1067], D_A_loss: 0.0873, D_B_loss: 0.0434, G_A_loss: 0.2832, G_B_loss: 0.4732\n",
      "Epoch [5/30], Step [631/1067], D_A_loss: 0.2321, D_B_loss: 0.2122, G_A_loss: 0.3570, G_B_loss: 0.1350\n",
      "Epoch [5/30], Step [641/1067], D_A_loss: 0.1871, D_B_loss: 0.2175, G_A_loss: 0.2180, G_B_loss: 0.1431\n",
      "Epoch [5/30], Step [651/1067], D_A_loss: 0.1902, D_B_loss: 0.1474, G_A_loss: 0.5869, G_B_loss: 0.2034\n",
      "Epoch [5/30], Step [661/1067], D_A_loss: 0.1244, D_B_loss: 0.0634, G_A_loss: 0.5785, G_B_loss: 0.3391\n",
      "Epoch [5/30], Step [671/1067], D_A_loss: 0.1260, D_B_loss: 0.4111, G_A_loss: 0.6745, G_B_loss: 0.4946\n",
      "Epoch [5/30], Step [681/1067], D_A_loss: 0.0349, D_B_loss: 0.0609, G_A_loss: 0.8530, G_B_loss: 0.6413\n",
      "Epoch [5/30], Step [691/1067], D_A_loss: 0.0734, D_B_loss: 0.1020, G_A_loss: 0.7203, G_B_loss: 0.4605\n",
      "Epoch [5/30], Step [701/1067], D_A_loss: 0.0274, D_B_loss: 0.0555, G_A_loss: 1.0197, G_B_loss: 0.2079\n",
      "Epoch [5/30], Step [711/1067], D_A_loss: 0.2389, D_B_loss: 0.0645, G_A_loss: 0.5299, G_B_loss: 0.2750\n",
      "Epoch [5/30], Step [721/1067], D_A_loss: 0.1125, D_B_loss: 0.0383, G_A_loss: 0.2790, G_B_loss: 0.1684\n",
      "Epoch [5/30], Step [731/1067], D_A_loss: 0.2908, D_B_loss: 0.2913, G_A_loss: 0.4045, G_B_loss: 0.1194\n",
      "Epoch [5/30], Step [741/1067], D_A_loss: 0.0540, D_B_loss: 0.1680, G_A_loss: 0.7844, G_B_loss: 0.2942\n",
      "Epoch [5/30], Step [751/1067], D_A_loss: 0.1353, D_B_loss: 0.2274, G_A_loss: 0.5188, G_B_loss: 0.2383\n",
      "Epoch [5/30], Step [761/1067], D_A_loss: 0.1796, D_B_loss: 0.1346, G_A_loss: 0.3104, G_B_loss: 0.3018\n",
      "Epoch [5/30], Step [771/1067], D_A_loss: 0.0625, D_B_loss: 0.0776, G_A_loss: 0.7208, G_B_loss: 0.5567\n",
      "Epoch [5/30], Step [781/1067], D_A_loss: 0.2041, D_B_loss: 0.2296, G_A_loss: 0.1402, G_B_loss: 0.3182\n",
      "Epoch [5/30], Step [791/1067], D_A_loss: 0.0939, D_B_loss: 0.0902, G_A_loss: 0.5546, G_B_loss: 0.3114\n",
      "Epoch [5/30], Step [801/1067], D_A_loss: 0.2294, D_B_loss: 0.1490, G_A_loss: 0.3107, G_B_loss: 1.3554\n",
      "Epoch [5/30], Step [811/1067], D_A_loss: 0.1083, D_B_loss: 0.1358, G_A_loss: 0.1393, G_B_loss: 0.1056\n",
      "Epoch [5/30], Step [821/1067], D_A_loss: 0.0118, D_B_loss: 0.1223, G_A_loss: 0.3242, G_B_loss: 0.4010\n",
      "Epoch [5/30], Step [831/1067], D_A_loss: 0.3578, D_B_loss: 0.1291, G_A_loss: 0.4159, G_B_loss: 0.2065\n",
      "Epoch [5/30], Step [841/1067], D_A_loss: 0.0769, D_B_loss: 0.0785, G_A_loss: 0.5750, G_B_loss: 0.2591\n",
      "Epoch [5/30], Step [851/1067], D_A_loss: 0.4043, D_B_loss: 0.0956, G_A_loss: 0.4014, G_B_loss: 0.7140\n",
      "Epoch [5/30], Step [861/1067], D_A_loss: 0.1156, D_B_loss: 0.1019, G_A_loss: 0.2502, G_B_loss: 0.3397\n",
      "Epoch [5/30], Step [871/1067], D_A_loss: 0.1518, D_B_loss: 0.1008, G_A_loss: 1.0981, G_B_loss: 0.3711\n",
      "Epoch [5/30], Step [881/1067], D_A_loss: 0.2801, D_B_loss: 0.0551, G_A_loss: 0.2846, G_B_loss: 0.0825\n",
      "Epoch [5/30], Step [891/1067], D_A_loss: 0.1780, D_B_loss: 0.2163, G_A_loss: 0.5800, G_B_loss: 0.3934\n",
      "Epoch [5/30], Step [901/1067], D_A_loss: 0.2018, D_B_loss: 0.1099, G_A_loss: 0.4242, G_B_loss: 0.1973\n",
      "Epoch [5/30], Step [911/1067], D_A_loss: 0.2585, D_B_loss: 0.3380, G_A_loss: 0.8900, G_B_loss: 0.4583\n",
      "Epoch [5/30], Step [921/1067], D_A_loss: 0.0736, D_B_loss: 0.2212, G_A_loss: 0.7716, G_B_loss: 0.1963\n",
      "Epoch [5/30], Step [931/1067], D_A_loss: 0.1399, D_B_loss: 0.0970, G_A_loss: 0.5288, G_B_loss: 0.5014\n",
      "Epoch [5/30], Step [941/1067], D_A_loss: 0.0681, D_B_loss: 0.1345, G_A_loss: 0.4425, G_B_loss: 0.5980\n",
      "Epoch [5/30], Step [951/1067], D_A_loss: 0.3513, D_B_loss: 0.0592, G_A_loss: 0.1105, G_B_loss: 0.5023\n",
      "Epoch [5/30], Step [961/1067], D_A_loss: 0.2050, D_B_loss: 0.1782, G_A_loss: 0.2609, G_B_loss: 0.2917\n",
      "Epoch [5/30], Step [971/1067], D_A_loss: 0.0525, D_B_loss: 0.0784, G_A_loss: 0.3644, G_B_loss: 0.5635\n",
      "Epoch [5/30], Step [981/1067], D_A_loss: 0.4960, D_B_loss: 0.1619, G_A_loss: 0.7705, G_B_loss: 0.1436\n",
      "Epoch [5/30], Step [991/1067], D_A_loss: 0.1917, D_B_loss: 0.0717, G_A_loss: 0.2292, G_B_loss: 0.1989\n",
      "Epoch [5/30], Step [1001/1067], D_A_loss: 0.0606, D_B_loss: 0.0933, G_A_loss: 0.3518, G_B_loss: 0.6631\n",
      "Epoch [5/30], Step [1011/1067], D_A_loss: 0.0395, D_B_loss: 0.0222, G_A_loss: 0.3506, G_B_loss: 0.1169\n",
      "Epoch [5/30], Step [1021/1067], D_A_loss: 0.1031, D_B_loss: 0.0395, G_A_loss: 0.4279, G_B_loss: 0.2326\n",
      "Epoch [5/30], Step [1031/1067], D_A_loss: 0.2187, D_B_loss: 0.1087, G_A_loss: 0.1712, G_B_loss: 0.2100\n",
      "Epoch [5/30], Step [1041/1067], D_A_loss: 0.1290, D_B_loss: 0.0823, G_A_loss: 0.6020, G_B_loss: 0.4685\n",
      "Epoch [5/30], Step [1051/1067], D_A_loss: 0.0967, D_B_loss: 0.0387, G_A_loss: 0.2397, G_B_loss: 0.7103\n",
      "Epoch [5/30], Step [1061/1067], D_A_loss: 0.1900, D_B_loss: 0.3642, G_A_loss: 0.5857, G_B_loss: 0.6209\n",
      "Epoch [6/30], Step [1/1067], D_A_loss: 0.0827, D_B_loss: 0.1833, G_A_loss: 0.2597, G_B_loss: 0.6166\n",
      "Epoch [6/30], Step [11/1067], D_A_loss: 0.1401, D_B_loss: 0.0871, G_A_loss: 0.7921, G_B_loss: 0.3575\n",
      "Epoch [6/30], Step [21/1067], D_A_loss: 0.1138, D_B_loss: 0.0368, G_A_loss: 0.3248, G_B_loss: 0.2083\n",
      "Epoch [6/30], Step [31/1067], D_A_loss: 0.2905, D_B_loss: 0.0617, G_A_loss: 0.7450, G_B_loss: 0.6516\n",
      "Epoch [6/30], Step [41/1067], D_A_loss: 0.2749, D_B_loss: 0.1565, G_A_loss: 0.3167, G_B_loss: 0.6790\n",
      "Epoch [6/30], Step [51/1067], D_A_loss: 0.2370, D_B_loss: 0.0828, G_A_loss: 0.6056, G_B_loss: 0.6550\n",
      "Epoch [6/30], Step [61/1067], D_A_loss: 0.0526, D_B_loss: 0.1636, G_A_loss: 0.4329, G_B_loss: 0.5454\n",
      "Epoch [6/30], Step [71/1067], D_A_loss: 0.0935, D_B_loss: 0.0160, G_A_loss: 0.1749, G_B_loss: 0.5191\n",
      "Epoch [6/30], Step [81/1067], D_A_loss: 0.0464, D_B_loss: 0.3189, G_A_loss: 0.6797, G_B_loss: 0.4130\n",
      "Epoch [6/30], Step [91/1067], D_A_loss: 0.0224, D_B_loss: 0.2410, G_A_loss: 0.1910, G_B_loss: 0.6784\n",
      "Epoch [6/30], Step [101/1067], D_A_loss: 0.0540, D_B_loss: 0.1697, G_A_loss: 0.2545, G_B_loss: 0.0983\n",
      "Epoch [6/30], Step [111/1067], D_A_loss: 0.1265, D_B_loss: 0.0826, G_A_loss: 0.3219, G_B_loss: 0.4776\n",
      "Epoch [6/30], Step [121/1067], D_A_loss: 0.0791, D_B_loss: 0.0509, G_A_loss: 0.3481, G_B_loss: 0.3241\n",
      "Epoch [6/30], Step [131/1067], D_A_loss: 0.0526, D_B_loss: 0.2420, G_A_loss: 0.2435, G_B_loss: 0.5884\n",
      "Epoch [6/30], Step [141/1067], D_A_loss: 0.1916, D_B_loss: 0.2503, G_A_loss: 0.6947, G_B_loss: 0.2805\n",
      "Epoch [6/30], Step [151/1067], D_A_loss: 0.0229, D_B_loss: 0.2690, G_A_loss: 0.3594, G_B_loss: 0.3945\n",
      "Epoch [6/30], Step [161/1067], D_A_loss: 0.0760, D_B_loss: 0.0911, G_A_loss: 0.3212, G_B_loss: 0.3297\n",
      "Epoch [6/30], Step [171/1067], D_A_loss: 0.0574, D_B_loss: 0.1878, G_A_loss: 0.6127, G_B_loss: 0.1569\n",
      "Epoch [6/30], Step [181/1067], D_A_loss: 0.2123, D_B_loss: 0.1026, G_A_loss: 0.4623, G_B_loss: 0.1158\n",
      "Epoch [6/30], Step [191/1067], D_A_loss: 0.2695, D_B_loss: 0.0913, G_A_loss: 0.3260, G_B_loss: 0.1649\n",
      "Epoch [6/30], Step [201/1067], D_A_loss: 0.3973, D_B_loss: 0.0417, G_A_loss: 0.3881, G_B_loss: 0.0341\n",
      "Epoch [6/30], Step [211/1067], D_A_loss: 0.2976, D_B_loss: 0.1845, G_A_loss: 0.6553, G_B_loss: 0.1935\n",
      "Epoch [6/30], Step [221/1067], D_A_loss: 0.1763, D_B_loss: 0.2139, G_A_loss: 0.1929, G_B_loss: 0.6968\n",
      "Epoch [6/30], Step [231/1067], D_A_loss: 0.1911, D_B_loss: 0.1339, G_A_loss: 0.2856, G_B_loss: 0.5185\n",
      "Epoch [6/30], Step [241/1067], D_A_loss: 0.1599, D_B_loss: 0.0679, G_A_loss: 0.3777, G_B_loss: 0.5234\n",
      "Epoch [6/30], Step [251/1067], D_A_loss: 0.1806, D_B_loss: 0.0204, G_A_loss: 0.4004, G_B_loss: 0.6472\n",
      "Epoch [6/30], Step [261/1067], D_A_loss: 0.1075, D_B_loss: 0.0425, G_A_loss: 0.7369, G_B_loss: 0.3547\n",
      "Epoch [6/30], Step [271/1067], D_A_loss: 0.1356, D_B_loss: 0.0954, G_A_loss: 0.3103, G_B_loss: 0.7093\n",
      "Epoch [6/30], Step [281/1067], D_A_loss: 0.0542, D_B_loss: 0.2306, G_A_loss: 0.4865, G_B_loss: 0.3975\n",
      "Epoch [6/30], Step [291/1067], D_A_loss: 0.0844, D_B_loss: 0.2079, G_A_loss: 1.3871, G_B_loss: 0.2033\n",
      "Epoch [6/30], Step [301/1067], D_A_loss: 0.2076, D_B_loss: 0.1627, G_A_loss: 0.3021, G_B_loss: 0.7035\n",
      "Epoch [6/30], Step [311/1067], D_A_loss: 0.0493, D_B_loss: 0.6109, G_A_loss: 1.5777, G_B_loss: 0.5871\n",
      "Epoch [6/30], Step [321/1067], D_A_loss: 0.0866, D_B_loss: 0.0400, G_A_loss: 0.7799, G_B_loss: 0.4227\n",
      "Epoch [6/30], Step [331/1067], D_A_loss: 0.0536, D_B_loss: 0.0624, G_A_loss: 0.2855, G_B_loss: 0.4259\n",
      "Epoch [6/30], Step [341/1067], D_A_loss: 0.0240, D_B_loss: 0.2654, G_A_loss: 0.8459, G_B_loss: 0.3487\n",
      "Epoch [6/30], Step [351/1067], D_A_loss: 0.2087, D_B_loss: 0.1376, G_A_loss: 0.2969, G_B_loss: 0.0430\n",
      "Epoch [6/30], Step [361/1067], D_A_loss: 0.1409, D_B_loss: 0.1103, G_A_loss: 0.4870, G_B_loss: 0.3031\n",
      "Epoch [6/30], Step [371/1067], D_A_loss: 0.1719, D_B_loss: 0.0931, G_A_loss: 0.3166, G_B_loss: 0.7859\n",
      "Epoch [6/30], Step [381/1067], D_A_loss: 0.2250, D_B_loss: 0.1842, G_A_loss: 0.6699, G_B_loss: 0.2876\n",
      "Epoch [6/30], Step [391/1067], D_A_loss: 0.0642, D_B_loss: 0.1957, G_A_loss: 0.2642, G_B_loss: 0.5416\n",
      "Epoch [6/30], Step [401/1067], D_A_loss: 0.0823, D_B_loss: 0.2836, G_A_loss: 0.2606, G_B_loss: 0.5133\n",
      "Epoch [6/30], Step [411/1067], D_A_loss: 0.0328, D_B_loss: 0.1729, G_A_loss: 0.3442, G_B_loss: 0.2226\n",
      "Epoch [6/30], Step [421/1067], D_A_loss: 0.0766, D_B_loss: 0.1935, G_A_loss: 0.3610, G_B_loss: 0.4768\n",
      "Epoch [6/30], Step [431/1067], D_A_loss: 0.2766, D_B_loss: 0.0360, G_A_loss: 0.4414, G_B_loss: 0.3270\n",
      "Epoch [6/30], Step [441/1067], D_A_loss: 0.0757, D_B_loss: 0.1233, G_A_loss: 0.8176, G_B_loss: 0.7531\n",
      "Epoch [6/30], Step [451/1067], D_A_loss: 0.1370, D_B_loss: 0.1589, G_A_loss: 0.5255, G_B_loss: 0.1306\n",
      "Epoch [6/30], Step [461/1067], D_A_loss: 0.0279, D_B_loss: 0.1704, G_A_loss: 0.2116, G_B_loss: 0.1165\n",
      "Epoch [6/30], Step [471/1067], D_A_loss: 0.2140, D_B_loss: 0.0532, G_A_loss: 0.3264, G_B_loss: 0.2135\n",
      "Epoch [6/30], Step [481/1067], D_A_loss: 0.1382, D_B_loss: 0.1469, G_A_loss: 0.5773, G_B_loss: 0.3826\n",
      "Epoch [6/30], Step [491/1067], D_A_loss: 0.1828, D_B_loss: 0.1211, G_A_loss: 0.4471, G_B_loss: 0.3976\n",
      "Epoch [6/30], Step [501/1067], D_A_loss: 0.2950, D_B_loss: 0.1311, G_A_loss: 0.4755, G_B_loss: 0.3896\n",
      "Epoch [6/30], Step [511/1067], D_A_loss: 0.2023, D_B_loss: 0.1506, G_A_loss: 0.2948, G_B_loss: 0.3331\n",
      "Epoch [6/30], Step [521/1067], D_A_loss: 0.1698, D_B_loss: 0.1138, G_A_loss: 0.3791, G_B_loss: 0.1255\n",
      "Epoch [6/30], Step [531/1067], D_A_loss: 0.2948, D_B_loss: 0.2241, G_A_loss: 0.5554, G_B_loss: 0.1028\n",
      "Epoch [6/30], Step [541/1067], D_A_loss: 0.1719, D_B_loss: 0.1143, G_A_loss: 0.4489, G_B_loss: 0.5705\n",
      "Epoch [6/30], Step [551/1067], D_A_loss: 0.2331, D_B_loss: 0.0662, G_A_loss: 0.2040, G_B_loss: 0.4505\n",
      "Epoch [6/30], Step [561/1067], D_A_loss: 0.1684, D_B_loss: 0.6124, G_A_loss: 0.0800, G_B_loss: 0.3465\n",
      "Epoch [6/30], Step [571/1067], D_A_loss: 0.2379, D_B_loss: 0.1584, G_A_loss: 0.1756, G_B_loss: 0.2419\n",
      "Epoch [6/30], Step [581/1067], D_A_loss: 0.2270, D_B_loss: 0.1116, G_A_loss: 0.5062, G_B_loss: 0.1673\n",
      "Epoch [6/30], Step [591/1067], D_A_loss: 0.2169, D_B_loss: 0.1177, G_A_loss: 0.5292, G_B_loss: 0.7500\n",
      "Epoch [6/30], Step [601/1067], D_A_loss: 0.2219, D_B_loss: 0.0528, G_A_loss: 0.2072, G_B_loss: 0.6257\n",
      "Epoch [6/30], Step [611/1067], D_A_loss: 0.0830, D_B_loss: 0.0895, G_A_loss: 0.4234, G_B_loss: 0.5069\n",
      "Epoch [6/30], Step [621/1067], D_A_loss: 0.1545, D_B_loss: 0.0958, G_A_loss: 0.7070, G_B_loss: 0.2719\n",
      "Epoch [6/30], Step [631/1067], D_A_loss: 0.0900, D_B_loss: 0.0468, G_A_loss: 0.3763, G_B_loss: 0.4160\n",
      "Epoch [6/30], Step [641/1067], D_A_loss: 0.1880, D_B_loss: 0.2047, G_A_loss: 1.1677, G_B_loss: 0.2175\n",
      "Epoch [6/30], Step [651/1067], D_A_loss: 0.1378, D_B_loss: 0.2253, G_A_loss: 0.3442, G_B_loss: 0.4505\n",
      "Epoch [6/30], Step [661/1067], D_A_loss: 0.1854, D_B_loss: 0.0948, G_A_loss: 0.5831, G_B_loss: 0.4584\n",
      "Epoch [6/30], Step [671/1067], D_A_loss: 0.1594, D_B_loss: 0.1424, G_A_loss: 0.3024, G_B_loss: 0.3820\n",
      "Epoch [6/30], Step [681/1067], D_A_loss: 0.2648, D_B_loss: 0.0524, G_A_loss: 0.4716, G_B_loss: 0.7970\n",
      "Epoch [6/30], Step [691/1067], D_A_loss: 0.1959, D_B_loss: 0.0986, G_A_loss: 0.3759, G_B_loss: 0.2589\n",
      "Epoch [6/30], Step [701/1067], D_A_loss: 0.1347, D_B_loss: 0.0715, G_A_loss: 0.8867, G_B_loss: 0.6577\n",
      "Epoch [6/30], Step [711/1067], D_A_loss: 0.2500, D_B_loss: 0.3163, G_A_loss: 0.2517, G_B_loss: 0.1129\n",
      "Epoch [6/30], Step [721/1067], D_A_loss: 0.1933, D_B_loss: 0.0476, G_A_loss: 0.5056, G_B_loss: 0.4165\n",
      "Epoch [6/30], Step [731/1067], D_A_loss: 0.3331, D_B_loss: 0.0795, G_A_loss: 0.2507, G_B_loss: 0.3070\n",
      "Epoch [6/30], Step [741/1067], D_A_loss: 0.0398, D_B_loss: 0.0963, G_A_loss: 0.4299, G_B_loss: 0.1358\n",
      "Epoch [6/30], Step [751/1067], D_A_loss: 0.2011, D_B_loss: 0.1584, G_A_loss: 0.3791, G_B_loss: 0.2032\n",
      "Epoch [6/30], Step [761/1067], D_A_loss: 0.1426, D_B_loss: 0.2988, G_A_loss: 0.7012, G_B_loss: 0.5268\n",
      "Epoch [6/30], Step [771/1067], D_A_loss: 0.0473, D_B_loss: 0.1937, G_A_loss: 0.6928, G_B_loss: 0.4154\n",
      "Epoch [6/30], Step [781/1067], D_A_loss: 0.2535, D_B_loss: 0.0993, G_A_loss: 0.5216, G_B_loss: 0.2826\n",
      "Epoch [6/30], Step [791/1067], D_A_loss: 0.0681, D_B_loss: 0.1478, G_A_loss: 0.3909, G_B_loss: 0.0633\n",
      "Epoch [6/30], Step [801/1067], D_A_loss: 0.0641, D_B_loss: 0.2810, G_A_loss: 0.2336, G_B_loss: 0.5085\n",
      "Epoch [6/30], Step [811/1067], D_A_loss: 0.1379, D_B_loss: 0.0270, G_A_loss: 0.4132, G_B_loss: 0.3284\n",
      "Epoch [6/30], Step [821/1067], D_A_loss: 0.1696, D_B_loss: 0.1553, G_A_loss: 0.2706, G_B_loss: 0.3116\n",
      "Epoch [6/30], Step [831/1067], D_A_loss: 0.0693, D_B_loss: 0.1892, G_A_loss: 0.5041, G_B_loss: 0.2826\n",
      "Epoch [6/30], Step [841/1067], D_A_loss: 0.0269, D_B_loss: 0.1671, G_A_loss: 0.2635, G_B_loss: 0.7036\n",
      "Epoch [6/30], Step [851/1067], D_A_loss: 0.0827, D_B_loss: 0.0871, G_A_loss: 0.7320, G_B_loss: 0.2023\n",
      "Epoch [6/30], Step [861/1067], D_A_loss: 0.1969, D_B_loss: 0.0879, G_A_loss: 0.4424, G_B_loss: 0.2738\n",
      "Epoch [6/30], Step [871/1067], D_A_loss: 0.0833, D_B_loss: 0.0896, G_A_loss: 0.6385, G_B_loss: 0.7356\n",
      "Epoch [6/30], Step [881/1067], D_A_loss: 0.2277, D_B_loss: 0.1974, G_A_loss: 0.2465, G_B_loss: 0.1892\n",
      "Epoch [6/30], Step [891/1067], D_A_loss: 0.0436, D_B_loss: 0.0494, G_A_loss: 0.6339, G_B_loss: 0.3325\n",
      "Epoch [6/30], Step [901/1067], D_A_loss: 0.0338, D_B_loss: 0.1739, G_A_loss: 0.9514, G_B_loss: 0.2949\n",
      "Epoch [6/30], Step [911/1067], D_A_loss: 0.3497, D_B_loss: 0.0287, G_A_loss: 0.4109, G_B_loss: 0.0541\n",
      "Epoch [6/30], Step [921/1067], D_A_loss: 0.1949, D_B_loss: 0.0560, G_A_loss: 0.6137, G_B_loss: 0.1836\n",
      "Epoch [6/30], Step [931/1067], D_A_loss: 0.0570, D_B_loss: 0.1437, G_A_loss: 0.3189, G_B_loss: 0.4228\n",
      "Epoch [6/30], Step [941/1067], D_A_loss: 0.1553, D_B_loss: 0.0650, G_A_loss: 0.4573, G_B_loss: 0.9715\n",
      "Epoch [6/30], Step [951/1067], D_A_loss: 0.1419, D_B_loss: 0.1145, G_A_loss: 0.3488, G_B_loss: 0.7107\n",
      "Epoch [6/30], Step [961/1067], D_A_loss: 0.0444, D_B_loss: 0.0832, G_A_loss: 0.3700, G_B_loss: 0.2639\n",
      "Epoch [6/30], Step [971/1067], D_A_loss: 0.0372, D_B_loss: 0.2353, G_A_loss: 1.3551, G_B_loss: 0.2865\n",
      "Epoch [6/30], Step [981/1067], D_A_loss: 0.0911, D_B_loss: 0.0619, G_A_loss: 0.3073, G_B_loss: 0.4516\n",
      "Epoch [6/30], Step [991/1067], D_A_loss: 0.0369, D_B_loss: 0.2022, G_A_loss: 0.2781, G_B_loss: 0.0939\n",
      "Epoch [6/30], Step [1001/1067], D_A_loss: 0.0925, D_B_loss: 0.0781, G_A_loss: 0.2674, G_B_loss: 0.7929\n",
      "Epoch [6/30], Step [1011/1067], D_A_loss: 0.0802, D_B_loss: 0.1021, G_A_loss: 0.7845, G_B_loss: 0.9160\n",
      "Epoch [6/30], Step [1021/1067], D_A_loss: 0.1566, D_B_loss: 0.1352, G_A_loss: 0.3978, G_B_loss: 0.4284\n",
      "Epoch [6/30], Step [1031/1067], D_A_loss: 0.0342, D_B_loss: 0.0399, G_A_loss: 0.5929, G_B_loss: 0.7192\n",
      "Epoch [6/30], Step [1041/1067], D_A_loss: 0.1240, D_B_loss: 0.2810, G_A_loss: 0.4244, G_B_loss: 0.3043\n",
      "Epoch [6/30], Step [1051/1067], D_A_loss: 0.2171, D_B_loss: 0.1359, G_A_loss: 0.4039, G_B_loss: 0.1059\n",
      "Epoch [6/30], Step [1061/1067], D_A_loss: 0.0975, D_B_loss: 0.2702, G_A_loss: 0.5753, G_B_loss: 0.4168\n",
      "Epoch [7/30], Step [1/1067], D_A_loss: 0.2339, D_B_loss: 0.0782, G_A_loss: 0.6255, G_B_loss: 0.1392\n",
      "Epoch [7/30], Step [11/1067], D_A_loss: 0.3891, D_B_loss: 0.1788, G_A_loss: 0.4370, G_B_loss: 0.2859\n",
      "Epoch [7/30], Step [21/1067], D_A_loss: 0.2361, D_B_loss: 0.0361, G_A_loss: 0.7239, G_B_loss: 0.5951\n",
      "Epoch [7/30], Step [31/1067], D_A_loss: 0.0764, D_B_loss: 0.0333, G_A_loss: 0.6025, G_B_loss: 0.2502\n",
      "Epoch [7/30], Step [41/1067], D_A_loss: 0.1538, D_B_loss: 0.1684, G_A_loss: 0.2675, G_B_loss: 0.8372\n",
      "Epoch [7/30], Step [51/1067], D_A_loss: 0.0184, D_B_loss: 0.0651, G_A_loss: 0.6113, G_B_loss: 1.1345\n",
      "Epoch [7/30], Step [61/1067], D_A_loss: 0.1034, D_B_loss: 0.0924, G_A_loss: 1.2841, G_B_loss: 0.3310\n",
      "Epoch [7/30], Step [71/1067], D_A_loss: 0.2489, D_B_loss: 0.2661, G_A_loss: 0.4348, G_B_loss: 0.2580\n",
      "Epoch [7/30], Step [81/1067], D_A_loss: 0.0554, D_B_loss: 0.0794, G_A_loss: 0.5659, G_B_loss: 0.3359\n",
      "Epoch [7/30], Step [91/1067], D_A_loss: 0.1639, D_B_loss: 0.0588, G_A_loss: 0.7997, G_B_loss: 0.2316\n",
      "Epoch [7/30], Step [101/1067], D_A_loss: 0.0831, D_B_loss: 0.1131, G_A_loss: 0.5111, G_B_loss: 0.5516\n",
      "Epoch [7/30], Step [111/1067], D_A_loss: 0.3183, D_B_loss: 0.1117, G_A_loss: 0.6909, G_B_loss: 0.2865\n",
      "Epoch [7/30], Step [121/1067], D_A_loss: 0.1428, D_B_loss: 0.1885, G_A_loss: 0.4989, G_B_loss: 0.7797\n",
      "Epoch [7/30], Step [131/1067], D_A_loss: 0.0498, D_B_loss: 0.1363, G_A_loss: 0.8588, G_B_loss: 0.6239\n",
      "Epoch [7/30], Step [141/1067], D_A_loss: 0.1261, D_B_loss: 0.0479, G_A_loss: 0.6663, G_B_loss: 0.3359\n",
      "Epoch [7/30], Step [151/1067], D_A_loss: 0.2344, D_B_loss: 0.1964, G_A_loss: 0.6270, G_B_loss: 0.9964\n",
      "Epoch [7/30], Step [161/1067], D_A_loss: 0.1080, D_B_loss: 0.0260, G_A_loss: 0.5009, G_B_loss: 0.5275\n",
      "Epoch [7/30], Step [171/1067], D_A_loss: 0.1863, D_B_loss: 0.0275, G_A_loss: 0.7217, G_B_loss: 0.3445\n",
      "Epoch [7/30], Step [181/1067], D_A_loss: 0.1223, D_B_loss: 0.1778, G_A_loss: 0.5985, G_B_loss: 0.3489\n",
      "Epoch [7/30], Step [191/1067], D_A_loss: 0.0559, D_B_loss: 0.0350, G_A_loss: 0.4801, G_B_loss: 0.6270\n",
      "Epoch [7/30], Step [201/1067], D_A_loss: 0.3132, D_B_loss: 0.2720, G_A_loss: 0.5895, G_B_loss: 0.1940\n",
      "Epoch [7/30], Step [211/1067], D_A_loss: 0.4172, D_B_loss: 0.0835, G_A_loss: 0.8659, G_B_loss: 0.2988\n",
      "Epoch [7/30], Step [221/1067], D_A_loss: 0.1558, D_B_loss: 0.1277, G_A_loss: 0.3859, G_B_loss: 0.5245\n",
      "Epoch [7/30], Step [231/1067], D_A_loss: 0.1061, D_B_loss: 0.0865, G_A_loss: 0.7364, G_B_loss: 0.4001\n",
      "Epoch [7/30], Step [241/1067], D_A_loss: 0.1565, D_B_loss: 0.3273, G_A_loss: 0.3789, G_B_loss: 0.5915\n",
      "Epoch [7/30], Step [251/1067], D_A_loss: 0.2232, D_B_loss: 0.1235, G_A_loss: 0.4974, G_B_loss: 0.1424\n",
      "Epoch [7/30], Step [261/1067], D_A_loss: 0.1090, D_B_loss: 0.0992, G_A_loss: 0.5406, G_B_loss: 0.7508\n",
      "Epoch [7/30], Step [271/1067], D_A_loss: 0.1776, D_B_loss: 0.0323, G_A_loss: 0.6834, G_B_loss: 0.6593\n",
      "Epoch [7/30], Step [281/1067], D_A_loss: 0.2657, D_B_loss: 0.0320, G_A_loss: 0.5040, G_B_loss: 0.1684\n",
      "Epoch [7/30], Step [291/1067], D_A_loss: 0.2287, D_B_loss: 0.1158, G_A_loss: 0.4179, G_B_loss: 0.8355\n",
      "Epoch [7/30], Step [301/1067], D_A_loss: 0.2725, D_B_loss: 0.0511, G_A_loss: 1.2471, G_B_loss: 0.3912\n",
      "Epoch [7/30], Step [311/1067], D_A_loss: 0.1360, D_B_loss: 0.1139, G_A_loss: 0.3603, G_B_loss: 0.7546\n",
      "Epoch [7/30], Step [321/1067], D_A_loss: 0.1513, D_B_loss: 0.1389, G_A_loss: 0.1956, G_B_loss: 0.1849\n",
      "Epoch [7/30], Step [331/1067], D_A_loss: 0.0837, D_B_loss: 0.1222, G_A_loss: 0.4279, G_B_loss: 0.4464\n",
      "Epoch [7/30], Step [341/1067], D_A_loss: 0.1499, D_B_loss: 0.0423, G_A_loss: 0.2675, G_B_loss: 0.3239\n",
      "Epoch [7/30], Step [351/1067], D_A_loss: 0.0642, D_B_loss: 0.0365, G_A_loss: 0.7667, G_B_loss: 0.3224\n",
      "Epoch [7/30], Step [361/1067], D_A_loss: 0.0696, D_B_loss: 0.2278, G_A_loss: 1.0967, G_B_loss: 0.1701\n",
      "Epoch [7/30], Step [371/1067], D_A_loss: 0.2292, D_B_loss: 0.1413, G_A_loss: 0.4023, G_B_loss: 0.4222\n",
      "Epoch [7/30], Step [381/1067], D_A_loss: 0.0568, D_B_loss: 0.0976, G_A_loss: 0.2892, G_B_loss: 0.9768\n",
      "Epoch [7/30], Step [391/1067], D_A_loss: 0.2511, D_B_loss: 0.6238, G_A_loss: 0.6891, G_B_loss: 0.2633\n",
      "Epoch [7/30], Step [401/1067], D_A_loss: 0.3690, D_B_loss: 0.0361, G_A_loss: 0.8387, G_B_loss: 0.0555\n",
      "Epoch [7/30], Step [411/1067], D_A_loss: 0.0971, D_B_loss: 0.0967, G_A_loss: 0.2026, G_B_loss: 0.2964\n",
      "Epoch [7/30], Step [421/1067], D_A_loss: 0.0229, D_B_loss: 0.0572, G_A_loss: 0.5684, G_B_loss: 0.7590\n",
      "Epoch [7/30], Step [431/1067], D_A_loss: 0.0677, D_B_loss: 0.0962, G_A_loss: 0.5420, G_B_loss: 0.0783\n",
      "Epoch [7/30], Step [441/1067], D_A_loss: 0.1200, D_B_loss: 0.1439, G_A_loss: 0.3359, G_B_loss: 0.7173\n",
      "Epoch [7/30], Step [451/1067], D_A_loss: 0.0692, D_B_loss: 0.1369, G_A_loss: 0.2930, G_B_loss: 0.8205\n",
      "Epoch [7/30], Step [461/1067], D_A_loss: 0.1854, D_B_loss: 0.0934, G_A_loss: 0.4504, G_B_loss: 0.1789\n",
      "Epoch [7/30], Step [471/1067], D_A_loss: 0.1339, D_B_loss: 0.0230, G_A_loss: 0.9034, G_B_loss: 0.3069\n",
      "Epoch [7/30], Step [481/1067], D_A_loss: 0.2630, D_B_loss: 0.3500, G_A_loss: 1.1088, G_B_loss: 0.1112\n",
      "Epoch [7/30], Step [491/1067], D_A_loss: 0.0614, D_B_loss: 0.2149, G_A_loss: 0.1570, G_B_loss: 0.7188\n",
      "Epoch [7/30], Step [501/1067], D_A_loss: 0.1504, D_B_loss: 0.1358, G_A_loss: 0.6093, G_B_loss: 0.5187\n",
      "Epoch [7/30], Step [511/1067], D_A_loss: 0.0935, D_B_loss: 0.2162, G_A_loss: 0.1822, G_B_loss: 0.4019\n",
      "Epoch [7/30], Step [521/1067], D_A_loss: 0.0902, D_B_loss: 0.1436, G_A_loss: 0.4406, G_B_loss: 0.3383\n",
      "Epoch [7/30], Step [531/1067], D_A_loss: 0.0266, D_B_loss: 0.1196, G_A_loss: 0.9278, G_B_loss: 0.4216\n",
      "Epoch [7/30], Step [541/1067], D_A_loss: 0.1100, D_B_loss: 0.4738, G_A_loss: 0.0688, G_B_loss: 0.5391\n",
      "Epoch [7/30], Step [551/1067], D_A_loss: 0.1060, D_B_loss: 0.1531, G_A_loss: 1.1162, G_B_loss: 0.5519\n",
      "Epoch [7/30], Step [561/1067], D_A_loss: 0.1075, D_B_loss: 0.1622, G_A_loss: 0.3679, G_B_loss: 0.3184\n",
      "Epoch [7/30], Step [571/1067], D_A_loss: 0.1579, D_B_loss: 0.0307, G_A_loss: 0.4304, G_B_loss: 0.4390\n",
      "Epoch [7/30], Step [581/1067], D_A_loss: 0.2505, D_B_loss: 0.0666, G_A_loss: 0.5696, G_B_loss: 0.3047\n",
      "Epoch [7/30], Step [591/1067], D_A_loss: 0.1437, D_B_loss: 0.1816, G_A_loss: 0.3290, G_B_loss: 0.2837\n",
      "Epoch [7/30], Step [601/1067], D_A_loss: 0.1424, D_B_loss: 0.1005, G_A_loss: 0.3524, G_B_loss: 0.8078\n",
      "Epoch [7/30], Step [611/1067], D_A_loss: 0.1460, D_B_loss: 0.4093, G_A_loss: 0.5651, G_B_loss: 0.3410\n",
      "Epoch [7/30], Step [621/1067], D_A_loss: 0.2416, D_B_loss: 0.0987, G_A_loss: 0.2308, G_B_loss: 0.5419\n",
      "Epoch [7/30], Step [631/1067], D_A_loss: 0.1375, D_B_loss: 0.0679, G_A_loss: 0.1309, G_B_loss: 0.6575\n",
      "Epoch [7/30], Step [641/1067], D_A_loss: 0.0231, D_B_loss: 0.1473, G_A_loss: 0.5658, G_B_loss: 0.2658\n",
      "Epoch [7/30], Step [651/1067], D_A_loss: 0.1022, D_B_loss: 0.0315, G_A_loss: 0.6412, G_B_loss: 0.0883\n",
      "Epoch [7/30], Step [661/1067], D_A_loss: 0.1440, D_B_loss: 0.1430, G_A_loss: 0.4584, G_B_loss: 0.5627\n",
      "Epoch [7/30], Step [671/1067], D_A_loss: 0.2039, D_B_loss: 0.0612, G_A_loss: 0.3760, G_B_loss: 0.2298\n",
      "Epoch [7/30], Step [681/1067], D_A_loss: 0.1225, D_B_loss: 0.2705, G_A_loss: 0.3159, G_B_loss: 0.6026\n",
      "Epoch [7/30], Step [691/1067], D_A_loss: 0.0640, D_B_loss: 0.0932, G_A_loss: 0.5007, G_B_loss: 0.0963\n",
      "Epoch [7/30], Step [701/1067], D_A_loss: 0.0345, D_B_loss: 0.3752, G_A_loss: 0.0516, G_B_loss: 0.7071\n",
      "Epoch [7/30], Step [711/1067], D_A_loss: 0.2643, D_B_loss: 0.0608, G_A_loss: 0.5738, G_B_loss: 0.0968\n",
      "Epoch [7/30], Step [721/1067], D_A_loss: 0.0677, D_B_loss: 0.0807, G_A_loss: 0.6295, G_B_loss: 0.5385\n",
      "Epoch [7/30], Step [731/1067], D_A_loss: 0.0961, D_B_loss: 0.1373, G_A_loss: 0.2500, G_B_loss: 0.4901\n",
      "Epoch [7/30], Step [741/1067], D_A_loss: 0.0540, D_B_loss: 0.0799, G_A_loss: 0.7208, G_B_loss: 0.4264\n",
      "Epoch [7/30], Step [751/1067], D_A_loss: 0.2570, D_B_loss: 0.2944, G_A_loss: 1.2710, G_B_loss: 0.1276\n",
      "Epoch [7/30], Step [761/1067], D_A_loss: 0.2493, D_B_loss: 0.0659, G_A_loss: 0.2699, G_B_loss: 0.2624\n",
      "Epoch [7/30], Step [771/1067], D_A_loss: 0.0914, D_B_loss: 0.0655, G_A_loss: 0.3389, G_B_loss: 0.4920\n",
      "Epoch [7/30], Step [781/1067], D_A_loss: 0.0977, D_B_loss: 0.3309, G_A_loss: 0.1079, G_B_loss: 0.3224\n",
      "Epoch [7/30], Step [791/1067], D_A_loss: 0.1398, D_B_loss: 0.0675, G_A_loss: 0.2685, G_B_loss: 0.2890\n",
      "Epoch [7/30], Step [801/1067], D_A_loss: 0.0424, D_B_loss: 0.0483, G_A_loss: 0.5583, G_B_loss: 0.5671\n",
      "Epoch [7/30], Step [811/1067], D_A_loss: 0.1241, D_B_loss: 0.1165, G_A_loss: 0.4135, G_B_loss: 0.4117\n",
      "Epoch [7/30], Step [821/1067], D_A_loss: 0.2384, D_B_loss: 0.0302, G_A_loss: 0.4771, G_B_loss: 0.3249\n",
      "Epoch [7/30], Step [831/1067], D_A_loss: 0.2509, D_B_loss: 0.0672, G_A_loss: 0.4690, G_B_loss: 0.1475\n",
      "Epoch [7/30], Step [841/1067], D_A_loss: 0.1648, D_B_loss: 0.0443, G_A_loss: 0.1631, G_B_loss: 0.2233\n",
      "Epoch [7/30], Step [851/1067], D_A_loss: 0.1679, D_B_loss: 0.1641, G_A_loss: 0.4637, G_B_loss: 1.0677\n",
      "Epoch [7/30], Step [861/1067], D_A_loss: 0.2826, D_B_loss: 0.2357, G_A_loss: 0.9465, G_B_loss: 0.1058\n",
      "Epoch [7/30], Step [871/1067], D_A_loss: 0.0266, D_B_loss: 0.3637, G_A_loss: 0.0851, G_B_loss: 0.5164\n",
      "Epoch [7/30], Step [881/1067], D_A_loss: 0.1573, D_B_loss: 0.0275, G_A_loss: 0.5064, G_B_loss: 0.4025\n",
      "Epoch [7/30], Step [891/1067], D_A_loss: 0.1962, D_B_loss: 0.0722, G_A_loss: 0.5299, G_B_loss: 0.1890\n",
      "Epoch [7/30], Step [901/1067], D_A_loss: 0.0662, D_B_loss: 0.3139, G_A_loss: 0.7671, G_B_loss: 0.4622\n",
      "Epoch [7/30], Step [911/1067], D_A_loss: 0.0767, D_B_loss: 0.0720, G_A_loss: 0.9152, G_B_loss: 0.5191\n",
      "Epoch [7/30], Step [921/1067], D_A_loss: 0.1007, D_B_loss: 0.1370, G_A_loss: 0.2840, G_B_loss: 0.5410\n",
      "Epoch [7/30], Step [931/1067], D_A_loss: 0.1221, D_B_loss: 0.0585, G_A_loss: 0.6997, G_B_loss: 0.2949\n",
      "Epoch [7/30], Step [941/1067], D_A_loss: 0.0499, D_B_loss: 0.1018, G_A_loss: 0.4676, G_B_loss: 0.5356\n",
      "Epoch [7/30], Step [951/1067], D_A_loss: 0.0204, D_B_loss: 0.0844, G_A_loss: 0.6990, G_B_loss: 0.1263\n",
      "Epoch [7/30], Step [961/1067], D_A_loss: 0.1901, D_B_loss: 0.1342, G_A_loss: 0.3488, G_B_loss: 0.8305\n",
      "Epoch [7/30], Step [971/1067], D_A_loss: 0.1142, D_B_loss: 0.1397, G_A_loss: 1.3952, G_B_loss: 0.2723\n",
      "Epoch [7/30], Step [981/1067], D_A_loss: 0.1246, D_B_loss: 0.0881, G_A_loss: 0.1918, G_B_loss: 0.4595\n",
      "Epoch [7/30], Step [991/1067], D_A_loss: 0.0945, D_B_loss: 0.0720, G_A_loss: 0.4230, G_B_loss: 0.6821\n",
      "Epoch [7/30], Step [1001/1067], D_A_loss: 0.0662, D_B_loss: 0.1332, G_A_loss: 0.2972, G_B_loss: 0.4959\n",
      "Epoch [7/30], Step [1011/1067], D_A_loss: 0.0521, D_B_loss: 0.1465, G_A_loss: 0.2728, G_B_loss: 0.7157\n",
      "Epoch [7/30], Step [1021/1067], D_A_loss: 0.0523, D_B_loss: 0.0631, G_A_loss: 0.6895, G_B_loss: 0.8023\n",
      "Epoch [7/30], Step [1031/1067], D_A_loss: 0.0452, D_B_loss: 0.1176, G_A_loss: 0.3247, G_B_loss: 0.7152\n",
      "Epoch [7/30], Step [1041/1067], D_A_loss: 0.1048, D_B_loss: 0.2253, G_A_loss: 0.4852, G_B_loss: 0.3518\n",
      "Epoch [7/30], Step [1051/1067], D_A_loss: 0.0857, D_B_loss: 0.1580, G_A_loss: 0.3278, G_B_loss: 0.4914\n",
      "Epoch [7/30], Step [1061/1067], D_A_loss: 0.0805, D_B_loss: 0.0997, G_A_loss: 0.3803, G_B_loss: 0.1723\n",
      "Epoch [8/30], Step [1/1067], D_A_loss: 0.1375, D_B_loss: 0.0207, G_A_loss: 0.3527, G_B_loss: 0.5052\n",
      "Epoch [8/30], Step [11/1067], D_A_loss: 0.2882, D_B_loss: 0.1489, G_A_loss: 0.2606, G_B_loss: 0.1367\n",
      "Epoch [8/30], Step [21/1067], D_A_loss: 0.1018, D_B_loss: 0.1030, G_A_loss: 0.3944, G_B_loss: 0.5804\n",
      "Epoch [8/30], Step [31/1067], D_A_loss: 0.2669, D_B_loss: 0.0584, G_A_loss: 0.5284, G_B_loss: 0.5536\n",
      "Epoch [8/30], Step [41/1067], D_A_loss: 0.0904, D_B_loss: 0.0658, G_A_loss: 0.3744, G_B_loss: 0.6574\n",
      "Epoch [8/30], Step [51/1067], D_A_loss: 0.1510, D_B_loss: 0.2042, G_A_loss: 0.5968, G_B_loss: 0.2582\n",
      "Epoch [8/30], Step [61/1067], D_A_loss: 0.0500, D_B_loss: 0.2433, G_A_loss: 0.5908, G_B_loss: 0.5733\n",
      "Epoch [8/30], Step [71/1067], D_A_loss: 0.1717, D_B_loss: 0.1859, G_A_loss: 0.1995, G_B_loss: 0.2858\n",
      "Epoch [8/30], Step [81/1067], D_A_loss: 0.0921, D_B_loss: 0.0755, G_A_loss: 0.5048, G_B_loss: 0.7741\n",
      "Epoch [8/30], Step [91/1067], D_A_loss: 0.0349, D_B_loss: 0.2417, G_A_loss: 0.5765, G_B_loss: 0.5233\n",
      "Epoch [8/30], Step [101/1067], D_A_loss: 0.0688, D_B_loss: 0.1630, G_A_loss: 0.5421, G_B_loss: 0.2523\n",
      "Epoch [8/30], Step [111/1067], D_A_loss: 0.0990, D_B_loss: 0.0477, G_A_loss: 0.7447, G_B_loss: 0.3986\n",
      "Epoch [8/30], Step [121/1067], D_A_loss: 0.1703, D_B_loss: 0.2174, G_A_loss: 0.2314, G_B_loss: 0.2186\n",
      "Epoch [8/30], Step [131/1067], D_A_loss: 0.0230, D_B_loss: 0.0309, G_A_loss: 0.3744, G_B_loss: 0.5567\n",
      "Epoch [8/30], Step [141/1067], D_A_loss: 0.1640, D_B_loss: 0.0366, G_A_loss: 0.1812, G_B_loss: 0.3064\n",
      "Epoch [8/30], Step [151/1067], D_A_loss: 0.0504, D_B_loss: 0.0544, G_A_loss: 0.5875, G_B_loss: 0.3907\n",
      "Epoch [8/30], Step [161/1067], D_A_loss: 0.0859, D_B_loss: 0.1984, G_A_loss: 0.9957, G_B_loss: 0.4294\n",
      "Epoch [8/30], Step [171/1067], D_A_loss: 0.2481, D_B_loss: 0.1585, G_A_loss: 0.6176, G_B_loss: 0.2976\n",
      "Epoch [8/30], Step [181/1067], D_A_loss: 0.2157, D_B_loss: 0.0868, G_A_loss: 0.3094, G_B_loss: 0.1902\n",
      "Epoch [8/30], Step [191/1067], D_A_loss: 0.2073, D_B_loss: 0.0720, G_A_loss: 0.4841, G_B_loss: 0.4372\n",
      "Epoch [8/30], Step [201/1067], D_A_loss: 0.1305, D_B_loss: 0.0309, G_A_loss: 0.6414, G_B_loss: 0.5663\n",
      "Epoch [8/30], Step [211/1067], D_A_loss: 0.0577, D_B_loss: 0.1195, G_A_loss: 0.8489, G_B_loss: 0.5154\n",
      "Epoch [8/30], Step [221/1067], D_A_loss: 0.0755, D_B_loss: 0.0427, G_A_loss: 0.3930, G_B_loss: 0.3904\n",
      "Epoch [8/30], Step [231/1067], D_A_loss: 0.2384, D_B_loss: 0.0619, G_A_loss: 0.6122, G_B_loss: 0.3425\n",
      "Epoch [8/30], Step [241/1067], D_A_loss: 0.0597, D_B_loss: 0.1296, G_A_loss: 0.1036, G_B_loss: 0.5164\n",
      "Epoch [8/30], Step [251/1067], D_A_loss: 0.1251, D_B_loss: 0.1878, G_A_loss: 0.5094, G_B_loss: 0.4620\n",
      "Epoch [8/30], Step [261/1067], D_A_loss: 0.1716, D_B_loss: 0.1347, G_A_loss: 0.3956, G_B_loss: 0.2501\n",
      "Epoch [8/30], Step [271/1067], D_A_loss: 0.1319, D_B_loss: 0.1612, G_A_loss: 0.7123, G_B_loss: 0.3817\n",
      "Epoch [8/30], Step [281/1067], D_A_loss: 0.2573, D_B_loss: 0.1043, G_A_loss: 0.2313, G_B_loss: 0.1231\n",
      "Epoch [8/30], Step [291/1067], D_A_loss: 0.2067, D_B_loss: 0.1931, G_A_loss: 0.2024, G_B_loss: 0.6415\n",
      "Epoch [8/30], Step [301/1067], D_A_loss: 0.1304, D_B_loss: 0.1766, G_A_loss: 0.9233, G_B_loss: 0.4776\n",
      "Epoch [8/30], Step [311/1067], D_A_loss: 0.4468, D_B_loss: 0.1489, G_A_loss: 0.3480, G_B_loss: 0.0305\n",
      "Epoch [8/30], Step [321/1067], D_A_loss: 0.1552, D_B_loss: 0.2857, G_A_loss: 0.2159, G_B_loss: 0.4683\n",
      "Epoch [8/30], Step [331/1067], D_A_loss: 0.0928, D_B_loss: 0.0623, G_A_loss: 0.4643, G_B_loss: 0.2801\n",
      "Epoch [8/30], Step [341/1067], D_A_loss: 0.2319, D_B_loss: 0.0854, G_A_loss: 0.3368, G_B_loss: 0.5834\n",
      "Epoch [8/30], Step [351/1067], D_A_loss: 0.2717, D_B_loss: 0.1908, G_A_loss: 0.2565, G_B_loss: 0.7684\n",
      "Epoch [8/30], Step [361/1067], D_A_loss: 0.2955, D_B_loss: 0.0789, G_A_loss: 0.0504, G_B_loss: 0.3867\n",
      "Epoch [8/30], Step [371/1067], D_A_loss: 0.3326, D_B_loss: 0.1542, G_A_loss: 0.2670, G_B_loss: 0.3011\n",
      "Epoch [8/30], Step [381/1067], D_A_loss: 0.1719, D_B_loss: 0.2255, G_A_loss: 0.2043, G_B_loss: 0.2769\n",
      "Epoch [8/30], Step [391/1067], D_A_loss: 0.2575, D_B_loss: 0.0525, G_A_loss: 0.7869, G_B_loss: 0.9413\n",
      "Epoch [8/30], Step [401/1067], D_A_loss: 0.1776, D_B_loss: 0.0421, G_A_loss: 0.8160, G_B_loss: 0.4859\n",
      "Epoch [8/30], Step [411/1067], D_A_loss: 0.0677, D_B_loss: 0.0496, G_A_loss: 0.6071, G_B_loss: 0.5166\n",
      "Epoch [8/30], Step [421/1067], D_A_loss: 0.3188, D_B_loss: 0.0987, G_A_loss: 0.7860, G_B_loss: 0.6877\n",
      "Epoch [8/30], Step [431/1067], D_A_loss: 0.2599, D_B_loss: 0.1291, G_A_loss: 0.6689, G_B_loss: 0.9436\n",
      "Epoch [8/30], Step [441/1067], D_A_loss: 0.1004, D_B_loss: 0.0813, G_A_loss: 0.7664, G_B_loss: 0.4494\n",
      "Epoch [8/30], Step [451/1067], D_A_loss: 0.1766, D_B_loss: 0.0550, G_A_loss: 0.5448, G_B_loss: 0.2609\n",
      "Epoch [8/30], Step [461/1067], D_A_loss: 0.1750, D_B_loss: 0.2971, G_A_loss: 0.0871, G_B_loss: 1.1668\n",
      "Epoch [8/30], Step [471/1067], D_A_loss: 0.0756, D_B_loss: 0.1209, G_A_loss: 0.9326, G_B_loss: 0.5399\n",
      "Epoch [8/30], Step [481/1067], D_A_loss: 0.1890, D_B_loss: 0.2353, G_A_loss: 0.1649, G_B_loss: 0.2584\n",
      "Epoch [8/30], Step [491/1067], D_A_loss: 0.0558, D_B_loss: 0.1086, G_A_loss: 0.6579, G_B_loss: 0.4871\n",
      "Epoch [8/30], Step [501/1067], D_A_loss: 0.1537, D_B_loss: 0.0697, G_A_loss: 0.4498, G_B_loss: 0.6923\n",
      "Epoch [8/30], Step [511/1067], D_A_loss: 0.1362, D_B_loss: 0.1133, G_A_loss: 0.3874, G_B_loss: 0.4906\n",
      "Epoch [8/30], Step [521/1067], D_A_loss: 0.1095, D_B_loss: 0.1152, G_A_loss: 0.5194, G_B_loss: 0.4446\n",
      "Epoch [8/30], Step [531/1067], D_A_loss: 0.0961, D_B_loss: 0.0452, G_A_loss: 0.4581, G_B_loss: 0.3482\n",
      "Epoch [8/30], Step [541/1067], D_A_loss: 0.1975, D_B_loss: 0.0748, G_A_loss: 1.0699, G_B_loss: 1.0871\n",
      "Epoch [8/30], Step [551/1067], D_A_loss: 0.4141, D_B_loss: 0.2290, G_A_loss: 0.7952, G_B_loss: 1.1021\n",
      "Epoch [8/30], Step [561/1067], D_A_loss: 0.2137, D_B_loss: 0.0549, G_A_loss: 0.6265, G_B_loss: 0.2138\n",
      "Epoch [8/30], Step [571/1067], D_A_loss: 0.1863, D_B_loss: 0.0529, G_A_loss: 0.2676, G_B_loss: 0.2757\n",
      "Epoch [8/30], Step [581/1067], D_A_loss: 0.0907, D_B_loss: 0.1301, G_A_loss: 0.5491, G_B_loss: 0.4425\n",
      "Epoch [8/30], Step [591/1067], D_A_loss: 0.0248, D_B_loss: 0.1512, G_A_loss: 0.8191, G_B_loss: 0.4117\n",
      "Epoch [8/30], Step [601/1067], D_A_loss: 0.0959, D_B_loss: 0.0481, G_A_loss: 0.6044, G_B_loss: 0.1782\n",
      "Epoch [8/30], Step [611/1067], D_A_loss: 0.1222, D_B_loss: 0.0584, G_A_loss: 0.5952, G_B_loss: 0.4313\n",
      "Epoch [8/30], Step [621/1067], D_A_loss: 0.0892, D_B_loss: 0.0434, G_A_loss: 0.3330, G_B_loss: 0.4264\n",
      "Epoch [8/30], Step [631/1067], D_A_loss: 0.2145, D_B_loss: 0.1855, G_A_loss: 0.3921, G_B_loss: 0.2559\n",
      "Epoch [8/30], Step [641/1067], D_A_loss: 0.0938, D_B_loss: 0.0922, G_A_loss: 1.1601, G_B_loss: 1.5173\n",
      "Epoch [8/30], Step [651/1067], D_A_loss: 0.2510, D_B_loss: 0.1380, G_A_loss: 0.4497, G_B_loss: 0.7161\n",
      "Epoch [8/30], Step [661/1067], D_A_loss: 0.0886, D_B_loss: 0.1430, G_A_loss: 1.0175, G_B_loss: 0.3073\n",
      "Epoch [8/30], Step [671/1067], D_A_loss: 0.0797, D_B_loss: 0.0600, G_A_loss: 0.5190, G_B_loss: 0.7541\n",
      "Epoch [8/30], Step [681/1067], D_A_loss: 0.0272, D_B_loss: 0.0263, G_A_loss: 0.6600, G_B_loss: 0.7546\n",
      "Epoch [8/30], Step [691/1067], D_A_loss: 0.0236, D_B_loss: 0.1179, G_A_loss: 0.3113, G_B_loss: 0.5394\n",
      "Epoch [8/30], Step [701/1067], D_A_loss: 0.1231, D_B_loss: 0.0449, G_A_loss: 1.1197, G_B_loss: 0.6059\n",
      "Epoch [8/30], Step [711/1067], D_A_loss: 0.0377, D_B_loss: 0.0269, G_A_loss: 0.6334, G_B_loss: 0.5160\n",
      "Epoch [8/30], Step [721/1067], D_A_loss: 0.2833, D_B_loss: 0.1065, G_A_loss: 0.4607, G_B_loss: 1.2320\n",
      "Epoch [8/30], Step [731/1067], D_A_loss: 0.2012, D_B_loss: 0.0857, G_A_loss: 0.5055, G_B_loss: 0.3768\n",
      "Epoch [8/30], Step [741/1067], D_A_loss: 0.0550, D_B_loss: 0.2164, G_A_loss: 0.1956, G_B_loss: 0.3384\n",
      "Epoch [8/30], Step [751/1067], D_A_loss: 0.0549, D_B_loss: 0.0426, G_A_loss: 0.8045, G_B_loss: 0.0793\n",
      "Epoch [8/30], Step [761/1067], D_A_loss: 0.2056, D_B_loss: 0.0241, G_A_loss: 0.4795, G_B_loss: 0.3523\n",
      "Epoch [8/30], Step [771/1067], D_A_loss: 0.0804, D_B_loss: 0.0243, G_A_loss: 0.3270, G_B_loss: 0.1706\n",
      "Epoch [8/30], Step [781/1067], D_A_loss: 0.1805, D_B_loss: 0.0968, G_A_loss: 0.4193, G_B_loss: 0.3580\n",
      "Epoch [8/30], Step [791/1067], D_A_loss: 0.0251, D_B_loss: 0.1704, G_A_loss: 1.0876, G_B_loss: 0.3689\n",
      "Epoch [8/30], Step [801/1067], D_A_loss: 0.0749, D_B_loss: 0.1515, G_A_loss: 0.6579, G_B_loss: 0.4323\n",
      "Epoch [8/30], Step [811/1067], D_A_loss: 0.0665, D_B_loss: 0.1252, G_A_loss: 0.3353, G_B_loss: 0.0715\n",
      "Epoch [8/30], Step [821/1067], D_A_loss: 0.1623, D_B_loss: 0.3688, G_A_loss: 1.4368, G_B_loss: 0.2217\n",
      "Epoch [8/30], Step [831/1067], D_A_loss: 0.2460, D_B_loss: 0.0221, G_A_loss: 0.4815, G_B_loss: 0.3497\n",
      "Epoch [8/30], Step [841/1067], D_A_loss: 0.0898, D_B_loss: 0.2759, G_A_loss: 0.5531, G_B_loss: 0.3875\n",
      "Epoch [8/30], Step [851/1067], D_A_loss: 0.2101, D_B_loss: 0.1251, G_A_loss: 0.6794, G_B_loss: 0.1668\n",
      "Epoch [8/30], Step [861/1067], D_A_loss: 0.3470, D_B_loss: 0.1610, G_A_loss: 0.2283, G_B_loss: 0.5757\n",
      "Epoch [8/30], Step [871/1067], D_A_loss: 0.2767, D_B_loss: 0.0403, G_A_loss: 0.3267, G_B_loss: 0.2928\n",
      "Epoch [8/30], Step [881/1067], D_A_loss: 0.0539, D_B_loss: 0.0355, G_A_loss: 0.5147, G_B_loss: 0.4720\n",
      "Epoch [8/30], Step [891/1067], D_A_loss: 0.2536, D_B_loss: 0.1748, G_A_loss: 0.1796, G_B_loss: 0.4912\n",
      "Epoch [8/30], Step [901/1067], D_A_loss: 0.0598, D_B_loss: 0.0856, G_A_loss: 0.5683, G_B_loss: 0.4064\n",
      "Epoch [8/30], Step [911/1067], D_A_loss: 0.3190, D_B_loss: 0.2606, G_A_loss: 0.6506, G_B_loss: 0.0584\n",
      "Epoch [8/30], Step [921/1067], D_A_loss: 0.1934, D_B_loss: 0.1188, G_A_loss: 0.3530, G_B_loss: 0.2027\n",
      "Epoch [8/30], Step [931/1067], D_A_loss: 0.1424, D_B_loss: 0.0470, G_A_loss: 0.2511, G_B_loss: 0.2649\n",
      "Epoch [8/30], Step [941/1067], D_A_loss: 0.0467, D_B_loss: 0.1054, G_A_loss: 0.5494, G_B_loss: 0.4088\n",
      "Epoch [8/30], Step [951/1067], D_A_loss: 0.1246, D_B_loss: 0.0773, G_A_loss: 0.4896, G_B_loss: 0.8665\n",
      "Epoch [8/30], Step [961/1067], D_A_loss: 0.5020, D_B_loss: 0.0428, G_A_loss: 0.3621, G_B_loss: 0.8168\n",
      "Epoch [8/30], Step [971/1067], D_A_loss: 0.1618, D_B_loss: 0.0854, G_A_loss: 0.4256, G_B_loss: 0.3427\n",
      "Epoch [8/30], Step [981/1067], D_A_loss: 0.0687, D_B_loss: 0.1159, G_A_loss: 0.8139, G_B_loss: 0.2835\n",
      "Epoch [8/30], Step [991/1067], D_A_loss: 0.1230, D_B_loss: 0.1609, G_A_loss: 0.2977, G_B_loss: 0.2271\n",
      "Epoch [8/30], Step [1001/1067], D_A_loss: 0.0441, D_B_loss: 0.0663, G_A_loss: 1.1871, G_B_loss: 0.3692\n",
      "Epoch [8/30], Step [1011/1067], D_A_loss: 0.2357, D_B_loss: 0.0898, G_A_loss: 0.2197, G_B_loss: 0.2175\n",
      "Epoch [8/30], Step [1021/1067], D_A_loss: 0.0367, D_B_loss: 0.0879, G_A_loss: 0.6120, G_B_loss: 0.2672\n",
      "Epoch [8/30], Step [1031/1067], D_A_loss: 0.1266, D_B_loss: 0.1044, G_A_loss: 0.8826, G_B_loss: 0.2912\n",
      "Epoch [8/30], Step [1041/1067], D_A_loss: 0.2258, D_B_loss: 0.0636, G_A_loss: 0.3900, G_B_loss: 0.1381\n",
      "Epoch [8/30], Step [1051/1067], D_A_loss: 0.3695, D_B_loss: 0.0350, G_A_loss: 0.4398, G_B_loss: 0.8363\n",
      "Epoch [8/30], Step [1061/1067], D_A_loss: 0.0535, D_B_loss: 0.0579, G_A_loss: 0.7397, G_B_loss: 0.4219\n",
      "Epoch [9/30], Step [1/1067], D_A_loss: 0.2248, D_B_loss: 0.0709, G_A_loss: 0.5569, G_B_loss: 0.3860\n",
      "Epoch [9/30], Step [11/1067], D_A_loss: 0.0165, D_B_loss: 0.0342, G_A_loss: 0.8542, G_B_loss: 0.9968\n",
      "Epoch [9/30], Step [21/1067], D_A_loss: 0.3798, D_B_loss: 0.0902, G_A_loss: 0.6035, G_B_loss: 1.0534\n",
      "Epoch [9/30], Step [31/1067], D_A_loss: 0.2978, D_B_loss: 0.0367, G_A_loss: 0.4176, G_B_loss: 0.1140\n",
      "Epoch [9/30], Step [41/1067], D_A_loss: 0.0539, D_B_loss: 0.1442, G_A_loss: 0.2890, G_B_loss: 0.5692\n",
      "Epoch [9/30], Step [51/1067], D_A_loss: 0.2252, D_B_loss: 0.0916, G_A_loss: 0.1886, G_B_loss: 0.2449\n",
      "Epoch [9/30], Step [61/1067], D_A_loss: 0.2093, D_B_loss: 0.1142, G_A_loss: 0.4267, G_B_loss: 0.1513\n",
      "Epoch [9/30], Step [71/1067], D_A_loss: 0.1668, D_B_loss: 0.0424, G_A_loss: 0.4858, G_B_loss: 0.2259\n",
      "Epoch [9/30], Step [81/1067], D_A_loss: 0.1045, D_B_loss: 0.1568, G_A_loss: 0.2376, G_B_loss: 0.4827\n",
      "Epoch [9/30], Step [91/1067], D_A_loss: 0.0766, D_B_loss: 0.1358, G_A_loss: 0.2837, G_B_loss: 0.4663\n",
      "Epoch [9/30], Step [101/1067], D_A_loss: 0.1703, D_B_loss: 0.1947, G_A_loss: 0.1737, G_B_loss: 0.1844\n",
      "Epoch [9/30], Step [111/1067], D_A_loss: 0.2322, D_B_loss: 0.0562, G_A_loss: 0.3256, G_B_loss: 0.5462\n",
      "Epoch [9/30], Step [121/1067], D_A_loss: 0.1268, D_B_loss: 0.1521, G_A_loss: 0.6385, G_B_loss: 0.5994\n",
      "Epoch [9/30], Step [131/1067], D_A_loss: 0.1655, D_B_loss: 0.0307, G_A_loss: 1.0272, G_B_loss: 0.2773\n",
      "Epoch [9/30], Step [141/1067], D_A_loss: 0.2148, D_B_loss: 0.1012, G_A_loss: 0.3601, G_B_loss: 0.9188\n",
      "Epoch [9/30], Step [151/1067], D_A_loss: 0.1857, D_B_loss: 0.1455, G_A_loss: 0.2675, G_B_loss: 0.0359\n",
      "Epoch [9/30], Step [161/1067], D_A_loss: 0.1742, D_B_loss: 0.0383, G_A_loss: 1.1145, G_B_loss: 0.3392\n",
      "Epoch [9/30], Step [171/1067], D_A_loss: 0.0659, D_B_loss: 0.0871, G_A_loss: 0.9075, G_B_loss: 0.4897\n",
      "Epoch [9/30], Step [181/1067], D_A_loss: 0.1232, D_B_loss: 0.1278, G_A_loss: 0.7428, G_B_loss: 0.9220\n",
      "Epoch [9/30], Step [191/1067], D_A_loss: 0.1190, D_B_loss: 0.0541, G_A_loss: 0.5514, G_B_loss: 0.3440\n",
      "Epoch [9/30], Step [201/1067], D_A_loss: 0.0573, D_B_loss: 0.0846, G_A_loss: 0.4563, G_B_loss: 0.5748\n",
      "Epoch [9/30], Step [211/1067], D_A_loss: 0.0475, D_B_loss: 0.0268, G_A_loss: 0.6993, G_B_loss: 0.3027\n",
      "Epoch [9/30], Step [221/1067], D_A_loss: 0.0911, D_B_loss: 0.4882, G_A_loss: 1.3075, G_B_loss: 0.3263\n",
      "Epoch [9/30], Step [231/1067], D_A_loss: 0.3542, D_B_loss: 0.0967, G_A_loss: 0.5242, G_B_loss: 0.2899\n",
      "Epoch [9/30], Step [241/1067], D_A_loss: 0.0391, D_B_loss: 0.0501, G_A_loss: 0.6238, G_B_loss: 1.2358\n",
      "Epoch [9/30], Step [251/1067], D_A_loss: 0.1631, D_B_loss: 0.0726, G_A_loss: 0.5518, G_B_loss: 0.2799\n",
      "Epoch [9/30], Step [261/1067], D_A_loss: 0.1938, D_B_loss: 0.0349, G_A_loss: 0.5552, G_B_loss: 0.2797\n",
      "Epoch [9/30], Step [271/1067], D_A_loss: 0.0887, D_B_loss: 0.0506, G_A_loss: 0.6516, G_B_loss: 0.6628\n",
      "Epoch [9/30], Step [281/1067], D_A_loss: 0.0240, D_B_loss: 0.1858, G_A_loss: 0.4444, G_B_loss: 0.8113\n",
      "Epoch [9/30], Step [291/1067], D_A_loss: 0.0774, D_B_loss: 0.2132, G_A_loss: 0.2355, G_B_loss: 0.1986\n",
      "Epoch [9/30], Step [301/1067], D_A_loss: 0.0525, D_B_loss: 0.0593, G_A_loss: 0.6121, G_B_loss: 0.4472\n",
      "Epoch [9/30], Step [311/1067], D_A_loss: 0.0795, D_B_loss: 0.0421, G_A_loss: 0.7217, G_B_loss: 0.5726\n",
      "Epoch [9/30], Step [321/1067], D_A_loss: 0.1325, D_B_loss: 0.0734, G_A_loss: 0.6630, G_B_loss: 0.3372\n",
      "Epoch [9/30], Step [331/1067], D_A_loss: 0.3760, D_B_loss: 0.0455, G_A_loss: 0.6463, G_B_loss: 0.9212\n",
      "Epoch [9/30], Step [341/1067], D_A_loss: 0.2420, D_B_loss: 0.0883, G_A_loss: 1.2097, G_B_loss: 0.3129\n",
      "Epoch [9/30], Step [351/1067], D_A_loss: 0.0211, D_B_loss: 0.1021, G_A_loss: 0.3936, G_B_loss: 0.4846\n",
      "Epoch [9/30], Step [361/1067], D_A_loss: 0.1215, D_B_loss: 0.0225, G_A_loss: 0.8157, G_B_loss: 0.9450\n",
      "Epoch [9/30], Step [371/1067], D_A_loss: 0.2330, D_B_loss: 0.0710, G_A_loss: 1.3108, G_B_loss: 0.6310\n",
      "Epoch [9/30], Step [381/1067], D_A_loss: 0.3502, D_B_loss: 0.0962, G_A_loss: 1.2030, G_B_loss: 0.1901\n",
      "Epoch [9/30], Step [391/1067], D_A_loss: 0.1851, D_B_loss: 0.0475, G_A_loss: 0.4292, G_B_loss: 0.2915\n",
      "Epoch [9/30], Step [401/1067], D_A_loss: 0.1260, D_B_loss: 0.1987, G_A_loss: 0.1937, G_B_loss: 0.4180\n",
      "Epoch [9/30], Step [411/1067], D_A_loss: 0.1400, D_B_loss: 0.0381, G_A_loss: 0.4332, G_B_loss: 0.2998\n",
      "Epoch [9/30], Step [421/1067], D_A_loss: 0.1092, D_B_loss: 0.1533, G_A_loss: 0.5622, G_B_loss: 0.5788\n",
      "Epoch [9/30], Step [431/1067], D_A_loss: 0.0883, D_B_loss: 0.1125, G_A_loss: 0.3574, G_B_loss: 0.4914\n",
      "Epoch [9/30], Step [441/1067], D_A_loss: 0.2047, D_B_loss: 0.0581, G_A_loss: 0.8657, G_B_loss: 0.8684\n",
      "Epoch [9/30], Step [451/1067], D_A_loss: 0.0674, D_B_loss: 0.0443, G_A_loss: 0.4430, G_B_loss: 0.2665\n",
      "Epoch [9/30], Step [461/1067], D_A_loss: 0.1088, D_B_loss: 0.1943, G_A_loss: 0.2238, G_B_loss: 0.7400\n",
      "Epoch [9/30], Step [471/1067], D_A_loss: 0.1729, D_B_loss: 0.1612, G_A_loss: 1.1192, G_B_loss: 0.5572\n",
      "Epoch [9/30], Step [481/1067], D_A_loss: 0.5673, D_B_loss: 0.0607, G_A_loss: 0.2380, G_B_loss: 0.9250\n",
      "Epoch [9/30], Step [491/1067], D_A_loss: 0.0523, D_B_loss: 0.2191, G_A_loss: 0.4509, G_B_loss: 0.5458\n",
      "Epoch [9/30], Step [501/1067], D_A_loss: 0.2729, D_B_loss: 0.1894, G_A_loss: 1.0417, G_B_loss: 0.5127\n",
      "Epoch [9/30], Step [511/1067], D_A_loss: 0.0382, D_B_loss: 0.1407, G_A_loss: 0.6912, G_B_loss: 0.1026\n",
      "Epoch [9/30], Step [521/1067], D_A_loss: 0.1275, D_B_loss: 0.1290, G_A_loss: 0.1964, G_B_loss: 0.3607\n",
      "Epoch [9/30], Step [531/1067], D_A_loss: 0.0658, D_B_loss: 0.1525, G_A_loss: 0.8638, G_B_loss: 0.5073\n",
      "Epoch [9/30], Step [541/1067], D_A_loss: 0.1691, D_B_loss: 0.1025, G_A_loss: 0.6195, G_B_loss: 0.4918\n",
      "Epoch [9/30], Step [551/1067], D_A_loss: 0.1951, D_B_loss: 0.0973, G_A_loss: 0.8251, G_B_loss: 0.1915\n",
      "Epoch [9/30], Step [561/1067], D_A_loss: 0.1989, D_B_loss: 0.0963, G_A_loss: 0.3946, G_B_loss: 0.5747\n",
      "Epoch [9/30], Step [571/1067], D_A_loss: 0.0809, D_B_loss: 0.0534, G_A_loss: 0.8851, G_B_loss: 0.5438\n",
      "Epoch [9/30], Step [581/1067], D_A_loss: 0.1330, D_B_loss: 0.1125, G_A_loss: 0.5482, G_B_loss: 0.3538\n",
      "Epoch [9/30], Step [591/1067], D_A_loss: 0.1398, D_B_loss: 0.2151, G_A_loss: 1.0318, G_B_loss: 0.3035\n",
      "Epoch [9/30], Step [601/1067], D_A_loss: 0.1684, D_B_loss: 0.0316, G_A_loss: 0.7625, G_B_loss: 0.6258\n",
      "Epoch [9/30], Step [611/1067], D_A_loss: 0.3470, D_B_loss: 0.0664, G_A_loss: 1.0096, G_B_loss: 0.0534\n",
      "Epoch [9/30], Step [621/1067], D_A_loss: 0.0208, D_B_loss: 0.2446, G_A_loss: 0.9911, G_B_loss: 0.4954\n",
      "Epoch [9/30], Step [631/1067], D_A_loss: 0.1254, D_B_loss: 0.1194, G_A_loss: 0.1855, G_B_loss: 0.3886\n",
      "Epoch [9/30], Step [641/1067], D_A_loss: 0.1774, D_B_loss: 0.0545, G_A_loss: 0.5243, G_B_loss: 0.5231\n",
      "Epoch [9/30], Step [651/1067], D_A_loss: 0.0450, D_B_loss: 0.0754, G_A_loss: 0.3889, G_B_loss: 0.3230\n",
      "Epoch [9/30], Step [661/1067], D_A_loss: 0.1012, D_B_loss: 0.1972, G_A_loss: 0.8755, G_B_loss: 0.4186\n",
      "Epoch [9/30], Step [671/1067], D_A_loss: 0.5211, D_B_loss: 0.1118, G_A_loss: 0.3829, G_B_loss: 0.9539\n",
      "Epoch [9/30], Step [681/1067], D_A_loss: 0.0926, D_B_loss: 0.0287, G_A_loss: 0.3940, G_B_loss: 0.2556\n",
      "Epoch [9/30], Step [691/1067], D_A_loss: 0.0473, D_B_loss: 0.0308, G_A_loss: 0.8608, G_B_loss: 0.3492\n",
      "Epoch [9/30], Step [701/1067], D_A_loss: 0.0421, D_B_loss: 0.0958, G_A_loss: 0.7433, G_B_loss: 0.4884\n",
      "Epoch [9/30], Step [711/1067], D_A_loss: 0.2460, D_B_loss: 0.0407, G_A_loss: 0.2084, G_B_loss: 0.1576\n",
      "Epoch [9/30], Step [721/1067], D_A_loss: 0.0399, D_B_loss: 0.1508, G_A_loss: 0.2404, G_B_loss: 0.4734\n",
      "Epoch [9/30], Step [731/1067], D_A_loss: 0.1122, D_B_loss: 0.0998, G_A_loss: 0.5089, G_B_loss: 0.3535\n",
      "Epoch [9/30], Step [741/1067], D_A_loss: 0.0960, D_B_loss: 0.0963, G_A_loss: 0.3899, G_B_loss: 0.3560\n",
      "Epoch [9/30], Step [751/1067], D_A_loss: 0.1372, D_B_loss: 0.0818, G_A_loss: 0.4453, G_B_loss: 0.5875\n",
      "Epoch [9/30], Step [761/1067], D_A_loss: 0.0635, D_B_loss: 0.2266, G_A_loss: 0.1339, G_B_loss: 0.1845\n",
      "Epoch [9/30], Step [771/1067], D_A_loss: 0.1112, D_B_loss: 0.1305, G_A_loss: 0.7474, G_B_loss: 0.2648\n",
      "Epoch [9/30], Step [781/1067], D_A_loss: 0.1234, D_B_loss: 0.0642, G_A_loss: 0.6617, G_B_loss: 0.3098\n",
      "Epoch [9/30], Step [791/1067], D_A_loss: 0.3687, D_B_loss: 0.1639, G_A_loss: 0.9862, G_B_loss: 0.2849\n",
      "Epoch [9/30], Step [801/1067], D_A_loss: 0.1846, D_B_loss: 0.2478, G_A_loss: 0.2359, G_B_loss: 0.4997\n",
      "Epoch [9/30], Step [811/1067], D_A_loss: 0.1982, D_B_loss: 0.0324, G_A_loss: 0.5237, G_B_loss: 1.0022\n",
      "Epoch [9/30], Step [821/1067], D_A_loss: 0.0643, D_B_loss: 0.1947, G_A_loss: 0.2586, G_B_loss: 0.5778\n",
      "Epoch [9/30], Step [831/1067], D_A_loss: 0.0666, D_B_loss: 0.0899, G_A_loss: 0.6740, G_B_loss: 0.2468\n",
      "Epoch [9/30], Step [841/1067], D_A_loss: 0.0586, D_B_loss: 0.1997, G_A_loss: 0.1893, G_B_loss: 0.5684\n",
      "Epoch [9/30], Step [851/1067], D_A_loss: 0.0385, D_B_loss: 0.2025, G_A_loss: 0.4580, G_B_loss: 0.5488\n",
      "Epoch [9/30], Step [861/1067], D_A_loss: 0.2810, D_B_loss: 0.2928, G_A_loss: 0.1328, G_B_loss: 0.1324\n",
      "Epoch [9/30], Step [871/1067], D_A_loss: 0.0365, D_B_loss: 0.1836, G_A_loss: 0.3996, G_B_loss: 0.1983\n",
      "Epoch [9/30], Step [881/1067], D_A_loss: 0.1503, D_B_loss: 0.1591, G_A_loss: 0.2992, G_B_loss: 0.2995\n",
      "Epoch [9/30], Step [891/1067], D_A_loss: 0.0644, D_B_loss: 0.0877, G_A_loss: 0.5403, G_B_loss: 0.5043\n",
      "Epoch [9/30], Step [901/1067], D_A_loss: 0.2251, D_B_loss: 0.3918, G_A_loss: 0.5404, G_B_loss: 0.3186\n",
      "Epoch [9/30], Step [911/1067], D_A_loss: 0.2982, D_B_loss: 0.1537, G_A_loss: 0.2589, G_B_loss: 0.2213\n",
      "Epoch [9/30], Step [921/1067], D_A_loss: 0.0203, D_B_loss: 0.0664, G_A_loss: 0.9304, G_B_loss: 0.5325\n",
      "Epoch [9/30], Step [931/1067], D_A_loss: 0.1488, D_B_loss: 0.0221, G_A_loss: 0.3250, G_B_loss: 0.3568\n",
      "Epoch [9/30], Step [941/1067], D_A_loss: 0.2705, D_B_loss: 0.1426, G_A_loss: 0.5991, G_B_loss: 0.0986\n",
      "Epoch [9/30], Step [951/1067], D_A_loss: 0.0512, D_B_loss: 0.0377, G_A_loss: 0.7454, G_B_loss: 0.4749\n",
      "Epoch [9/30], Step [961/1067], D_A_loss: 0.2162, D_B_loss: 0.0262, G_A_loss: 0.5287, G_B_loss: 0.4008\n",
      "Epoch [9/30], Step [971/1067], D_A_loss: 0.1431, D_B_loss: 0.0651, G_A_loss: 0.4911, G_B_loss: 0.2982\n",
      "Epoch [9/30], Step [981/1067], D_A_loss: 0.0872, D_B_loss: 0.0628, G_A_loss: 0.9739, G_B_loss: 0.4614\n",
      "Epoch [9/30], Step [991/1067], D_A_loss: 0.3322, D_B_loss: 0.1148, G_A_loss: 0.3903, G_B_loss: 0.1110\n",
      "Epoch [9/30], Step [1001/1067], D_A_loss: 0.7196, D_B_loss: 0.0431, G_A_loss: 0.9121, G_B_loss: 0.7198\n",
      "Epoch [9/30], Step [1011/1067], D_A_loss: 0.0356, D_B_loss: 0.0433, G_A_loss: 0.7680, G_B_loss: 0.7261\n",
      "Epoch [9/30], Step [1021/1067], D_A_loss: 0.0983, D_B_loss: 0.0306, G_A_loss: 0.8639, G_B_loss: 0.3821\n",
      "Epoch [9/30], Step [1031/1067], D_A_loss: 0.2680, D_B_loss: 0.0956, G_A_loss: 0.3634, G_B_loss: 0.9165\n",
      "Epoch [9/30], Step [1041/1067], D_A_loss: 0.1862, D_B_loss: 0.0537, G_A_loss: 0.6162, G_B_loss: 0.7175\n",
      "Epoch [9/30], Step [1051/1067], D_A_loss: 0.1744, D_B_loss: 0.0489, G_A_loss: 0.1647, G_B_loss: 0.2448\n",
      "Epoch [9/30], Step [1061/1067], D_A_loss: 0.1967, D_B_loss: 0.0240, G_A_loss: 0.2005, G_B_loss: 0.6809\n",
      "Epoch [10/30], Step [1/1067], D_A_loss: 0.1139, D_B_loss: 0.2261, G_A_loss: 0.1833, G_B_loss: 0.2667\n",
      "Epoch [10/30], Step [11/1067], D_A_loss: 0.1540, D_B_loss: 0.0349, G_A_loss: 0.6379, G_B_loss: 0.3089\n",
      "Epoch [10/30], Step [21/1067], D_A_loss: 0.0670, D_B_loss: 0.0608, G_A_loss: 0.5136, G_B_loss: 0.5108\n",
      "Epoch [10/30], Step [31/1067], D_A_loss: 0.0249, D_B_loss: 0.1209, G_A_loss: 1.0282, G_B_loss: 0.7156\n",
      "Epoch [10/30], Step [41/1067], D_A_loss: 0.0532, D_B_loss: 0.0371, G_A_loss: 0.3925, G_B_loss: 0.5566\n",
      "Epoch [10/30], Step [51/1067], D_A_loss: 0.0900, D_B_loss: 0.0623, G_A_loss: 0.6322, G_B_loss: 0.6029\n",
      "Epoch [10/30], Step [61/1067], D_A_loss: 0.5486, D_B_loss: 0.1165, G_A_loss: 1.0645, G_B_loss: 0.7736\n",
      "Epoch [10/30], Step [71/1067], D_A_loss: 0.0519, D_B_loss: 0.0224, G_A_loss: 0.7142, G_B_loss: 0.6884\n",
      "Epoch [10/30], Step [81/1067], D_A_loss: 0.1451, D_B_loss: 0.0588, G_A_loss: 0.4495, G_B_loss: 0.4393\n",
      "Epoch [10/30], Step [91/1067], D_A_loss: 0.1833, D_B_loss: 0.2814, G_A_loss: 0.4147, G_B_loss: 0.6859\n",
      "Epoch [10/30], Step [101/1067], D_A_loss: 0.0593, D_B_loss: 0.0234, G_A_loss: 0.9835, G_B_loss: 0.9156\n",
      "Epoch [10/30], Step [111/1067], D_A_loss: 0.2490, D_B_loss: 0.0837, G_A_loss: 0.4978, G_B_loss: 0.5905\n",
      "Epoch [10/30], Step [121/1067], D_A_loss: 0.1244, D_B_loss: 0.0541, G_A_loss: 0.9046, G_B_loss: 0.3308\n",
      "Epoch [10/30], Step [131/1067], D_A_loss: 0.0453, D_B_loss: 0.1310, G_A_loss: 0.3341, G_B_loss: 0.6116\n",
      "Epoch [10/30], Step [141/1067], D_A_loss: 0.0710, D_B_loss: 0.2151, G_A_loss: 0.3276, G_B_loss: 0.1162\n",
      "Epoch [10/30], Step [151/1067], D_A_loss: 0.1274, D_B_loss: 0.1173, G_A_loss: 0.3706, G_B_loss: 0.9075\n",
      "Epoch [10/30], Step [161/1067], D_A_loss: 0.0633, D_B_loss: 0.0363, G_A_loss: 1.0238, G_B_loss: 0.2444\n",
      "Epoch [10/30], Step [171/1067], D_A_loss: 0.2184, D_B_loss: 0.0337, G_A_loss: 0.4452, G_B_loss: 0.0850\n",
      "Epoch [10/30], Step [181/1067], D_A_loss: 0.0481, D_B_loss: 0.1015, G_A_loss: 0.5288, G_B_loss: 0.5600\n",
      "Epoch [10/30], Step [191/1067], D_A_loss: 0.0701, D_B_loss: 0.1331, G_A_loss: 0.4522, G_B_loss: 0.5142\n",
      "Epoch [10/30], Step [201/1067], D_A_loss: 0.1409, D_B_loss: 0.0553, G_A_loss: 0.6895, G_B_loss: 0.5198\n",
      "Epoch [10/30], Step [211/1067], D_A_loss: 0.0528, D_B_loss: 0.0300, G_A_loss: 1.1524, G_B_loss: 0.5655\n",
      "Epoch [10/30], Step [221/1067], D_A_loss: 0.0590, D_B_loss: 0.0299, G_A_loss: 0.5556, G_B_loss: 0.4963\n",
      "Epoch [10/30], Step [231/1067], D_A_loss: 0.0174, D_B_loss: 0.0535, G_A_loss: 0.6215, G_B_loss: 0.8524\n",
      "Epoch [10/30], Step [241/1067], D_A_loss: 0.0714, D_B_loss: 0.0662, G_A_loss: 0.4793, G_B_loss: 0.8037\n",
      "Epoch [10/30], Step [251/1067], D_A_loss: 0.0500, D_B_loss: 0.0458, G_A_loss: 0.7433, G_B_loss: 0.8524\n",
      "Epoch [10/30], Step [261/1067], D_A_loss: 0.1538, D_B_loss: 0.0253, G_A_loss: 1.1465, G_B_loss: 0.2443\n",
      "Epoch [10/30], Step [271/1067], D_A_loss: 0.2602, D_B_loss: 0.0330, G_A_loss: 0.2994, G_B_loss: 0.2339\n",
      "Epoch [10/30], Step [281/1067], D_A_loss: 0.2258, D_B_loss: 0.2688, G_A_loss: 0.2322, G_B_loss: 0.6214\n",
      "Epoch [10/30], Step [291/1067], D_A_loss: 0.1431, D_B_loss: 0.0559, G_A_loss: 0.2135, G_B_loss: 0.3086\n",
      "Epoch [10/30], Step [301/1067], D_A_loss: 0.3643, D_B_loss: 0.0195, G_A_loss: 0.1749, G_B_loss: 0.4074\n",
      "Epoch [10/30], Step [311/1067], D_A_loss: 0.0552, D_B_loss: 0.0844, G_A_loss: 0.3239, G_B_loss: 0.3937\n",
      "Epoch [10/30], Step [321/1067], D_A_loss: 0.4977, D_B_loss: 0.2961, G_A_loss: 0.0826, G_B_loss: 0.0936\n",
      "Epoch [10/30], Step [331/1067], D_A_loss: 0.2420, D_B_loss: 0.1259, G_A_loss: 0.5233, G_B_loss: 0.3524\n",
      "Epoch [10/30], Step [341/1067], D_A_loss: 0.1256, D_B_loss: 0.0737, G_A_loss: 0.8285, G_B_loss: 0.3192\n",
      "Epoch [10/30], Step [351/1067], D_A_loss: 0.1243, D_B_loss: 0.0339, G_A_loss: 0.6972, G_B_loss: 0.4643\n",
      "Epoch [10/30], Step [361/1067], D_A_loss: 0.0516, D_B_loss: 0.1430, G_A_loss: 0.4294, G_B_loss: 0.5925\n",
      "Epoch [10/30], Step [371/1067], D_A_loss: 0.1202, D_B_loss: 0.0472, G_A_loss: 0.7038, G_B_loss: 0.2740\n",
      "Epoch [10/30], Step [381/1067], D_A_loss: 0.1730, D_B_loss: 0.0969, G_A_loss: 0.5352, G_B_loss: 0.4991\n",
      "Epoch [10/30], Step [391/1067], D_A_loss: 0.2601, D_B_loss: 0.0568, G_A_loss: 0.3139, G_B_loss: 0.7184\n",
      "Epoch [10/30], Step [401/1067], D_A_loss: 0.2391, D_B_loss: 0.1118, G_A_loss: 1.7396, G_B_loss: 0.1725\n",
      "Epoch [10/30], Step [411/1067], D_A_loss: 0.1198, D_B_loss: 0.0482, G_A_loss: 0.6584, G_B_loss: 0.4655\n",
      "Epoch [10/30], Step [421/1067], D_A_loss: 0.1243, D_B_loss: 0.0441, G_A_loss: 0.5687, G_B_loss: 0.7292\n",
      "Epoch [10/30], Step [431/1067], D_A_loss: 0.1212, D_B_loss: 0.0908, G_A_loss: 0.4702, G_B_loss: 0.8811\n",
      "Epoch [10/30], Step [441/1067], D_A_loss: 0.1510, D_B_loss: 0.0151, G_A_loss: 0.1413, G_B_loss: 0.4912\n",
      "Epoch [10/30], Step [451/1067], D_A_loss: 0.0834, D_B_loss: 0.0501, G_A_loss: 0.2941, G_B_loss: 0.4601\n",
      "Epoch [10/30], Step [461/1067], D_A_loss: 0.1641, D_B_loss: 0.0653, G_A_loss: 0.4820, G_B_loss: 0.6263\n",
      "Epoch [10/30], Step [471/1067], D_A_loss: 0.0440, D_B_loss: 0.0431, G_A_loss: 0.6677, G_B_loss: 0.2166\n",
      "Epoch [10/30], Step [481/1067], D_A_loss: 0.0471, D_B_loss: 0.1457, G_A_loss: 1.3855, G_B_loss: 0.7862\n",
      "Epoch [10/30], Step [491/1067], D_A_loss: 0.1844, D_B_loss: 0.0501, G_A_loss: 0.5445, G_B_loss: 0.9987\n",
      "Epoch [10/30], Step [501/1067], D_A_loss: 0.2761, D_B_loss: 0.0746, G_A_loss: 0.2613, G_B_loss: 0.7249\n",
      "Epoch [10/30], Step [511/1067], D_A_loss: 0.1743, D_B_loss: 0.1254, G_A_loss: 0.1656, G_B_loss: 0.7007\n",
      "Epoch [10/30], Step [521/1067], D_A_loss: 0.0647, D_B_loss: 0.0722, G_A_loss: 0.5323, G_B_loss: 0.3097\n",
      "Epoch [10/30], Step [531/1067], D_A_loss: 0.1919, D_B_loss: 0.0517, G_A_loss: 0.6801, G_B_loss: 0.1735\n",
      "Epoch [10/30], Step [541/1067], D_A_loss: 0.1685, D_B_loss: 0.1114, G_A_loss: 0.2162, G_B_loss: 0.4845\n",
      "Epoch [10/30], Step [551/1067], D_A_loss: 0.0638, D_B_loss: 0.0687, G_A_loss: 0.1980, G_B_loss: 0.4412\n",
      "Epoch [10/30], Step [561/1067], D_A_loss: 0.1957, D_B_loss: 0.0695, G_A_loss: 0.5396, G_B_loss: 0.2139\n",
      "Epoch [10/30], Step [571/1067], D_A_loss: 0.0673, D_B_loss: 0.0392, G_A_loss: 1.1504, G_B_loss: 0.4780\n",
      "Epoch [10/30], Step [581/1067], D_A_loss: 0.1017, D_B_loss: 0.0965, G_A_loss: 0.3845, G_B_loss: 0.3507\n",
      "Epoch [10/30], Step [591/1067], D_A_loss: 0.1250, D_B_loss: 0.2961, G_A_loss: 0.6592, G_B_loss: 0.2626\n",
      "Epoch [10/30], Step [601/1067], D_A_loss: 0.1509, D_B_loss: 0.2952, G_A_loss: 0.1800, G_B_loss: 0.4379\n",
      "Epoch [10/30], Step [611/1067], D_A_loss: 0.1103, D_B_loss: 0.0733, G_A_loss: 0.5964, G_B_loss: 0.5803\n",
      "Epoch [10/30], Step [621/1067], D_A_loss: 0.3433, D_B_loss: 0.1424, G_A_loss: 0.2679, G_B_loss: 0.2822\n",
      "Epoch [10/30], Step [631/1067], D_A_loss: 0.0908, D_B_loss: 0.1805, G_A_loss: 0.2717, G_B_loss: 0.5269\n",
      "Epoch [10/30], Step [641/1067], D_A_loss: 0.2379, D_B_loss: 0.0363, G_A_loss: 0.9002, G_B_loss: 0.5596\n",
      "Epoch [10/30], Step [651/1067], D_A_loss: 0.0316, D_B_loss: 0.0566, G_A_loss: 0.5433, G_B_loss: 0.5557\n",
      "Epoch [10/30], Step [661/1067], D_A_loss: 0.2753, D_B_loss: 0.0670, G_A_loss: 0.4806, G_B_loss: 0.3851\n",
      "Epoch [10/30], Step [671/1067], D_A_loss: 0.0951, D_B_loss: 0.0197, G_A_loss: 0.7846, G_B_loss: 0.4361\n",
      "Epoch [10/30], Step [681/1067], D_A_loss: 0.1921, D_B_loss: 0.0366, G_A_loss: 0.9154, G_B_loss: 0.4197\n",
      "Epoch [10/30], Step [691/1067], D_A_loss: 0.2182, D_B_loss: 0.0470, G_A_loss: 0.7243, G_B_loss: 0.3355\n",
      "Epoch [10/30], Step [701/1067], D_A_loss: 0.0577, D_B_loss: 0.0430, G_A_loss: 0.7150, G_B_loss: 0.2783\n",
      "Epoch [10/30], Step [711/1067], D_A_loss: 0.1212, D_B_loss: 0.1370, G_A_loss: 0.8014, G_B_loss: 0.3642\n",
      "Epoch [10/30], Step [721/1067], D_A_loss: 0.1518, D_B_loss: 0.1719, G_A_loss: 0.7836, G_B_loss: 0.1426\n",
      "Epoch [10/30], Step [731/1067], D_A_loss: 0.1268, D_B_loss: 0.0731, G_A_loss: 0.7205, G_B_loss: 0.4066\n",
      "Epoch [10/30], Step [741/1067], D_A_loss: 0.0512, D_B_loss: 0.0906, G_A_loss: 0.7620, G_B_loss: 0.3429\n",
      "Epoch [10/30], Step [751/1067], D_A_loss: 0.5061, D_B_loss: 0.1338, G_A_loss: 0.2856, G_B_loss: 0.8393\n",
      "Epoch [10/30], Step [761/1067], D_A_loss: 0.0782, D_B_loss: 0.0174, G_A_loss: 0.0311, G_B_loss: 0.5478\n",
      "Epoch [10/30], Step [771/1067], D_A_loss: 0.0496, D_B_loss: 0.1463, G_A_loss: 0.7679, G_B_loss: 0.6214\n",
      "Epoch [10/30], Step [781/1067], D_A_loss: 0.1605, D_B_loss: 0.0471, G_A_loss: 0.6020, G_B_loss: 0.2958\n",
      "Epoch [10/30], Step [791/1067], D_A_loss: 0.1304, D_B_loss: 0.0464, G_A_loss: 0.6463, G_B_loss: 0.4010\n",
      "Epoch [10/30], Step [801/1067], D_A_loss: 0.1984, D_B_loss: 0.2822, G_A_loss: 0.0851, G_B_loss: 0.4450\n",
      "Epoch [10/30], Step [811/1067], D_A_loss: 0.0536, D_B_loss: 0.0470, G_A_loss: 0.6644, G_B_loss: 0.2672\n",
      "Epoch [10/30], Step [821/1067], D_A_loss: 0.1398, D_B_loss: 0.0429, G_A_loss: 0.3034, G_B_loss: 0.3681\n",
      "Epoch [10/30], Step [831/1067], D_A_loss: 0.2249, D_B_loss: 0.1239, G_A_loss: 0.3231, G_B_loss: 0.2995\n",
      "Epoch [10/30], Step [841/1067], D_A_loss: 0.1403, D_B_loss: 0.0564, G_A_loss: 0.3699, G_B_loss: 0.5402\n",
      "Epoch [10/30], Step [851/1067], D_A_loss: 0.1203, D_B_loss: 0.0414, G_A_loss: 0.6726, G_B_loss: 0.4396\n",
      "Epoch [10/30], Step [861/1067], D_A_loss: 0.0829, D_B_loss: 0.1942, G_A_loss: 0.3383, G_B_loss: 0.4150\n",
      "Epoch [10/30], Step [871/1067], D_A_loss: 0.2190, D_B_loss: 0.1071, G_A_loss: 0.5733, G_B_loss: 0.5008\n",
      "Epoch [10/30], Step [881/1067], D_A_loss: 0.2325, D_B_loss: 0.1117, G_A_loss: 0.3907, G_B_loss: 0.6586\n",
      "Epoch [10/30], Step [891/1067], D_A_loss: 0.0578, D_B_loss: 0.1100, G_A_loss: 0.4003, G_B_loss: 0.3383\n",
      "Epoch [10/30], Step [901/1067], D_A_loss: 0.0418, D_B_loss: 0.1152, G_A_loss: 0.9102, G_B_loss: 0.4674\n",
      "Epoch [10/30], Step [911/1067], D_A_loss: 0.2333, D_B_loss: 0.2846, G_A_loss: 0.0954, G_B_loss: 0.6064\n",
      "Epoch [10/30], Step [921/1067], D_A_loss: 0.1041, D_B_loss: 0.0763, G_A_loss: 0.7257, G_B_loss: 0.6430\n",
      "Epoch [10/30], Step [931/1067], D_A_loss: 0.2374, D_B_loss: 0.2091, G_A_loss: 0.5136, G_B_loss: 0.5992\n",
      "Epoch [10/30], Step [941/1067], D_A_loss: 0.1546, D_B_loss: 0.0862, G_A_loss: 0.8015, G_B_loss: 0.3151\n",
      "Epoch [10/30], Step [951/1067], D_A_loss: 0.0448, D_B_loss: 0.1052, G_A_loss: 1.0611, G_B_loss: 0.5885\n",
      "Epoch [10/30], Step [961/1067], D_A_loss: 0.0746, D_B_loss: 0.3873, G_A_loss: 0.8501, G_B_loss: 0.2304\n",
      "Epoch [10/30], Step [971/1067], D_A_loss: 0.1935, D_B_loss: 0.2839, G_A_loss: 0.2455, G_B_loss: 1.2607\n",
      "Epoch [10/30], Step [981/1067], D_A_loss: 0.0424, D_B_loss: 0.0298, G_A_loss: 0.2227, G_B_loss: 0.4248\n",
      "Epoch [10/30], Step [991/1067], D_A_loss: 0.1202, D_B_loss: 0.1492, G_A_loss: 0.5713, G_B_loss: 0.4010\n",
      "Epoch [10/30], Step [1001/1067], D_A_loss: 0.1586, D_B_loss: 0.1708, G_A_loss: 0.2177, G_B_loss: 0.5629\n",
      "Epoch [10/30], Step [1011/1067], D_A_loss: 0.2794, D_B_loss: 0.0877, G_A_loss: 0.6172, G_B_loss: 0.1451\n",
      "Epoch [10/30], Step [1021/1067], D_A_loss: 0.1345, D_B_loss: 0.2356, G_A_loss: 0.1422, G_B_loss: 0.4175\n",
      "Epoch [10/30], Step [1031/1067], D_A_loss: 0.2194, D_B_loss: 0.1845, G_A_loss: 0.2137, G_B_loss: 0.6654\n",
      "Epoch [10/30], Step [1041/1067], D_A_loss: 0.1493, D_B_loss: 0.0588, G_A_loss: 0.7479, G_B_loss: 0.2950\n",
      "Epoch [10/30], Step [1051/1067], D_A_loss: 0.1048, D_B_loss: 0.1214, G_A_loss: 0.8549, G_B_loss: 0.3729\n",
      "Epoch [10/30], Step [1061/1067], D_A_loss: 0.1233, D_B_loss: 0.1774, G_A_loss: 0.3194, G_B_loss: 0.6465\n",
      "Epoch [11/30], Step [1/1067], D_A_loss: 0.2693, D_B_loss: 0.0324, G_A_loss: 0.6189, G_B_loss: 0.1975\n",
      "Epoch [11/30], Step [11/1067], D_A_loss: 0.1178, D_B_loss: 0.0239, G_A_loss: 0.4762, G_B_loss: 0.3709\n",
      "Epoch [11/30], Step [21/1067], D_A_loss: 0.1088, D_B_loss: 0.0783, G_A_loss: 0.1628, G_B_loss: 0.5871\n",
      "Epoch [11/30], Step [31/1067], D_A_loss: 0.3353, D_B_loss: 0.1072, G_A_loss: 0.4050, G_B_loss: 1.2870\n",
      "Epoch [11/30], Step [41/1067], D_A_loss: 0.0668, D_B_loss: 0.1047, G_A_loss: 0.4309, G_B_loss: 0.6026\n",
      "Epoch [11/30], Step [51/1067], D_A_loss: 0.2934, D_B_loss: 0.2587, G_A_loss: 0.1804, G_B_loss: 0.0945\n",
      "Epoch [11/30], Step [61/1067], D_A_loss: 0.1430, D_B_loss: 0.1777, G_A_loss: 0.2881, G_B_loss: 0.3439\n",
      "Epoch [11/30], Step [71/1067], D_A_loss: 0.2125, D_B_loss: 0.2405, G_A_loss: 0.8152, G_B_loss: 0.2038\n",
      "Epoch [11/30], Step [81/1067], D_A_loss: 0.1752, D_B_loss: 0.1420, G_A_loss: 0.5115, G_B_loss: 0.4634\n",
      "Epoch [11/30], Step [91/1067], D_A_loss: 0.0689, D_B_loss: 0.1319, G_A_loss: 0.3287, G_B_loss: 0.4619\n",
      "Epoch [11/30], Step [101/1067], D_A_loss: 0.2680, D_B_loss: 0.1857, G_A_loss: 0.2119, G_B_loss: 1.0278\n",
      "Epoch [11/30], Step [111/1067], D_A_loss: 0.2272, D_B_loss: 0.0830, G_A_loss: 0.8198, G_B_loss: 0.1795\n",
      "Epoch [11/30], Step [121/1067], D_A_loss: 0.2631, D_B_loss: 0.1357, G_A_loss: 0.3238, G_B_loss: 0.2673\n",
      "Epoch [11/30], Step [131/1067], D_A_loss: 0.1582, D_B_loss: 0.1314, G_A_loss: 0.6919, G_B_loss: 0.1743\n",
      "Epoch [11/30], Step [141/1067], D_A_loss: 0.1583, D_B_loss: 0.0640, G_A_loss: 0.5436, G_B_loss: 0.3765\n",
      "Epoch [11/30], Step [151/1067], D_A_loss: 0.1325, D_B_loss: 0.4233, G_A_loss: 1.0377, G_B_loss: 0.3717\n",
      "Epoch [11/30], Step [161/1067], D_A_loss: 0.0834, D_B_loss: 0.1646, G_A_loss: 0.2561, G_B_loss: 0.7265\n",
      "Epoch [11/30], Step [171/1067], D_A_loss: 0.0293, D_B_loss: 0.0659, G_A_loss: 0.7831, G_B_loss: 0.4490\n",
      "Epoch [11/30], Step [181/1067], D_A_loss: 0.1604, D_B_loss: 0.0901, G_A_loss: 0.7637, G_B_loss: 0.4902\n",
      "Epoch [11/30], Step [191/1067], D_A_loss: 0.2322, D_B_loss: 0.0234, G_A_loss: 0.7229, G_B_loss: 0.1848\n",
      "Epoch [11/30], Step [201/1067], D_A_loss: 0.1586, D_B_loss: 0.1026, G_A_loss: 1.0542, G_B_loss: 0.4005\n",
      "Epoch [11/30], Step [211/1067], D_A_loss: 0.1357, D_B_loss: 0.0484, G_A_loss: 0.9158, G_B_loss: 0.3393\n",
      "Epoch [11/30], Step [221/1067], D_A_loss: 0.0966, D_B_loss: 0.1231, G_A_loss: 0.4963, G_B_loss: 0.4402\n",
      "Epoch [11/30], Step [231/1067], D_A_loss: 0.1768, D_B_loss: 0.1253, G_A_loss: 0.3279, G_B_loss: 0.2197\n",
      "Epoch [11/30], Step [241/1067], D_A_loss: 0.1260, D_B_loss: 0.0764, G_A_loss: 0.4095, G_B_loss: 0.4751\n",
      "Epoch [11/30], Step [251/1067], D_A_loss: 0.2205, D_B_loss: 0.0542, G_A_loss: 0.5440, G_B_loss: 0.1643\n",
      "Epoch [11/30], Step [261/1067], D_A_loss: 0.0869, D_B_loss: 0.1408, G_A_loss: 0.3520, G_B_loss: 0.6997\n",
      "Epoch [11/30], Step [271/1067], D_A_loss: 0.3015, D_B_loss: 0.0384, G_A_loss: 0.7752, G_B_loss: 0.1411\n",
      "Epoch [11/30], Step [281/1067], D_A_loss: 0.1635, D_B_loss: 0.1311, G_A_loss: 0.1942, G_B_loss: 0.2608\n",
      "Epoch [11/30], Step [291/1067], D_A_loss: 0.2697, D_B_loss: 0.0442, G_A_loss: 0.7027, G_B_loss: 0.2646\n",
      "Epoch [11/30], Step [301/1067], D_A_loss: 0.0320, D_B_loss: 0.1451, G_A_loss: 0.2650, G_B_loss: 0.4978\n",
      "Epoch [11/30], Step [311/1067], D_A_loss: 0.1257, D_B_loss: 0.1176, G_A_loss: 0.3333, G_B_loss: 0.2668\n",
      "Epoch [11/30], Step [321/1067], D_A_loss: 0.3218, D_B_loss: 0.0352, G_A_loss: 0.3756, G_B_loss: 0.5157\n",
      "Epoch [11/30], Step [331/1067], D_A_loss: 0.1215, D_B_loss: 0.1600, G_A_loss: 0.5490, G_B_loss: 0.2645\n",
      "Epoch [11/30], Step [341/1067], D_A_loss: 0.0649, D_B_loss: 0.1489, G_A_loss: 0.6296, G_B_loss: 0.4725\n",
      "Epoch [11/30], Step [351/1067], D_A_loss: 0.0957, D_B_loss: 0.1601, G_A_loss: 0.7224, G_B_loss: 0.3477\n",
      "Epoch [11/30], Step [361/1067], D_A_loss: 0.0666, D_B_loss: 0.0603, G_A_loss: 0.6888, G_B_loss: 0.8699\n",
      "Epoch [11/30], Step [371/1067], D_A_loss: 0.1951, D_B_loss: 0.0295, G_A_loss: 0.8128, G_B_loss: 0.2546\n",
      "Epoch [11/30], Step [381/1067], D_A_loss: 0.1714, D_B_loss: 0.1987, G_A_loss: 0.8836, G_B_loss: 0.2681\n",
      "Epoch [11/30], Step [391/1067], D_A_loss: 0.1105, D_B_loss: 0.2919, G_A_loss: 0.2123, G_B_loss: 0.1017\n",
      "Epoch [11/30], Step [401/1067], D_A_loss: 0.3826, D_B_loss: 0.0419, G_A_loss: 0.6371, G_B_loss: 0.1300\n",
      "Epoch [11/30], Step [411/1067], D_A_loss: 0.2418, D_B_loss: 0.1332, G_A_loss: 0.5687, G_B_loss: 0.3277\n",
      "Epoch [11/30], Step [421/1067], D_A_loss: 0.1809, D_B_loss: 0.2057, G_A_loss: 1.0524, G_B_loss: 0.4486\n",
      "Epoch [11/30], Step [431/1067], D_A_loss: 0.1299, D_B_loss: 0.2367, G_A_loss: 0.6585, G_B_loss: 0.3367\n",
      "Epoch [11/30], Step [441/1067], D_A_loss: 0.1824, D_B_loss: 0.0885, G_A_loss: 0.2364, G_B_loss: 0.8404\n",
      "Epoch [11/30], Step [451/1067], D_A_loss: 0.2509, D_B_loss: 0.1279, G_A_loss: 0.4035, G_B_loss: 0.3369\n",
      "Epoch [11/30], Step [461/1067], D_A_loss: 0.1963, D_B_loss: 0.1530, G_A_loss: 0.5776, G_B_loss: 0.7454\n",
      "Epoch [11/30], Step [471/1067], D_A_loss: 0.2709, D_B_loss: 0.1839, G_A_loss: 0.6397, G_B_loss: 0.1638\n",
      "Epoch [11/30], Step [481/1067], D_A_loss: 0.1415, D_B_loss: 0.0256, G_A_loss: 0.7387, G_B_loss: 0.4240\n",
      "Epoch [11/30], Step [491/1067], D_A_loss: 0.1072, D_B_loss: 0.2071, G_A_loss: 0.8664, G_B_loss: 0.3119\n",
      "Epoch [11/30], Step [501/1067], D_A_loss: 0.1434, D_B_loss: 0.0430, G_A_loss: 0.7128, G_B_loss: 0.7371\n",
      "Epoch [11/30], Step [511/1067], D_A_loss: 0.1872, D_B_loss: 0.0817, G_A_loss: 0.8146, G_B_loss: 0.3365\n",
      "Epoch [11/30], Step [521/1067], D_A_loss: 0.1271, D_B_loss: 0.0965, G_A_loss: 0.8767, G_B_loss: 0.4264\n",
      "Epoch [11/30], Step [531/1067], D_A_loss: 0.0742, D_B_loss: 0.0397, G_A_loss: 0.7180, G_B_loss: 0.8584\n",
      "Epoch [11/30], Step [541/1067], D_A_loss: 0.0865, D_B_loss: 0.1491, G_A_loss: 0.2818, G_B_loss: 0.4910\n",
      "Epoch [11/30], Step [551/1067], D_A_loss: 0.0355, D_B_loss: 0.1127, G_A_loss: 0.3192, G_B_loss: 0.5389\n",
      "Epoch [11/30], Step [561/1067], D_A_loss: 0.1649, D_B_loss: 0.2005, G_A_loss: 0.3580, G_B_loss: 0.2982\n",
      "Epoch [11/30], Step [571/1067], D_A_loss: 0.1043, D_B_loss: 0.0574, G_A_loss: 0.6333, G_B_loss: 0.5104\n",
      "Epoch [11/30], Step [581/1067], D_A_loss: 0.1781, D_B_loss: 0.0816, G_A_loss: 1.0842, G_B_loss: 0.1509\n",
      "Epoch [11/30], Step [591/1067], D_A_loss: 0.1218, D_B_loss: 0.2480, G_A_loss: 0.3585, G_B_loss: 0.3270\n",
      "Epoch [11/30], Step [601/1067], D_A_loss: 0.0801, D_B_loss: 0.1757, G_A_loss: 0.3546, G_B_loss: 0.8945\n",
      "Epoch [11/30], Step [611/1067], D_A_loss: 0.2080, D_B_loss: 0.1019, G_A_loss: 0.6565, G_B_loss: 0.8449\n",
      "Epoch [11/30], Step [621/1067], D_A_loss: 0.0347, D_B_loss: 0.0684, G_A_loss: 0.4147, G_B_loss: 0.4686\n",
      "Epoch [11/30], Step [631/1067], D_A_loss: 0.5129, D_B_loss: 0.0757, G_A_loss: 0.7925, G_B_loss: 0.0505\n",
      "Epoch [11/30], Step [641/1067], D_A_loss: 0.1592, D_B_loss: 0.0418, G_A_loss: 0.4677, G_B_loss: 0.2714\n",
      "Epoch [11/30], Step [651/1067], D_A_loss: 0.1267, D_B_loss: 0.0631, G_A_loss: 0.6937, G_B_loss: 0.3430\n",
      "Epoch [11/30], Step [661/1067], D_A_loss: 0.3248, D_B_loss: 0.0963, G_A_loss: 0.4296, G_B_loss: 0.2791\n",
      "Epoch [11/30], Step [671/1067], D_A_loss: 0.0671, D_B_loss: 0.0677, G_A_loss: 0.6886, G_B_loss: 0.3994\n",
      "Epoch [11/30], Step [681/1067], D_A_loss: 0.1656, D_B_loss: 0.1355, G_A_loss: 0.5798, G_B_loss: 0.5784\n",
      "Epoch [11/30], Step [691/1067], D_A_loss: 0.3618, D_B_loss: 0.0781, G_A_loss: 0.4311, G_B_loss: 0.6760\n",
      "Epoch [11/30], Step [701/1067], D_A_loss: 0.0876, D_B_loss: 0.1208, G_A_loss: 0.4282, G_B_loss: 0.1328\n",
      "Epoch [11/30], Step [711/1067], D_A_loss: 0.1573, D_B_loss: 0.0457, G_A_loss: 1.0682, G_B_loss: 0.7727\n",
      "Epoch [11/30], Step [721/1067], D_A_loss: 0.3276, D_B_loss: 0.3233, G_A_loss: 0.5283, G_B_loss: 0.0955\n",
      "Epoch [11/30], Step [731/1067], D_A_loss: 0.0979, D_B_loss: 0.1273, G_A_loss: 0.6777, G_B_loss: 0.3604\n",
      "Epoch [11/30], Step [741/1067], D_A_loss: 0.1506, D_B_loss: 0.1515, G_A_loss: 0.1928, G_B_loss: 0.3490\n",
      "Epoch [11/30], Step [751/1067], D_A_loss: 0.1102, D_B_loss: 0.1483, G_A_loss: 1.2580, G_B_loss: 0.3054\n",
      "Epoch [11/30], Step [761/1067], D_A_loss: 0.3066, D_B_loss: 0.1766, G_A_loss: 0.3145, G_B_loss: 0.0823\n",
      "Epoch [11/30], Step [771/1067], D_A_loss: 0.4049, D_B_loss: 0.2423, G_A_loss: 0.3703, G_B_loss: 1.1438\n",
      "Epoch [11/30], Step [781/1067], D_A_loss: 0.3441, D_B_loss: 0.1186, G_A_loss: 0.4213, G_B_loss: 0.3381\n",
      "Epoch [11/30], Step [791/1067], D_A_loss: 0.1461, D_B_loss: 0.3345, G_A_loss: 0.2713, G_B_loss: 0.1398\n",
      "Epoch [11/30], Step [801/1067], D_A_loss: 0.1938, D_B_loss: 0.1158, G_A_loss: 1.0179, G_B_loss: 0.1928\n",
      "Epoch [11/30], Step [811/1067], D_A_loss: 0.2764, D_B_loss: 0.2487, G_A_loss: 0.8497, G_B_loss: 0.0240\n",
      "Epoch [11/30], Step [821/1067], D_A_loss: 0.0846, D_B_loss: 0.1752, G_A_loss: 0.7404, G_B_loss: 0.6634\n",
      "Epoch [11/30], Step [831/1067], D_A_loss: 0.0420, D_B_loss: 0.2294, G_A_loss: 0.1992, G_B_loss: 0.2912\n",
      "Epoch [11/30], Step [841/1067], D_A_loss: 0.1047, D_B_loss: 0.1435, G_A_loss: 0.3131, G_B_loss: 0.4147\n",
      "Epoch [11/30], Step [851/1067], D_A_loss: 0.1269, D_B_loss: 0.0315, G_A_loss: 0.5889, G_B_loss: 0.3072\n",
      "Epoch [11/30], Step [861/1067], D_A_loss: 0.3993, D_B_loss: 0.2083, G_A_loss: 0.2251, G_B_loss: 0.0575\n",
      "Epoch [11/30], Step [871/1067], D_A_loss: 0.2614, D_B_loss: 0.2380, G_A_loss: 0.8943, G_B_loss: 0.1467\n",
      "Epoch [11/30], Step [881/1067], D_A_loss: 0.1840, D_B_loss: 0.1011, G_A_loss: 0.4785, G_B_loss: 0.4935\n",
      "Epoch [11/30], Step [891/1067], D_A_loss: 0.0622, D_B_loss: 0.0808, G_A_loss: 0.7232, G_B_loss: 0.6437\n",
      "Epoch [11/30], Step [901/1067], D_A_loss: 0.0805, D_B_loss: 0.1783, G_A_loss: 0.3092, G_B_loss: 0.4112\n",
      "Epoch [11/30], Step [911/1067], D_A_loss: 0.0677, D_B_loss: 0.0793, G_A_loss: 0.8113, G_B_loss: 0.7782\n",
      "Epoch [11/30], Step [921/1067], D_A_loss: 0.2213, D_B_loss: 0.0572, G_A_loss: 0.4173, G_B_loss: 0.4324\n",
      "Epoch [11/30], Step [931/1067], D_A_loss: 0.0301, D_B_loss: 0.0967, G_A_loss: 0.8243, G_B_loss: 0.7959\n",
      "Epoch [11/30], Step [941/1067], D_A_loss: 0.1430, D_B_loss: 0.0596, G_A_loss: 0.5414, G_B_loss: 0.3552\n",
      "Epoch [11/30], Step [951/1067], D_A_loss: 0.1190, D_B_loss: 0.1366, G_A_loss: 0.5741, G_B_loss: 0.3169\n",
      "Epoch [11/30], Step [961/1067], D_A_loss: 0.0486, D_B_loss: 0.2157, G_A_loss: 0.5391, G_B_loss: 0.0471\n",
      "Epoch [11/30], Step [971/1067], D_A_loss: 0.2289, D_B_loss: 0.0618, G_A_loss: 0.5149, G_B_loss: 0.5982\n",
      "Epoch [11/30], Step [981/1067], D_A_loss: 0.0786, D_B_loss: 0.1170, G_A_loss: 0.6107, G_B_loss: 0.2136\n",
      "Epoch [11/30], Step [991/1067], D_A_loss: 0.0657, D_B_loss: 0.0298, G_A_loss: 0.7156, G_B_loss: 0.1969\n",
      "Epoch [11/30], Step [1001/1067], D_A_loss: 0.1367, D_B_loss: 0.1704, G_A_loss: 0.2112, G_B_loss: 0.3795\n",
      "Epoch [11/30], Step [1011/1067], D_A_loss: 0.1223, D_B_loss: 0.2897, G_A_loss: 1.1936, G_B_loss: 0.5714\n",
      "Epoch [11/30], Step [1021/1067], D_A_loss: 0.0505, D_B_loss: 0.0483, G_A_loss: 0.6400, G_B_loss: 0.5866\n",
      "Epoch [11/30], Step [1031/1067], D_A_loss: 0.1402, D_B_loss: 0.0956, G_A_loss: 0.6579, G_B_loss: 0.1826\n",
      "Epoch [11/30], Step [1041/1067], D_A_loss: 0.1198, D_B_loss: 0.1349, G_A_loss: 0.3159, G_B_loss: 0.5419\n",
      "Epoch [11/30], Step [1051/1067], D_A_loss: 0.1775, D_B_loss: 0.1615, G_A_loss: 0.6986, G_B_loss: 0.3575\n",
      "Epoch [11/30], Step [1061/1067], D_A_loss: 0.0817, D_B_loss: 0.0660, G_A_loss: 0.3878, G_B_loss: 0.2412\n",
      "Epoch [12/30], Step [1/1067], D_A_loss: 0.2452, D_B_loss: 0.0628, G_A_loss: 0.6901, G_B_loss: 0.5988\n",
      "Epoch [12/30], Step [11/1067], D_A_loss: 0.0802, D_B_loss: 0.1614, G_A_loss: 0.4567, G_B_loss: 0.3440\n",
      "Epoch [12/30], Step [21/1067], D_A_loss: 0.1757, D_B_loss: 0.0547, G_A_loss: 0.5206, G_B_loss: 0.2137\n",
      "Epoch [12/30], Step [31/1067], D_A_loss: 0.2035, D_B_loss: 0.1974, G_A_loss: 0.1131, G_B_loss: 0.3437\n",
      "Epoch [12/30], Step [41/1067], D_A_loss: 0.1054, D_B_loss: 0.0441, G_A_loss: 0.3774, G_B_loss: 0.1766\n",
      "Epoch [12/30], Step [51/1067], D_A_loss: 0.0521, D_B_loss: 0.0183, G_A_loss: 0.4495, G_B_loss: 0.3737\n",
      "Epoch [12/30], Step [61/1067], D_A_loss: 0.0951, D_B_loss: 0.0238, G_A_loss: 0.4083, G_B_loss: 0.4471\n",
      "Epoch [12/30], Step [71/1067], D_A_loss: 0.2644, D_B_loss: 0.0682, G_A_loss: 0.9495, G_B_loss: 0.0979\n",
      "Epoch [12/30], Step [81/1067], D_A_loss: 0.2756, D_B_loss: 0.0704, G_A_loss: 0.6102, G_B_loss: 0.8621\n",
      "Epoch [12/30], Step [91/1067], D_A_loss: 0.0813, D_B_loss: 0.1997, G_A_loss: 1.2057, G_B_loss: 0.6174\n",
      "Epoch [12/30], Step [101/1067], D_A_loss: 0.1165, D_B_loss: 0.0636, G_A_loss: 0.2629, G_B_loss: 0.3019\n",
      "Epoch [12/30], Step [111/1067], D_A_loss: 0.2084, D_B_loss: 0.1935, G_A_loss: 0.4058, G_B_loss: 0.2592\n",
      "Epoch [12/30], Step [121/1067], D_A_loss: 0.0920, D_B_loss: 0.1985, G_A_loss: 0.1711, G_B_loss: 0.5191\n",
      "Epoch [12/30], Step [131/1067], D_A_loss: 0.2463, D_B_loss: 0.0330, G_A_loss: 0.6601, G_B_loss: 0.3494\n",
      "Epoch [12/30], Step [141/1067], D_A_loss: 0.1984, D_B_loss: 0.1213, G_A_loss: 0.3393, G_B_loss: 0.3677\n",
      "Epoch [12/30], Step [151/1067], D_A_loss: 0.0451, D_B_loss: 0.0521, G_A_loss: 0.7461, G_B_loss: 0.6349\n",
      "Epoch [12/30], Step [161/1067], D_A_loss: 0.2370, D_B_loss: 0.0300, G_A_loss: 0.5009, G_B_loss: 0.1959\n",
      "Epoch [12/30], Step [171/1067], D_A_loss: 0.0731, D_B_loss: 0.0907, G_A_loss: 0.2060, G_B_loss: 0.1980\n",
      "Epoch [12/30], Step [181/1067], D_A_loss: 0.0558, D_B_loss: 0.0443, G_A_loss: 0.8187, G_B_loss: 0.8986\n",
      "Epoch [12/30], Step [191/1067], D_A_loss: 0.7585, D_B_loss: 0.3479, G_A_loss: 0.5662, G_B_loss: 0.0530\n",
      "Epoch [12/30], Step [201/1067], D_A_loss: 0.0979, D_B_loss: 0.0884, G_A_loss: 0.5440, G_B_loss: 0.3653\n",
      "Epoch [12/30], Step [211/1067], D_A_loss: 0.1714, D_B_loss: 0.0527, G_A_loss: 0.8971, G_B_loss: 0.0896\n",
      "Epoch [12/30], Step [221/1067], D_A_loss: 0.1887, D_B_loss: 0.0291, G_A_loss: 0.5321, G_B_loss: 0.2761\n",
      "Epoch [12/30], Step [231/1067], D_A_loss: 0.2651, D_B_loss: 0.0171, G_A_loss: 0.2706, G_B_loss: 0.1349\n",
      "Epoch [12/30], Step [241/1067], D_A_loss: 0.1357, D_B_loss: 0.0477, G_A_loss: 0.7528, G_B_loss: 0.4576\n",
      "Epoch [12/30], Step [251/1067], D_A_loss: 0.2945, D_B_loss: 0.2374, G_A_loss: 0.4353, G_B_loss: 0.0794\n",
      "Epoch [12/30], Step [261/1067], D_A_loss: 0.1426, D_B_loss: 0.0934, G_A_loss: 0.9475, G_B_loss: 0.2564\n",
      "Epoch [12/30], Step [271/1067], D_A_loss: 0.1589, D_B_loss: 0.0721, G_A_loss: 0.9927, G_B_loss: 0.2858\n",
      "Epoch [12/30], Step [281/1067], D_A_loss: 0.1381, D_B_loss: 0.1850, G_A_loss: 0.3814, G_B_loss: 0.4715\n",
      "Epoch [12/30], Step [291/1067], D_A_loss: 0.0638, D_B_loss: 0.1998, G_A_loss: 0.2273, G_B_loss: 0.5420\n",
      "Epoch [12/30], Step [301/1067], D_A_loss: 0.3085, D_B_loss: 0.1200, G_A_loss: 0.4560, G_B_loss: 0.4365\n",
      "Epoch [12/30], Step [311/1067], D_A_loss: 0.1741, D_B_loss: 0.1331, G_A_loss: 0.4120, G_B_loss: 0.7956\n",
      "Epoch [12/30], Step [321/1067], D_A_loss: 0.1198, D_B_loss: 0.2215, G_A_loss: 0.6947, G_B_loss: 0.2964\n",
      "Epoch [12/30], Step [331/1067], D_A_loss: 0.0789, D_B_loss: 0.0935, G_A_loss: 0.5673, G_B_loss: 0.1506\n",
      "Epoch [12/30], Step [341/1067], D_A_loss: 0.2004, D_B_loss: 0.1004, G_A_loss: 0.1918, G_B_loss: 0.2002\n",
      "Epoch [12/30], Step [351/1067], D_A_loss: 0.0694, D_B_loss: 0.1635, G_A_loss: 0.8338, G_B_loss: 0.1387\n",
      "Epoch [12/30], Step [361/1067], D_A_loss: 0.1899, D_B_loss: 0.0274, G_A_loss: 0.7192, G_B_loss: 0.2203\n",
      "Epoch [12/30], Step [371/1067], D_A_loss: 0.2127, D_B_loss: 0.1997, G_A_loss: 0.3870, G_B_loss: 0.2912\n",
      "Epoch [12/30], Step [381/1067], D_A_loss: 0.2046, D_B_loss: 0.1602, G_A_loss: 0.1795, G_B_loss: 0.1639\n",
      "Epoch [12/30], Step [391/1067], D_A_loss: 0.2815, D_B_loss: 0.1386, G_A_loss: 0.3690, G_B_loss: 0.2886\n",
      "Epoch [12/30], Step [401/1067], D_A_loss: 0.0409, D_B_loss: 0.0621, G_A_loss: 0.9338, G_B_loss: 0.7544\n",
      "Epoch [12/30], Step [411/1067], D_A_loss: 0.1271, D_B_loss: 0.0992, G_A_loss: 0.5866, G_B_loss: 0.3170\n",
      "Epoch [12/30], Step [421/1067], D_A_loss: 0.0267, D_B_loss: 0.0231, G_A_loss: 0.5365, G_B_loss: 0.5665\n",
      "Epoch [12/30], Step [431/1067], D_A_loss: 0.1181, D_B_loss: 0.0617, G_A_loss: 0.5970, G_B_loss: 0.3160\n",
      "Epoch [12/30], Step [441/1067], D_A_loss: 0.2640, D_B_loss: 0.1351, G_A_loss: 0.8089, G_B_loss: 0.4060\n",
      "Epoch [12/30], Step [451/1067], D_A_loss: 0.1322, D_B_loss: 0.0853, G_A_loss: 0.3913, G_B_loss: 0.3448\n",
      "Epoch [12/30], Step [461/1067], D_A_loss: 0.1783, D_B_loss: 0.0922, G_A_loss: 0.3851, G_B_loss: 0.6706\n",
      "Epoch [12/30], Step [471/1067], D_A_loss: 0.0567, D_B_loss: 0.2088, G_A_loss: 0.1709, G_B_loss: 0.1585\n",
      "Epoch [12/30], Step [481/1067], D_A_loss: 0.2597, D_B_loss: 0.0876, G_A_loss: 0.5065, G_B_loss: 0.0985\n",
      "Epoch [12/30], Step [491/1067], D_A_loss: 0.1841, D_B_loss: 0.0714, G_A_loss: 0.6816, G_B_loss: 0.5793\n",
      "Epoch [12/30], Step [501/1067], D_A_loss: 0.1224, D_B_loss: 0.0430, G_A_loss: 0.6538, G_B_loss: 0.5987\n",
      "Epoch [12/30], Step [511/1067], D_A_loss: 0.1196, D_B_loss: 0.1507, G_A_loss: 0.7233, G_B_loss: 0.2949\n",
      "Epoch [12/30], Step [521/1067], D_A_loss: 0.0653, D_B_loss: 0.0507, G_A_loss: 0.6512, G_B_loss: 0.5351\n",
      "Epoch [12/30], Step [531/1067], D_A_loss: 0.2398, D_B_loss: 0.3083, G_A_loss: 0.3231, G_B_loss: 0.2938\n",
      "Epoch [12/30], Step [541/1067], D_A_loss: 0.0337, D_B_loss: 0.0339, G_A_loss: 1.1994, G_B_loss: 0.8841\n",
      "Epoch [12/30], Step [551/1067], D_A_loss: 0.1227, D_B_loss: 0.5024, G_A_loss: 0.3100, G_B_loss: 0.4470\n",
      "Epoch [12/30], Step [561/1067], D_A_loss: 0.0609, D_B_loss: 0.0277, G_A_loss: 0.7065, G_B_loss: 0.2407\n",
      "Epoch [12/30], Step [571/1067], D_A_loss: 0.0367, D_B_loss: 0.0258, G_A_loss: 0.7523, G_B_loss: 0.2620\n",
      "Epoch [12/30], Step [581/1067], D_A_loss: 0.3893, D_B_loss: 0.2711, G_A_loss: 0.8065, G_B_loss: 0.5958\n",
      "Epoch [12/30], Step [591/1067], D_A_loss: 0.1451, D_B_loss: 0.1302, G_A_loss: 0.2942, G_B_loss: 0.2647\n",
      "Epoch [12/30], Step [601/1067], D_A_loss: 0.2205, D_B_loss: 0.0323, G_A_loss: 0.7767, G_B_loss: 0.4741\n",
      "Epoch [12/30], Step [611/1067], D_A_loss: 0.1255, D_B_loss: 0.2109, G_A_loss: 0.4904, G_B_loss: 0.3152\n",
      "Epoch [12/30], Step [621/1067], D_A_loss: 0.1189, D_B_loss: 0.0501, G_A_loss: 0.5464, G_B_loss: 0.3876\n",
      "Epoch [12/30], Step [631/1067], D_A_loss: 0.1544, D_B_loss: 0.0741, G_A_loss: 0.5837, G_B_loss: 0.2937\n",
      "Epoch [12/30], Step [641/1067], D_A_loss: 0.3391, D_B_loss: 0.0713, G_A_loss: 0.4928, G_B_loss: 1.4685\n",
      "Epoch [12/30], Step [651/1067], D_A_loss: 0.1251, D_B_loss: 0.0290, G_A_loss: 0.9908, G_B_loss: 0.6190\n",
      "Epoch [12/30], Step [661/1067], D_A_loss: 0.1116, D_B_loss: 0.0399, G_A_loss: 0.4438, G_B_loss: 0.6517\n",
      "Epoch [12/30], Step [671/1067], D_A_loss: 0.0925, D_B_loss: 0.1488, G_A_loss: 0.7619, G_B_loss: 0.3736\n",
      "Epoch [12/30], Step [681/1067], D_A_loss: 0.0268, D_B_loss: 0.2500, G_A_loss: 0.1273, G_B_loss: 0.3368\n",
      "Epoch [12/30], Step [691/1067], D_A_loss: 0.1732, D_B_loss: 0.1099, G_A_loss: 0.9662, G_B_loss: 0.2626\n",
      "Epoch [12/30], Step [701/1067], D_A_loss: 0.0983, D_B_loss: 0.0769, G_A_loss: 0.4412, G_B_loss: 0.3829\n",
      "Epoch [12/30], Step [711/1067], D_A_loss: 0.0652, D_B_loss: 0.1332, G_A_loss: 0.3616, G_B_loss: 0.5544\n",
      "Epoch [12/30], Step [721/1067], D_A_loss: 0.1688, D_B_loss: 0.0705, G_A_loss: 0.4003, G_B_loss: 0.3621\n",
      "Epoch [12/30], Step [731/1067], D_A_loss: 0.1107, D_B_loss: 0.0974, G_A_loss: 0.9279, G_B_loss: 0.3195\n",
      "Epoch [12/30], Step [741/1067], D_A_loss: 0.1189, D_B_loss: 0.1356, G_A_loss: 0.6617, G_B_loss: 0.5110\n",
      "Epoch [12/30], Step [751/1067], D_A_loss: 0.2282, D_B_loss: 0.2377, G_A_loss: 0.1199, G_B_loss: 0.1417\n",
      "Epoch [12/30], Step [761/1067], D_A_loss: 0.1400, D_B_loss: 0.0430, G_A_loss: 1.1225, G_B_loss: 0.3285\n",
      "Epoch [12/30], Step [771/1067], D_A_loss: 0.1083, D_B_loss: 0.1049, G_A_loss: 0.7848, G_B_loss: 0.3598\n",
      "Epoch [12/30], Step [781/1067], D_A_loss: 0.0360, D_B_loss: 0.1255, G_A_loss: 0.5956, G_B_loss: 0.4704\n",
      "Epoch [12/30], Step [791/1067], D_A_loss: 0.1114, D_B_loss: 0.0664, G_A_loss: 0.5371, G_B_loss: 0.1954\n",
      "Epoch [12/30], Step [801/1067], D_A_loss: 0.1846, D_B_loss: 0.0381, G_A_loss: 0.6859, G_B_loss: 0.6090\n",
      "Epoch [12/30], Step [811/1067], D_A_loss: 0.0869, D_B_loss: 0.0635, G_A_loss: 0.5462, G_B_loss: 0.4440\n",
      "Epoch [12/30], Step [821/1067], D_A_loss: 0.1659, D_B_loss: 0.0524, G_A_loss: 0.8159, G_B_loss: 0.2202\n",
      "Epoch [12/30], Step [831/1067], D_A_loss: 0.2100, D_B_loss: 0.1072, G_A_loss: 0.3529, G_B_loss: 0.2156\n",
      "Epoch [12/30], Step [841/1067], D_A_loss: 0.0619, D_B_loss: 0.0507, G_A_loss: 0.9268, G_B_loss: 0.6020\n",
      "Epoch [12/30], Step [851/1067], D_A_loss: 0.0739, D_B_loss: 0.0759, G_A_loss: 0.4780, G_B_loss: 0.5590\n",
      "Epoch [12/30], Step [861/1067], D_A_loss: 0.1021, D_B_loss: 0.2967, G_A_loss: 1.3687, G_B_loss: 0.4453\n",
      "Epoch [12/30], Step [871/1067], D_A_loss: 0.0299, D_B_loss: 0.1475, G_A_loss: 0.3189, G_B_loss: 0.5104\n",
      "Epoch [12/30], Step [881/1067], D_A_loss: 0.0970, D_B_loss: 0.2595, G_A_loss: 0.1657, G_B_loss: 0.4052\n",
      "Epoch [12/30], Step [891/1067], D_A_loss: 0.0873, D_B_loss: 0.0657, G_A_loss: 0.4669, G_B_loss: 0.5120\n",
      "Epoch [12/30], Step [901/1067], D_A_loss: 0.0372, D_B_loss: 0.0618, G_A_loss: 0.3102, G_B_loss: 0.4024\n",
      "Epoch [12/30], Step [911/1067], D_A_loss: 0.0592, D_B_loss: 0.0959, G_A_loss: 0.0903, G_B_loss: 0.4841\n",
      "Epoch [12/30], Step [921/1067], D_A_loss: 0.0331, D_B_loss: 0.0909, G_A_loss: 0.5647, G_B_loss: 0.2119\n",
      "Epoch [12/30], Step [931/1067], D_A_loss: 0.1164, D_B_loss: 0.0994, G_A_loss: 0.4502, G_B_loss: 0.2363\n",
      "Epoch [12/30], Step [941/1067], D_A_loss: 0.2021, D_B_loss: 0.1192, G_A_loss: 0.3071, G_B_loss: 0.0899\n",
      "Epoch [12/30], Step [951/1067], D_A_loss: 0.1639, D_B_loss: 0.0367, G_A_loss: 0.6343, G_B_loss: 0.3429\n",
      "Epoch [12/30], Step [961/1067], D_A_loss: 0.1551, D_B_loss: 0.0518, G_A_loss: 0.4505, G_B_loss: 0.5014\n",
      "Epoch [12/30], Step [971/1067], D_A_loss: 0.0386, D_B_loss: 0.1713, G_A_loss: 0.2064, G_B_loss: 0.3263\n",
      "Epoch [12/30], Step [981/1067], D_A_loss: 0.2497, D_B_loss: 0.1188, G_A_loss: 0.4625, G_B_loss: 0.1901\n",
      "Epoch [12/30], Step [991/1067], D_A_loss: 0.2895, D_B_loss: 0.0915, G_A_loss: 0.4223, G_B_loss: 0.3485\n",
      "Epoch [12/30], Step [1001/1067], D_A_loss: 0.0154, D_B_loss: 0.0554, G_A_loss: 0.7335, G_B_loss: 0.2706\n",
      "Epoch [12/30], Step [1011/1067], D_A_loss: 0.0557, D_B_loss: 0.0249, G_A_loss: 0.5112, G_B_loss: 0.2045\n",
      "Epoch [12/30], Step [1021/1067], D_A_loss: 0.0705, D_B_loss: 0.0739, G_A_loss: 0.9611, G_B_loss: 0.5293\n",
      "Epoch [12/30], Step [1031/1067], D_A_loss: 0.1143, D_B_loss: 0.1062, G_A_loss: 0.5330, G_B_loss: 0.3492\n",
      "Epoch [12/30], Step [1041/1067], D_A_loss: 0.2162, D_B_loss: 0.2894, G_A_loss: 0.5153, G_B_loss: 0.1867\n",
      "Epoch [12/30], Step [1051/1067], D_A_loss: 0.0604, D_B_loss: 0.0232, G_A_loss: 1.1741, G_B_loss: 0.3587\n",
      "Epoch [12/30], Step [1061/1067], D_A_loss: 0.1536, D_B_loss: 0.2076, G_A_loss: 0.4282, G_B_loss: 0.4656\n",
      "Epoch [13/30], Step [1/1067], D_A_loss: 0.1142, D_B_loss: 0.1920, G_A_loss: 0.4466, G_B_loss: 0.7741\n",
      "Epoch [13/30], Step [11/1067], D_A_loss: 0.2782, D_B_loss: 0.0977, G_A_loss: 0.2564, G_B_loss: 0.2277\n",
      "Epoch [13/30], Step [21/1067], D_A_loss: 0.0791, D_B_loss: 0.0315, G_A_loss: 0.1182, G_B_loss: 0.8182\n",
      "Epoch [13/30], Step [31/1067], D_A_loss: 0.2201, D_B_loss: 0.2170, G_A_loss: 0.1493, G_B_loss: 0.2375\n",
      "Epoch [13/30], Step [41/1067], D_A_loss: 0.4600, D_B_loss: 0.0646, G_A_loss: 0.6211, G_B_loss: 0.0577\n",
      "Epoch [13/30], Step [51/1067], D_A_loss: 0.0879, D_B_loss: 0.0621, G_A_loss: 0.9338, G_B_loss: 0.6980\n",
      "Epoch [13/30], Step [61/1067], D_A_loss: 0.1393, D_B_loss: 0.2493, G_A_loss: 0.4284, G_B_loss: 0.4909\n",
      "Epoch [13/30], Step [71/1067], D_A_loss: 0.0303, D_B_loss: 0.1534, G_A_loss: 0.3510, G_B_loss: 0.4027\n",
      "Epoch [13/30], Step [81/1067], D_A_loss: 0.0933, D_B_loss: 0.0476, G_A_loss: 0.2177, G_B_loss: 0.4312\n",
      "Epoch [13/30], Step [91/1067], D_A_loss: 0.0638, D_B_loss: 0.1104, G_A_loss: 0.8542, G_B_loss: 0.5967\n",
      "Epoch [13/30], Step [101/1067], D_A_loss: 0.0960, D_B_loss: 0.0233, G_A_loss: 0.8378, G_B_loss: 0.5759\n",
      "Epoch [13/30], Step [111/1067], D_A_loss: 0.2023, D_B_loss: 0.0525, G_A_loss: 0.3787, G_B_loss: 0.1935\n",
      "Epoch [13/30], Step [121/1067], D_A_loss: 0.0791, D_B_loss: 0.1485, G_A_loss: 0.4676, G_B_loss: 0.5357\n",
      "Epoch [13/30], Step [131/1067], D_A_loss: 0.0610, D_B_loss: 0.1921, G_A_loss: 0.6735, G_B_loss: 0.2979\n",
      "Epoch [13/30], Step [141/1067], D_A_loss: 0.0523, D_B_loss: 0.1077, G_A_loss: 0.5278, G_B_loss: 0.4609\n",
      "Epoch [13/30], Step [151/1067], D_A_loss: 0.1200, D_B_loss: 0.0290, G_A_loss: 0.5698, G_B_loss: 0.7780\n",
      "Epoch [13/30], Step [161/1067], D_A_loss: 0.0294, D_B_loss: 0.1728, G_A_loss: 0.8343, G_B_loss: 0.8439\n",
      "Epoch [13/30], Step [171/1067], D_A_loss: 0.0941, D_B_loss: 0.0217, G_A_loss: 0.5183, G_B_loss: 0.3100\n",
      "Epoch [13/30], Step [181/1067], D_A_loss: 0.1308, D_B_loss: 0.0303, G_A_loss: 0.6929, G_B_loss: 0.3894\n",
      "Epoch [13/30], Step [191/1067], D_A_loss: 0.2402, D_B_loss: 0.0899, G_A_loss: 1.2101, G_B_loss: 0.2356\n",
      "Epoch [13/30], Step [201/1067], D_A_loss: 0.1360, D_B_loss: 0.0690, G_A_loss: 0.5926, G_B_loss: 0.5641\n",
      "Epoch [13/30], Step [211/1067], D_A_loss: 0.0831, D_B_loss: 0.0617, G_A_loss: 1.1058, G_B_loss: 0.3777\n",
      "Epoch [13/30], Step [221/1067], D_A_loss: 0.1519, D_B_loss: 0.2025, G_A_loss: 0.7927, G_B_loss: 0.4780\n",
      "Epoch [13/30], Step [231/1067], D_A_loss: 0.2722, D_B_loss: 0.1812, G_A_loss: 0.2111, G_B_loss: 0.1884\n",
      "Epoch [13/30], Step [241/1067], D_A_loss: 0.0591, D_B_loss: 0.0613, G_A_loss: 0.2791, G_B_loss: 0.4245\n",
      "Epoch [13/30], Step [251/1067], D_A_loss: 0.1594, D_B_loss: 0.0331, G_A_loss: 0.8602, G_B_loss: 0.2518\n",
      "Epoch [13/30], Step [261/1067], D_A_loss: 0.2352, D_B_loss: 0.0151, G_A_loss: 0.8673, G_B_loss: 0.1659\n",
      "Epoch [13/30], Step [271/1067], D_A_loss: 0.1149, D_B_loss: 0.0443, G_A_loss: 0.5957, G_B_loss: 0.4648\n",
      "Epoch [13/30], Step [281/1067], D_A_loss: 0.2245, D_B_loss: 0.1193, G_A_loss: 0.7453, G_B_loss: 0.2125\n",
      "Epoch [13/30], Step [291/1067], D_A_loss: 0.1072, D_B_loss: 0.0670, G_A_loss: 0.6351, G_B_loss: 0.6170\n",
      "Epoch [13/30], Step [301/1067], D_A_loss: 0.0712, D_B_loss: 0.0214, G_A_loss: 1.1844, G_B_loss: 0.3302\n",
      "Epoch [13/30], Step [311/1067], D_A_loss: 0.0502, D_B_loss: 0.0626, G_A_loss: 0.7134, G_B_loss: 0.3235\n",
      "Epoch [13/30], Step [321/1067], D_A_loss: 0.1300, D_B_loss: 0.0368, G_A_loss: 0.6705, G_B_loss: 0.4892\n",
      "Epoch [13/30], Step [331/1067], D_A_loss: 0.0581, D_B_loss: 0.0459, G_A_loss: 0.2130, G_B_loss: 0.3882\n",
      "Epoch [13/30], Step [341/1067], D_A_loss: 0.0885, D_B_loss: 0.0280, G_A_loss: 0.7005, G_B_loss: 0.5254\n",
      "Epoch [13/30], Step [351/1067], D_A_loss: 0.0898, D_B_loss: 0.0486, G_A_loss: 0.6353, G_B_loss: 0.2587\n",
      "Epoch [13/30], Step [361/1067], D_A_loss: 0.2270, D_B_loss: 0.0410, G_A_loss: 0.6094, G_B_loss: 0.5174\n",
      "Epoch [13/30], Step [371/1067], D_A_loss: 0.0742, D_B_loss: 0.1290, G_A_loss: 0.5017, G_B_loss: 0.6535\n",
      "Epoch [13/30], Step [381/1067], D_A_loss: 0.0317, D_B_loss: 0.0452, G_A_loss: 0.9563, G_B_loss: 0.2015\n",
      "Epoch [13/30], Step [391/1067], D_A_loss: 0.0392, D_B_loss: 0.0284, G_A_loss: 0.4247, G_B_loss: 0.6973\n",
      "Epoch [13/30], Step [401/1067], D_A_loss: 0.0700, D_B_loss: 0.0252, G_A_loss: 0.5394, G_B_loss: 0.2726\n",
      "Epoch [13/30], Step [411/1067], D_A_loss: 0.0755, D_B_loss: 0.0311, G_A_loss: 0.2132, G_B_loss: 1.6093\n",
      "Epoch [13/30], Step [421/1067], D_A_loss: 0.2510, D_B_loss: 0.1434, G_A_loss: 0.4762, G_B_loss: 0.7570\n",
      "Epoch [13/30], Step [431/1067], D_A_loss: 0.0637, D_B_loss: 0.0856, G_A_loss: 0.4862, G_B_loss: 0.7902\n",
      "Epoch [13/30], Step [441/1067], D_A_loss: 0.1943, D_B_loss: 0.0844, G_A_loss: 0.2616, G_B_loss: 0.3782\n",
      "Epoch [13/30], Step [451/1067], D_A_loss: 0.1030, D_B_loss: 0.3547, G_A_loss: 1.1938, G_B_loss: 0.4287\n",
      "Epoch [13/30], Step [461/1067], D_A_loss: 0.1972, D_B_loss: 0.1064, G_A_loss: 0.6525, G_B_loss: 0.4491\n",
      "Epoch [13/30], Step [471/1067], D_A_loss: 0.1940, D_B_loss: 0.1859, G_A_loss: 0.2012, G_B_loss: 0.5650\n",
      "Epoch [13/30], Step [481/1067], D_A_loss: 0.1606, D_B_loss: 0.0830, G_A_loss: 0.6333, G_B_loss: 1.1977\n",
      "Epoch [13/30], Step [491/1067], D_A_loss: 0.2350, D_B_loss: 0.1596, G_A_loss: 0.3396, G_B_loss: 0.5644\n",
      "Epoch [13/30], Step [501/1067], D_A_loss: 0.0479, D_B_loss: 0.1505, G_A_loss: 0.3110, G_B_loss: 0.6772\n",
      "Epoch [13/30], Step [511/1067], D_A_loss: 0.0871, D_B_loss: 0.1056, G_A_loss: 0.7482, G_B_loss: 0.0988\n",
      "Epoch [13/30], Step [521/1067], D_A_loss: 0.1297, D_B_loss: 0.0557, G_A_loss: 0.6841, G_B_loss: 0.6554\n",
      "Epoch [13/30], Step [531/1067], D_A_loss: 0.4074, D_B_loss: 0.0869, G_A_loss: 0.4093, G_B_loss: 0.2479\n",
      "Epoch [13/30], Step [541/1067], D_A_loss: 0.2986, D_B_loss: 0.1559, G_A_loss: 0.3998, G_B_loss: 0.1305\n",
      "Epoch [13/30], Step [551/1067], D_A_loss: 0.1556, D_B_loss: 0.1513, G_A_loss: 0.2684, G_B_loss: 0.5928\n",
      "Epoch [13/30], Step [561/1067], D_A_loss: 0.4148, D_B_loss: 0.0602, G_A_loss: 0.4938, G_B_loss: 0.0301\n",
      "Epoch [13/30], Step [571/1067], D_A_loss: 0.1778, D_B_loss: 0.1174, G_A_loss: 0.3018, G_B_loss: 0.8787\n",
      "Epoch [13/30], Step [581/1067], D_A_loss: 0.1927, D_B_loss: 0.1261, G_A_loss: 0.4690, G_B_loss: 0.3510\n",
      "Epoch [13/30], Step [591/1067], D_A_loss: 0.0334, D_B_loss: 0.0121, G_A_loss: 0.6180, G_B_loss: 0.3495\n",
      "Epoch [13/30], Step [601/1067], D_A_loss: 0.0264, D_B_loss: 0.0185, G_A_loss: 0.9102, G_B_loss: 0.4380\n",
      "Epoch [13/30], Step [611/1067], D_A_loss: 0.0911, D_B_loss: 0.1936, G_A_loss: 0.7863, G_B_loss: 0.4808\n",
      "Epoch [13/30], Step [621/1067], D_A_loss: 0.1095, D_B_loss: 0.0777, G_A_loss: 0.4176, G_B_loss: 0.7991\n",
      "Epoch [13/30], Step [631/1067], D_A_loss: 0.1284, D_B_loss: 0.0402, G_A_loss: 0.6667, G_B_loss: 0.4965\n",
      "Epoch [13/30], Step [641/1067], D_A_loss: 0.1061, D_B_loss: 0.0442, G_A_loss: 0.9009, G_B_loss: 0.4162\n",
      "Epoch [13/30], Step [651/1067], D_A_loss: 0.2146, D_B_loss: 0.0908, G_A_loss: 0.4605, G_B_loss: 0.2011\n",
      "Epoch [13/30], Step [661/1067], D_A_loss: 0.1053, D_B_loss: 0.0192, G_A_loss: 0.4654, G_B_loss: 0.8327\n",
      "Epoch [13/30], Step [671/1067], D_A_loss: 0.0890, D_B_loss: 0.2551, G_A_loss: 0.1747, G_B_loss: 0.3788\n",
      "Epoch [13/30], Step [681/1067], D_A_loss: 0.0714, D_B_loss: 0.0404, G_A_loss: 0.7598, G_B_loss: 0.5265\n",
      "Epoch [13/30], Step [691/1067], D_A_loss: 0.1001, D_B_loss: 0.1376, G_A_loss: 0.3167, G_B_loss: 0.1476\n",
      "Epoch [13/30], Step [701/1067], D_A_loss: 0.2411, D_B_loss: 0.1151, G_A_loss: 0.4223, G_B_loss: 0.4593\n",
      "Epoch [13/30], Step [711/1067], D_A_loss: 0.1416, D_B_loss: 0.2741, G_A_loss: 0.9124, G_B_loss: 0.7425\n",
      "Epoch [13/30], Step [721/1067], D_A_loss: 0.2291, D_B_loss: 0.0370, G_A_loss: 0.9131, G_B_loss: 0.4343\n",
      "Epoch [13/30], Step [731/1067], D_A_loss: 0.0990, D_B_loss: 0.1134, G_A_loss: 0.5683, G_B_loss: 0.3395\n",
      "Epoch [13/30], Step [741/1067], D_A_loss: 0.1193, D_B_loss: 0.0669, G_A_loss: 0.2613, G_B_loss: 0.6746\n",
      "Epoch [13/30], Step [751/1067], D_A_loss: 0.1267, D_B_loss: 0.1168, G_A_loss: 0.3711, G_B_loss: 0.4495\n",
      "Epoch [13/30], Step [761/1067], D_A_loss: 0.1017, D_B_loss: 0.0623, G_A_loss: 0.5668, G_B_loss: 0.1455\n",
      "Epoch [13/30], Step [771/1067], D_A_loss: 0.0186, D_B_loss: 0.0313, G_A_loss: 0.9134, G_B_loss: 0.9601\n",
      "Epoch [13/30], Step [781/1067], D_A_loss: 0.0698, D_B_loss: 0.0284, G_A_loss: 0.7199, G_B_loss: 0.5139\n",
      "Epoch [13/30], Step [791/1067], D_A_loss: 0.0414, D_B_loss: 0.0564, G_A_loss: 0.5814, G_B_loss: 0.8371\n",
      "Epoch [13/30], Step [801/1067], D_A_loss: 0.0936, D_B_loss: 0.0548, G_A_loss: 1.0164, G_B_loss: 0.5546\n",
      "Epoch [13/30], Step [811/1067], D_A_loss: 0.1673, D_B_loss: 0.1200, G_A_loss: 1.0413, G_B_loss: 0.2454\n",
      "Epoch [13/30], Step [821/1067], D_A_loss: 0.1479, D_B_loss: 0.0602, G_A_loss: 0.9616, G_B_loss: 0.1224\n",
      "Epoch [13/30], Step [831/1067], D_A_loss: 0.2769, D_B_loss: 0.2401, G_A_loss: 0.5148, G_B_loss: 0.4842\n",
      "Epoch [13/30], Step [841/1067], D_A_loss: 0.0634, D_B_loss: 0.0526, G_A_loss: 0.5392, G_B_loss: 0.6076\n",
      "Epoch [13/30], Step [851/1067], D_A_loss: 0.1152, D_B_loss: 0.0362, G_A_loss: 0.7733, G_B_loss: 0.4676\n",
      "Epoch [13/30], Step [861/1067], D_A_loss: 0.0796, D_B_loss: 0.1294, G_A_loss: 0.9609, G_B_loss: 0.6889\n",
      "Epoch [13/30], Step [871/1067], D_A_loss: 0.0572, D_B_loss: 0.1568, G_A_loss: 0.3119, G_B_loss: 0.2471\n",
      "Epoch [13/30], Step [881/1067], D_A_loss: 0.1024, D_B_loss: 0.0445, G_A_loss: 0.8205, G_B_loss: 0.6020\n",
      "Epoch [13/30], Step [891/1067], D_A_loss: 0.0441, D_B_loss: 0.1154, G_A_loss: 0.3242, G_B_loss: 0.5056\n",
      "Epoch [13/30], Step [901/1067], D_A_loss: 0.1330, D_B_loss: 0.0171, G_A_loss: 0.8939, G_B_loss: 0.4323\n",
      "Epoch [13/30], Step [911/1067], D_A_loss: 0.1736, D_B_loss: 0.0192, G_A_loss: 0.7451, G_B_loss: 0.7198\n",
      "Epoch [13/30], Step [921/1067], D_A_loss: 0.1047, D_B_loss: 0.1995, G_A_loss: 0.2457, G_B_loss: 0.8119\n",
      "Epoch [13/30], Step [931/1067], D_A_loss: 0.0500, D_B_loss: 0.0213, G_A_loss: 0.5909, G_B_loss: 0.5604\n",
      "Epoch [13/30], Step [941/1067], D_A_loss: 0.0377, D_B_loss: 0.0875, G_A_loss: 0.4817, G_B_loss: 0.4035\n",
      "Epoch [13/30], Step [951/1067], D_A_loss: 0.0365, D_B_loss: 0.1236, G_A_loss: 0.6261, G_B_loss: 0.3553\n",
      "Epoch [13/30], Step [961/1067], D_A_loss: 0.0415, D_B_loss: 0.0797, G_A_loss: 0.4467, G_B_loss: 0.6684\n",
      "Epoch [13/30], Step [971/1067], D_A_loss: 0.0485, D_B_loss: 0.1562, G_A_loss: 0.5036, G_B_loss: 0.8653\n",
      "Epoch [13/30], Step [981/1067], D_A_loss: 0.1283, D_B_loss: 0.1309, G_A_loss: 0.4534, G_B_loss: 0.6007\n",
      "Epoch [13/30], Step [991/1067], D_A_loss: 0.0560, D_B_loss: 0.2353, G_A_loss: 1.3418, G_B_loss: 0.1630\n",
      "Epoch [13/30], Step [1001/1067], D_A_loss: 0.0538, D_B_loss: 0.0242, G_A_loss: 0.7980, G_B_loss: 0.4440\n",
      "Epoch [13/30], Step [1011/1067], D_A_loss: 0.0483, D_B_loss: 0.0912, G_A_loss: 0.4755, G_B_loss: 0.5834\n",
      "Epoch [13/30], Step [1021/1067], D_A_loss: 0.2199, D_B_loss: 0.0838, G_A_loss: 0.8916, G_B_loss: 0.1505\n",
      "Epoch [13/30], Step [1031/1067], D_A_loss: 0.1462, D_B_loss: 0.0790, G_A_loss: 0.4684, G_B_loss: 0.5552\n",
      "Epoch [13/30], Step [1041/1067], D_A_loss: 0.1432, D_B_loss: 0.1747, G_A_loss: 0.2336, G_B_loss: 0.2840\n",
      "Epoch [13/30], Step [1051/1067], D_A_loss: 0.1353, D_B_loss: 0.0219, G_A_loss: 0.5868, G_B_loss: 0.3604\n",
      "Epoch [13/30], Step [1061/1067], D_A_loss: 0.1262, D_B_loss: 0.0292, G_A_loss: 0.7485, G_B_loss: 0.4645\n",
      "Epoch [14/30], Step [1/1067], D_A_loss: 0.0897, D_B_loss: 0.0762, G_A_loss: 1.4047, G_B_loss: 0.2770\n",
      "Epoch [14/30], Step [11/1067], D_A_loss: 0.0515, D_B_loss: 0.1134, G_A_loss: 0.3905, G_B_loss: 0.5848\n",
      "Epoch [14/30], Step [21/1067], D_A_loss: 0.2753, D_B_loss: 0.0425, G_A_loss: 0.3270, G_B_loss: 0.0872\n",
      "Epoch [14/30], Step [31/1067], D_A_loss: 0.1095, D_B_loss: 0.1055, G_A_loss: 0.3967, G_B_loss: 0.1770\n",
      "Epoch [14/30], Step [41/1067], D_A_loss: 0.1053, D_B_loss: 0.0783, G_A_loss: 0.7049, G_B_loss: 0.4691\n",
      "Epoch [14/30], Step [51/1067], D_A_loss: 0.1440, D_B_loss: 0.1002, G_A_loss: 0.3756, G_B_loss: 0.2722\n",
      "Epoch [14/30], Step [61/1067], D_A_loss: 0.0352, D_B_loss: 0.0294, G_A_loss: 0.6425, G_B_loss: 0.3826\n",
      "Epoch [14/30], Step [71/1067], D_A_loss: 0.0381, D_B_loss: 0.0790, G_A_loss: 0.4824, G_B_loss: 0.4470\n",
      "Epoch [14/30], Step [81/1067], D_A_loss: 0.0911, D_B_loss: 0.1219, G_A_loss: 0.8287, G_B_loss: 0.6384\n",
      "Epoch [14/30], Step [91/1067], D_A_loss: 0.1474, D_B_loss: 0.1653, G_A_loss: 0.9122, G_B_loss: 0.7324\n",
      "Epoch [14/30], Step [101/1067], D_A_loss: 0.1241, D_B_loss: 0.1198, G_A_loss: 0.6772, G_B_loss: 0.4558\n",
      "Epoch [14/30], Step [111/1067], D_A_loss: 0.1065, D_B_loss: 0.0303, G_A_loss: 0.4637, G_B_loss: 0.3620\n",
      "Epoch [14/30], Step [121/1067], D_A_loss: 0.1051, D_B_loss: 0.1449, G_A_loss: 0.4992, G_B_loss: 0.3178\n",
      "Epoch [14/30], Step [131/1067], D_A_loss: 0.0402, D_B_loss: 0.0403, G_A_loss: 0.7057, G_B_loss: 0.7755\n",
      "Epoch [14/30], Step [141/1067], D_A_loss: 0.0533, D_B_loss: 0.1288, G_A_loss: 0.6222, G_B_loss: 0.2274\n",
      "Epoch [14/30], Step [151/1067], D_A_loss: 0.0851, D_B_loss: 0.0439, G_A_loss: 0.5238, G_B_loss: 0.8260\n",
      "Epoch [14/30], Step [161/1067], D_A_loss: 0.2699, D_B_loss: 0.0651, G_A_loss: 0.7625, G_B_loss: 0.4636\n",
      "Epoch [14/30], Step [171/1067], D_A_loss: 0.0692, D_B_loss: 0.1375, G_A_loss: 0.8130, G_B_loss: 0.6038\n",
      "Epoch [14/30], Step [181/1067], D_A_loss: 0.1260, D_B_loss: 0.0144, G_A_loss: 0.8924, G_B_loss: 0.3429\n",
      "Epoch [14/30], Step [191/1067], D_A_loss: 0.2025, D_B_loss: 0.1432, G_A_loss: 0.3289, G_B_loss: 0.4313\n",
      "Epoch [14/30], Step [201/1067], D_A_loss: 0.0861, D_B_loss: 0.0179, G_A_loss: 0.8852, G_B_loss: 0.4651\n",
      "Epoch [14/30], Step [211/1067], D_A_loss: 0.0691, D_B_loss: 0.0247, G_A_loss: 0.5627, G_B_loss: 0.7779\n",
      "Epoch [14/30], Step [221/1067], D_A_loss: 0.2370, D_B_loss: 0.0726, G_A_loss: 0.5038, G_B_loss: 0.2488\n",
      "Epoch [14/30], Step [231/1067], D_A_loss: 0.1896, D_B_loss: 0.0861, G_A_loss: 0.9799, G_B_loss: 0.2196\n",
      "Epoch [14/30], Step [241/1067], D_A_loss: 0.1598, D_B_loss: 0.3007, G_A_loss: 0.1560, G_B_loss: 0.3890\n",
      "Epoch [14/30], Step [251/1067], D_A_loss: 0.2749, D_B_loss: 0.0404, G_A_loss: 0.6713, G_B_loss: 0.4672\n",
      "Epoch [14/30], Step [261/1067], D_A_loss: 0.1270, D_B_loss: 0.0271, G_A_loss: 0.6349, G_B_loss: 0.8137\n",
      "Epoch [14/30], Step [271/1067], D_A_loss: 0.1012, D_B_loss: 0.1237, G_A_loss: 0.8684, G_B_loss: 0.4932\n",
      "Epoch [14/30], Step [281/1067], D_A_loss: 0.0154, D_B_loss: 0.0504, G_A_loss: 0.6100, G_B_loss: 1.0896\n",
      "Epoch [14/30], Step [291/1067], D_A_loss: 0.0209, D_B_loss: 0.1026, G_A_loss: 1.1795, G_B_loss: 0.2849\n",
      "Epoch [14/30], Step [301/1067], D_A_loss: 0.0917, D_B_loss: 0.0205, G_A_loss: 0.8868, G_B_loss: 0.4226\n",
      "Epoch [14/30], Step [311/1067], D_A_loss: 0.1262, D_B_loss: 0.0263, G_A_loss: 0.9786, G_B_loss: 0.3416\n",
      "Epoch [14/30], Step [321/1067], D_A_loss: 0.5593, D_B_loss: 0.0639, G_A_loss: 0.5465, G_B_loss: 0.5245\n",
      "Epoch [14/30], Step [331/1067], D_A_loss: 0.1173, D_B_loss: 0.1577, G_A_loss: 0.4517, G_B_loss: 0.5745\n",
      "Epoch [14/30], Step [341/1067], D_A_loss: 0.1125, D_B_loss: 0.0722, G_A_loss: 0.3979, G_B_loss: 0.8221\n",
      "Epoch [14/30], Step [351/1067], D_A_loss: 0.1438, D_B_loss: 0.0388, G_A_loss: 0.6930, G_B_loss: 0.8665\n",
      "Epoch [14/30], Step [361/1067], D_A_loss: 0.0889, D_B_loss: 0.0529, G_A_loss: 0.4569, G_B_loss: 0.4269\n",
      "Epoch [14/30], Step [371/1067], D_A_loss: 0.1202, D_B_loss: 0.0554, G_A_loss: 0.4483, G_B_loss: 0.1655\n",
      "Epoch [14/30], Step [381/1067], D_A_loss: 0.1183, D_B_loss: 0.1333, G_A_loss: 1.3556, G_B_loss: 1.0140\n",
      "Epoch [14/30], Step [391/1067], D_A_loss: 0.1106, D_B_loss: 0.3792, G_A_loss: 0.9951, G_B_loss: 0.6606\n",
      "Epoch [14/30], Step [401/1067], D_A_loss: 0.0689, D_B_loss: 0.0484, G_A_loss: 0.8423, G_B_loss: 0.5432\n",
      "Epoch [14/30], Step [411/1067], D_A_loss: 0.0487, D_B_loss: 0.3231, G_A_loss: 0.8930, G_B_loss: 0.6053\n",
      "Epoch [14/30], Step [421/1067], D_A_loss: 0.0891, D_B_loss: 0.1025, G_A_loss: 0.7097, G_B_loss: 0.4288\n",
      "Epoch [14/30], Step [431/1067], D_A_loss: 0.1668, D_B_loss: 0.0489, G_A_loss: 0.8663, G_B_loss: 0.6987\n",
      "Epoch [14/30], Step [441/1067], D_A_loss: 0.0922, D_B_loss: 0.0744, G_A_loss: 0.4997, G_B_loss: 0.7259\n",
      "Epoch [14/30], Step [451/1067], D_A_loss: 0.0283, D_B_loss: 0.1427, G_A_loss: 0.5216, G_B_loss: 0.3549\n",
      "Epoch [14/30], Step [461/1067], D_A_loss: 0.0893, D_B_loss: 0.2853, G_A_loss: 0.1459, G_B_loss: 0.3572\n",
      "Epoch [14/30], Step [471/1067], D_A_loss: 0.0438, D_B_loss: 0.0838, G_A_loss: 1.0480, G_B_loss: 0.9823\n",
      "Epoch [14/30], Step [481/1067], D_A_loss: 0.0810, D_B_loss: 0.1018, G_A_loss: 0.7370, G_B_loss: 0.6318\n",
      "Epoch [14/30], Step [491/1067], D_A_loss: 0.0314, D_B_loss: 0.0532, G_A_loss: 0.7715, G_B_loss: 0.3593\n",
      "Epoch [14/30], Step [501/1067], D_A_loss: 0.0487, D_B_loss: 0.0414, G_A_loss: 0.4099, G_B_loss: 0.7261\n",
      "Epoch [14/30], Step [511/1067], D_A_loss: 0.1430, D_B_loss: 0.1118, G_A_loss: 0.4438, G_B_loss: 0.3167\n",
      "Epoch [14/30], Step [521/1067], D_A_loss: 0.0446, D_B_loss: 0.0894, G_A_loss: 0.5342, G_B_loss: 0.6024\n",
      "Epoch [14/30], Step [531/1067], D_A_loss: 0.0553, D_B_loss: 0.1458, G_A_loss: 0.2613, G_B_loss: 0.3606\n",
      "Epoch [14/30], Step [541/1067], D_A_loss: 0.0873, D_B_loss: 0.0827, G_A_loss: 0.9657, G_B_loss: 0.4950\n",
      "Epoch [14/30], Step [551/1067], D_A_loss: 0.0413, D_B_loss: 0.0657, G_A_loss: 0.8799, G_B_loss: 0.2980\n",
      "Epoch [14/30], Step [561/1067], D_A_loss: 0.1145, D_B_loss: 0.0331, G_A_loss: 1.4244, G_B_loss: 0.3761\n",
      "Epoch [14/30], Step [571/1067], D_A_loss: 0.0555, D_B_loss: 0.0614, G_A_loss: 0.5383, G_B_loss: 0.9769\n",
      "Epoch [14/30], Step [581/1067], D_A_loss: 0.1200, D_B_loss: 0.0623, G_A_loss: 0.8955, G_B_loss: 0.3306\n",
      "Epoch [14/30], Step [591/1067], D_A_loss: 0.1957, D_B_loss: 0.0203, G_A_loss: 0.8655, G_B_loss: 0.4447\n",
      "Epoch [14/30], Step [601/1067], D_A_loss: 0.2360, D_B_loss: 0.1054, G_A_loss: 0.5325, G_B_loss: 0.1586\n",
      "Epoch [14/30], Step [611/1067], D_A_loss: 0.2466, D_B_loss: 0.0328, G_A_loss: 0.2954, G_B_loss: 0.2126\n",
      "Epoch [14/30], Step [621/1067], D_A_loss: 0.1216, D_B_loss: 0.2970, G_A_loss: 0.1058, G_B_loss: 0.4427\n",
      "Epoch [14/30], Step [631/1067], D_A_loss: 0.1059, D_B_loss: 0.1931, G_A_loss: 0.3635, G_B_loss: 0.4724\n",
      "Epoch [14/30], Step [641/1067], D_A_loss: 0.2013, D_B_loss: 0.1608, G_A_loss: 0.3825, G_B_loss: 0.3367\n",
      "Epoch [14/30], Step [651/1067], D_A_loss: 0.0747, D_B_loss: 0.0223, G_A_loss: 0.5091, G_B_loss: 0.5465\n",
      "Epoch [14/30], Step [661/1067], D_A_loss: 0.2791, D_B_loss: 0.0259, G_A_loss: 0.5944, G_B_loss: 0.6681\n",
      "Epoch [14/30], Step [671/1067], D_A_loss: 0.2008, D_B_loss: 0.0956, G_A_loss: 0.1980, G_B_loss: 0.3163\n",
      "Epoch [14/30], Step [681/1067], D_A_loss: 0.0548, D_B_loss: 0.0926, G_A_loss: 0.8033, G_B_loss: 0.2590\n",
      "Epoch [14/30], Step [691/1067], D_A_loss: 0.0626, D_B_loss: 0.1265, G_A_loss: 0.5520, G_B_loss: 0.5862\n",
      "Epoch [14/30], Step [701/1067], D_A_loss: 0.1047, D_B_loss: 0.0495, G_A_loss: 0.6151, G_B_loss: 0.3896\n",
      "Epoch [14/30], Step [711/1067], D_A_loss: 0.1038, D_B_loss: 0.1305, G_A_loss: 0.4120, G_B_loss: 0.6489\n",
      "Epoch [14/30], Step [721/1067], D_A_loss: 0.1652, D_B_loss: 0.1476, G_A_loss: 0.4406, G_B_loss: 0.2241\n",
      "Epoch [14/30], Step [731/1067], D_A_loss: 0.1237, D_B_loss: 0.1090, G_A_loss: 0.7072, G_B_loss: 0.3176\n",
      "Epoch [14/30], Step [741/1067], D_A_loss: 0.0361, D_B_loss: 0.0943, G_A_loss: 0.4204, G_B_loss: 0.6519\n",
      "Epoch [14/30], Step [751/1067], D_A_loss: 0.0860, D_B_loss: 0.0905, G_A_loss: 0.2282, G_B_loss: 0.4297\n",
      "Epoch [14/30], Step [761/1067], D_A_loss: 0.2286, D_B_loss: 0.0207, G_A_loss: 0.6734, G_B_loss: 0.7323\n",
      "Epoch [14/30], Step [771/1067], D_A_loss: 0.3369, D_B_loss: 0.1031, G_A_loss: 0.5177, G_B_loss: 0.0952\n",
      "Epoch [14/30], Step [781/1067], D_A_loss: 0.1808, D_B_loss: 0.0922, G_A_loss: 0.4046, G_B_loss: 0.3659\n",
      "Epoch [14/30], Step [791/1067], D_A_loss: 0.1191, D_B_loss: 0.0916, G_A_loss: 0.4101, G_B_loss: 1.1831\n",
      "Epoch [14/30], Step [801/1067], D_A_loss: 0.1196, D_B_loss: 0.1638, G_A_loss: 0.6415, G_B_loss: 0.3415\n",
      "Epoch [14/30], Step [811/1067], D_A_loss: 0.0213, D_B_loss: 0.0453, G_A_loss: 0.6191, G_B_loss: 0.8827\n",
      "Epoch [14/30], Step [821/1067], D_A_loss: 0.0373, D_B_loss: 0.0223, G_A_loss: 1.0114, G_B_loss: 0.7279\n",
      "Epoch [14/30], Step [831/1067], D_A_loss: 0.0811, D_B_loss: 0.0705, G_A_loss: 0.3270, G_B_loss: 1.2144\n",
      "Epoch [14/30], Step [841/1067], D_A_loss: 0.1713, D_B_loss: 0.2919, G_A_loss: 0.5570, G_B_loss: 0.2202\n",
      "Epoch [14/30], Step [851/1067], D_A_loss: 0.1337, D_B_loss: 0.2525, G_A_loss: 0.5143, G_B_loss: 0.1977\n",
      "Epoch [14/30], Step [861/1067], D_A_loss: 0.0400, D_B_loss: 0.0402, G_A_loss: 0.5774, G_B_loss: 0.3823\n",
      "Epoch [14/30], Step [871/1067], D_A_loss: 0.1401, D_B_loss: 0.1323, G_A_loss: 0.6859, G_B_loss: 0.7595\n",
      "Epoch [14/30], Step [881/1067], D_A_loss: 0.0967, D_B_loss: 0.0924, G_A_loss: 0.4743, G_B_loss: 0.2471\n",
      "Epoch [14/30], Step [891/1067], D_A_loss: 0.0424, D_B_loss: 0.1232, G_A_loss: 0.8181, G_B_loss: 0.4848\n",
      "Epoch [14/30], Step [901/1067], D_A_loss: 0.1274, D_B_loss: 0.0284, G_A_loss: 0.8144, G_B_loss: 0.3584\n",
      "Epoch [14/30], Step [911/1067], D_A_loss: 0.1942, D_B_loss: 0.1183, G_A_loss: 1.0423, G_B_loss: 0.7870\n",
      "Epoch [14/30], Step [921/1067], D_A_loss: 0.1856, D_B_loss: 0.0190, G_A_loss: 0.9258, G_B_loss: 1.1189\n",
      "Epoch [14/30], Step [931/1067], D_A_loss: 0.2210, D_B_loss: 0.0720, G_A_loss: 0.2470, G_B_loss: 0.7098\n",
      "Epoch [14/30], Step [941/1067], D_A_loss: 0.0259, D_B_loss: 0.0861, G_A_loss: 0.4285, G_B_loss: 0.7463\n",
      "Epoch [14/30], Step [951/1067], D_A_loss: 0.1215, D_B_loss: 0.0375, G_A_loss: 0.1213, G_B_loss: 0.3291\n",
      "Epoch [14/30], Step [961/1067], D_A_loss: 0.0792, D_B_loss: 0.0961, G_A_loss: 0.5120, G_B_loss: 0.3455\n",
      "Epoch [14/30], Step [971/1067], D_A_loss: 0.1296, D_B_loss: 0.0409, G_A_loss: 0.7586, G_B_loss: 0.3729\n",
      "Epoch [14/30], Step [981/1067], D_A_loss: 0.0237, D_B_loss: 0.0275, G_A_loss: 0.6775, G_B_loss: 0.4887\n",
      "Epoch [14/30], Step [991/1067], D_A_loss: 0.0314, D_B_loss: 0.0563, G_A_loss: 0.5792, G_B_loss: 0.8920\n",
      "Epoch [14/30], Step [1001/1067], D_A_loss: 0.1955, D_B_loss: 0.0592, G_A_loss: 0.3647, G_B_loss: 0.7186\n",
      "Epoch [14/30], Step [1011/1067], D_A_loss: 0.1126, D_B_loss: 0.0773, G_A_loss: 0.2698, G_B_loss: 0.4384\n",
      "Epoch [14/30], Step [1021/1067], D_A_loss: 0.0241, D_B_loss: 0.1293, G_A_loss: 0.5489, G_B_loss: 0.2428\n",
      "Epoch [14/30], Step [1031/1067], D_A_loss: 0.1910, D_B_loss: 0.0833, G_A_loss: 0.4599, G_B_loss: 0.3965\n",
      "Epoch [14/30], Step [1041/1067], D_A_loss: 0.1327, D_B_loss: 0.1372, G_A_loss: 0.3517, G_B_loss: 0.3775\n",
      "Epoch [14/30], Step [1051/1067], D_A_loss: 0.0509, D_B_loss: 0.1129, G_A_loss: 0.7395, G_B_loss: 0.5711\n",
      "Epoch [14/30], Step [1061/1067], D_A_loss: 0.0429, D_B_loss: 0.2055, G_A_loss: 0.8917, G_B_loss: 0.6980\n",
      "Epoch [15/30], Step [1/1067], D_A_loss: 0.0824, D_B_loss: 0.1513, G_A_loss: 0.9380, G_B_loss: 0.3578\n",
      "Epoch [15/30], Step [11/1067], D_A_loss: 0.0439, D_B_loss: 0.1365, G_A_loss: 0.3510, G_B_loss: 0.7456\n",
      "Epoch [15/30], Step [21/1067], D_A_loss: 0.1196, D_B_loss: 0.0182, G_A_loss: 0.8097, G_B_loss: 0.5822\n",
      "Epoch [15/30], Step [31/1067], D_A_loss: 0.1664, D_B_loss: 0.1338, G_A_loss: 0.3531, G_B_loss: 0.6256\n",
      "Epoch [15/30], Step [41/1067], D_A_loss: 0.2188, D_B_loss: 0.0232, G_A_loss: 1.0266, G_B_loss: 1.1241\n",
      "Epoch [15/30], Step [51/1067], D_A_loss: 0.1491, D_B_loss: 0.0612, G_A_loss: 0.6535, G_B_loss: 0.6203\n",
      "Epoch [15/30], Step [61/1067], D_A_loss: 0.0783, D_B_loss: 0.2650, G_A_loss: 0.7530, G_B_loss: 0.6236\n",
      "Epoch [15/30], Step [71/1067], D_A_loss: 0.2001, D_B_loss: 0.0214, G_A_loss: 0.7374, G_B_loss: 0.4051\n",
      "Epoch [15/30], Step [81/1067], D_A_loss: 0.3276, D_B_loss: 0.0495, G_A_loss: 0.7286, G_B_loss: 0.1587\n",
      "Epoch [15/30], Step [91/1067], D_A_loss: 0.0958, D_B_loss: 0.0487, G_A_loss: 0.4496, G_B_loss: 0.4174\n",
      "Epoch [15/30], Step [101/1067], D_A_loss: 0.2676, D_B_loss: 0.0364, G_A_loss: 0.8469, G_B_loss: 0.7283\n",
      "Epoch [15/30], Step [111/1067], D_A_loss: 0.0845, D_B_loss: 0.1141, G_A_loss: 0.3334, G_B_loss: 0.4785\n",
      "Epoch [15/30], Step [121/1067], D_A_loss: 0.1804, D_B_loss: 0.0598, G_A_loss: 0.3039, G_B_loss: 1.3961\n",
      "Epoch [15/30], Step [131/1067], D_A_loss: 0.1288, D_B_loss: 0.1431, G_A_loss: 0.9307, G_B_loss: 0.9806\n",
      "Epoch [15/30], Step [141/1067], D_A_loss: 0.2926, D_B_loss: 0.2581, G_A_loss: 0.1155, G_B_loss: 0.6886\n",
      "Epoch [15/30], Step [151/1067], D_A_loss: 0.0935, D_B_loss: 0.0587, G_A_loss: 0.7253, G_B_loss: 0.9592\n",
      "Epoch [15/30], Step [161/1067], D_A_loss: 0.1973, D_B_loss: 0.1833, G_A_loss: 0.1984, G_B_loss: 0.1793\n",
      "Epoch [15/30], Step [171/1067], D_A_loss: 0.1768, D_B_loss: 0.1084, G_A_loss: 0.7965, G_B_loss: 0.2630\n",
      "Epoch [15/30], Step [181/1067], D_A_loss: 0.0560, D_B_loss: 0.0355, G_A_loss: 0.5324, G_B_loss: 0.4267\n",
      "Epoch [15/30], Step [191/1067], D_A_loss: 0.1148, D_B_loss: 0.0994, G_A_loss: 0.2424, G_B_loss: 0.4866\n",
      "Epoch [15/30], Step [201/1067], D_A_loss: 0.2303, D_B_loss: 0.0282, G_A_loss: 0.8438, G_B_loss: 0.5727\n",
      "Epoch [15/30], Step [211/1067], D_A_loss: 0.0827, D_B_loss: 0.0900, G_A_loss: 0.5873, G_B_loss: 0.4473\n",
      "Epoch [15/30], Step [221/1067], D_A_loss: 0.0676, D_B_loss: 0.0846, G_A_loss: 0.3463, G_B_loss: 0.5726\n",
      "Epoch [15/30], Step [231/1067], D_A_loss: 0.3878, D_B_loss: 0.1520, G_A_loss: 0.4618, G_B_loss: 0.2581\n",
      "Epoch [15/30], Step [241/1067], D_A_loss: 0.0802, D_B_loss: 0.1692, G_A_loss: 1.3528, G_B_loss: 0.0771\n",
      "Epoch [15/30], Step [251/1067], D_A_loss: 0.0871, D_B_loss: 0.0662, G_A_loss: 0.8782, G_B_loss: 0.2349\n",
      "Epoch [15/30], Step [261/1067], D_A_loss: 0.0745, D_B_loss: 0.1079, G_A_loss: 0.5505, G_B_loss: 0.7382\n",
      "Epoch [15/30], Step [271/1067], D_A_loss: 0.1603, D_B_loss: 0.0995, G_A_loss: 0.6616, G_B_loss: 0.2913\n",
      "Epoch [15/30], Step [281/1067], D_A_loss: 0.0945, D_B_loss: 0.0627, G_A_loss: 0.2730, G_B_loss: 0.8185\n",
      "Epoch [15/30], Step [291/1067], D_A_loss: 0.2474, D_B_loss: 0.2587, G_A_loss: 1.1055, G_B_loss: 0.2451\n",
      "Epoch [15/30], Step [301/1067], D_A_loss: 0.1066, D_B_loss: 0.1334, G_A_loss: 0.7199, G_B_loss: 0.4154\n",
      "Epoch [15/30], Step [311/1067], D_A_loss: 0.1205, D_B_loss: 0.1850, G_A_loss: 0.3201, G_B_loss: 0.5076\n",
      "Epoch [15/30], Step [321/1067], D_A_loss: 0.2595, D_B_loss: 0.1687, G_A_loss: 0.7152, G_B_loss: 0.4400\n",
      "Epoch [15/30], Step [331/1067], D_A_loss: 0.1290, D_B_loss: 0.0954, G_A_loss: 0.3790, G_B_loss: 0.8664\n",
      "Epoch [15/30], Step [341/1067], D_A_loss: 0.2349, D_B_loss: 0.1747, G_A_loss: 0.2363, G_B_loss: 0.1562\n",
      "Epoch [15/30], Step [351/1067], D_A_loss: 0.1860, D_B_loss: 0.0415, G_A_loss: 0.7615, G_B_loss: 0.2856\n",
      "Epoch [15/30], Step [361/1067], D_A_loss: 0.0445, D_B_loss: 0.0302, G_A_loss: 0.8170, G_B_loss: 0.6983\n",
      "Epoch [15/30], Step [371/1067], D_A_loss: 0.0289, D_B_loss: 0.0306, G_A_loss: 0.9028, G_B_loss: 1.0747\n",
      "Epoch [15/30], Step [381/1067], D_A_loss: 0.3436, D_B_loss: 0.0335, G_A_loss: 0.3097, G_B_loss: 0.2543\n",
      "Epoch [15/30], Step [391/1067], D_A_loss: 0.0194, D_B_loss: 0.1977, G_A_loss: 0.2271, G_B_loss: 0.1856\n",
      "Epoch [15/30], Step [401/1067], D_A_loss: 0.2062, D_B_loss: 0.0583, G_A_loss: 0.2226, G_B_loss: 0.3020\n",
      "Epoch [15/30], Step [411/1067], D_A_loss: 0.1915, D_B_loss: 0.2064, G_A_loss: 0.4108, G_B_loss: 0.2332\n",
      "Epoch [15/30], Step [421/1067], D_A_loss: 0.1364, D_B_loss: 0.0931, G_A_loss: 0.9447, G_B_loss: 0.3460\n",
      "Epoch [15/30], Step [431/1067], D_A_loss: 0.0965, D_B_loss: 0.1183, G_A_loss: 0.4079, G_B_loss: 0.3831\n",
      "Epoch [15/30], Step [441/1067], D_A_loss: 0.1306, D_B_loss: 0.0814, G_A_loss: 0.4104, G_B_loss: 0.4272\n",
      "Epoch [15/30], Step [451/1067], D_A_loss: 0.2644, D_B_loss: 0.0275, G_A_loss: 0.8593, G_B_loss: 0.1419\n",
      "Epoch [15/30], Step [461/1067], D_A_loss: 0.0279, D_B_loss: 0.0712, G_A_loss: 0.9815, G_B_loss: 0.9278\n",
      "Epoch [15/30], Step [471/1067], D_A_loss: 0.1543, D_B_loss: 0.0578, G_A_loss: 0.5718, G_B_loss: 0.5711\n",
      "Epoch [15/30], Step [481/1067], D_A_loss: 0.0426, D_B_loss: 0.0403, G_A_loss: 0.1665, G_B_loss: 0.7278\n",
      "Epoch [15/30], Step [491/1067], D_A_loss: 0.1599, D_B_loss: 0.0648, G_A_loss: 0.6945, G_B_loss: 0.6320\n",
      "Epoch [15/30], Step [501/1067], D_A_loss: 0.1786, D_B_loss: 0.0225, G_A_loss: 0.3295, G_B_loss: 0.4409\n",
      "Epoch [15/30], Step [511/1067], D_A_loss: 0.0341, D_B_loss: 0.1411, G_A_loss: 0.5998, G_B_loss: 0.6458\n",
      "Epoch [15/30], Step [521/1067], D_A_loss: 0.1324, D_B_loss: 0.2052, G_A_loss: 0.3177, G_B_loss: 0.4014\n",
      "Epoch [15/30], Step [531/1067], D_A_loss: 0.1390, D_B_loss: 0.0617, G_A_loss: 0.4985, G_B_loss: 0.3662\n",
      "Epoch [15/30], Step [541/1067], D_A_loss: 0.0394, D_B_loss: 0.2422, G_A_loss: 0.1514, G_B_loss: 0.5987\n",
      "Epoch [15/30], Step [551/1067], D_A_loss: 0.1349, D_B_loss: 0.0553, G_A_loss: 0.5301, G_B_loss: 0.4947\n",
      "Epoch [15/30], Step [561/1067], D_A_loss: 0.5729, D_B_loss: 0.0307, G_A_loss: 0.3752, G_B_loss: 0.1214\n",
      "Epoch [15/30], Step [571/1067], D_A_loss: 0.2054, D_B_loss: 0.0723, G_A_loss: 0.5284, G_B_loss: 0.9063\n",
      "Epoch [15/30], Step [581/1067], D_A_loss: 0.0982, D_B_loss: 0.0758, G_A_loss: 0.6885, G_B_loss: 0.6463\n",
      "Epoch [15/30], Step [591/1067], D_A_loss: 0.0824, D_B_loss: 0.2001, G_A_loss: 0.3881, G_B_loss: 0.7952\n",
      "Epoch [15/30], Step [601/1067], D_A_loss: 0.1974, D_B_loss: 0.0900, G_A_loss: 0.3828, G_B_loss: 0.6389\n",
      "Epoch [15/30], Step [611/1067], D_A_loss: 0.1810, D_B_loss: 0.1032, G_A_loss: 0.5420, G_B_loss: 0.3351\n",
      "Epoch [15/30], Step [621/1067], D_A_loss: 0.1601, D_B_loss: 0.1088, G_A_loss: 0.4353, G_B_loss: 0.4239\n",
      "Epoch [15/30], Step [631/1067], D_A_loss: 0.0802, D_B_loss: 0.0673, G_A_loss: 0.8803, G_B_loss: 0.4608\n",
      "Epoch [15/30], Step [641/1067], D_A_loss: 0.0595, D_B_loss: 0.0832, G_A_loss: 0.6076, G_B_loss: 0.3076\n",
      "Epoch [15/30], Step [651/1067], D_A_loss: 0.0721, D_B_loss: 0.1038, G_A_loss: 0.6484, G_B_loss: 0.8023\n",
      "Epoch [15/30], Step [661/1067], D_A_loss: 0.0777, D_B_loss: 0.0602, G_A_loss: 0.4123, G_B_loss: 0.3359\n",
      "Epoch [15/30], Step [671/1067], D_A_loss: 0.1442, D_B_loss: 0.0749, G_A_loss: 0.8146, G_B_loss: 0.3552\n",
      "Epoch [15/30], Step [681/1067], D_A_loss: 0.2627, D_B_loss: 0.1974, G_A_loss: 0.3796, G_B_loss: 0.3419\n",
      "Epoch [15/30], Step [691/1067], D_A_loss: 0.0871, D_B_loss: 0.1594, G_A_loss: 0.5728, G_B_loss: 0.9683\n",
      "Epoch [15/30], Step [701/1067], D_A_loss: 0.0615, D_B_loss: 0.0961, G_A_loss: 0.4858, G_B_loss: 0.9537\n",
      "Epoch [15/30], Step [711/1067], D_A_loss: 0.0774, D_B_loss: 0.1296, G_A_loss: 0.4981, G_B_loss: 0.7275\n",
      "Epoch [15/30], Step [721/1067], D_A_loss: 0.0616, D_B_loss: 0.1099, G_A_loss: 0.3527, G_B_loss: 0.3223\n",
      "Epoch [15/30], Step [731/1067], D_A_loss: 0.0770, D_B_loss: 0.0704, G_A_loss: 0.5161, G_B_loss: 0.4605\n",
      "Epoch [15/30], Step [741/1067], D_A_loss: 0.1619, D_B_loss: 0.0504, G_A_loss: 0.5986, G_B_loss: 0.2705\n",
      "Epoch [15/30], Step [751/1067], D_A_loss: 0.0582, D_B_loss: 0.0396, G_A_loss: 0.9993, G_B_loss: 0.4959\n",
      "Epoch [15/30], Step [761/1067], D_A_loss: 0.1574, D_B_loss: 0.0312, G_A_loss: 0.8328, G_B_loss: 0.2740\n",
      "Epoch [15/30], Step [771/1067], D_A_loss: 0.2538, D_B_loss: 0.0742, G_A_loss: 0.6597, G_B_loss: 0.8037\n",
      "Epoch [15/30], Step [781/1067], D_A_loss: 0.1830, D_B_loss: 0.1245, G_A_loss: 0.3302, G_B_loss: 0.2528\n",
      "Epoch [15/30], Step [791/1067], D_A_loss: 0.0253, D_B_loss: 0.1190, G_A_loss: 0.4981, G_B_loss: 0.2807\n",
      "Epoch [15/30], Step [801/1067], D_A_loss: 0.1581, D_B_loss: 0.1372, G_A_loss: 0.3521, G_B_loss: 0.3471\n",
      "Epoch [15/30], Step [811/1067], D_A_loss: 0.0695, D_B_loss: 0.0693, G_A_loss: 0.6339, G_B_loss: 0.3358\n",
      "Epoch [15/30], Step [821/1067], D_A_loss: 0.1244, D_B_loss: 0.0459, G_A_loss: 0.4457, G_B_loss: 0.5113\n",
      "Epoch [15/30], Step [831/1067], D_A_loss: 0.1724, D_B_loss: 0.0758, G_A_loss: 0.4043, G_B_loss: 0.6198\n",
      "Epoch [15/30], Step [841/1067], D_A_loss: 0.0408, D_B_loss: 0.1911, G_A_loss: 0.6601, G_B_loss: 0.2149\n",
      "Epoch [15/30], Step [851/1067], D_A_loss: 0.0304, D_B_loss: 0.0692, G_A_loss: 0.9706, G_B_loss: 0.3638\n",
      "Epoch [15/30], Step [861/1067], D_A_loss: 0.0574, D_B_loss: 0.0898, G_A_loss: 0.3066, G_B_loss: 0.7085\n",
      "Epoch [15/30], Step [871/1067], D_A_loss: 0.2134, D_B_loss: 0.0158, G_A_loss: 0.6194, G_B_loss: 0.6356\n",
      "Epoch [15/30], Step [881/1067], D_A_loss: 0.0531, D_B_loss: 0.2081, G_A_loss: 0.3493, G_B_loss: 0.1569\n",
      "Epoch [15/30], Step [891/1067], D_A_loss: 0.1916, D_B_loss: 0.0708, G_A_loss: 0.3939, G_B_loss: 0.2114\n",
      "Epoch [15/30], Step [901/1067], D_A_loss: 0.1197, D_B_loss: 0.0586, G_A_loss: 0.8353, G_B_loss: 0.5218\n",
      "Epoch [15/30], Step [911/1067], D_A_loss: 0.0823, D_B_loss: 0.0596, G_A_loss: 0.5567, G_B_loss: 0.7081\n",
      "Epoch [15/30], Step [921/1067], D_A_loss: 0.0700, D_B_loss: 0.0718, G_A_loss: 0.5572, G_B_loss: 0.6882\n",
      "Epoch [15/30], Step [931/1067], D_A_loss: 0.0491, D_B_loss: 0.0419, G_A_loss: 0.7056, G_B_loss: 0.2929\n",
      "Epoch [15/30], Step [941/1067], D_A_loss: 0.1054, D_B_loss: 0.0725, G_A_loss: 1.1059, G_B_loss: 0.5106\n",
      "Epoch [15/30], Step [951/1067], D_A_loss: 0.0804, D_B_loss: 0.1102, G_A_loss: 0.1953, G_B_loss: 0.2586\n",
      "Epoch [15/30], Step [961/1067], D_A_loss: 0.1314, D_B_loss: 0.0362, G_A_loss: 0.2008, G_B_loss: 0.4443\n",
      "Epoch [15/30], Step [971/1067], D_A_loss: 0.1653, D_B_loss: 0.1125, G_A_loss: 0.5567, G_B_loss: 0.3869\n",
      "Epoch [15/30], Step [981/1067], D_A_loss: 0.0844, D_B_loss: 0.1123, G_A_loss: 0.9844, G_B_loss: 0.6270\n",
      "Epoch [15/30], Step [991/1067], D_A_loss: 0.1835, D_B_loss: 0.0833, G_A_loss: 0.3979, G_B_loss: 0.6421\n",
      "Epoch [15/30], Step [1001/1067], D_A_loss: 0.1635, D_B_loss: 0.1958, G_A_loss: 0.1908, G_B_loss: 0.2775\n",
      "Epoch [15/30], Step [1011/1067], D_A_loss: 0.1070, D_B_loss: 0.0926, G_A_loss: 0.4636, G_B_loss: 0.4711\n",
      "Epoch [15/30], Step [1021/1067], D_A_loss: 0.2921, D_B_loss: 0.0838, G_A_loss: 1.0851, G_B_loss: 0.2278\n",
      "Epoch [15/30], Step [1031/1067], D_A_loss: 0.0500, D_B_loss: 0.0329, G_A_loss: 0.3951, G_B_loss: 0.5295\n",
      "Epoch [15/30], Step [1041/1067], D_A_loss: 0.0697, D_B_loss: 0.1757, G_A_loss: 0.7541, G_B_loss: 0.6248\n",
      "Epoch [15/30], Step [1051/1067], D_A_loss: 0.1440, D_B_loss: 0.1003, G_A_loss: 0.4277, G_B_loss: 0.4783\n",
      "Epoch [15/30], Step [1061/1067], D_A_loss: 0.0943, D_B_loss: 0.3171, G_A_loss: 0.6733, G_B_loss: 0.0797\n",
      "Epoch [16/30], Step [1/1067], D_A_loss: 0.1696, D_B_loss: 0.1842, G_A_loss: 0.6717, G_B_loss: 0.2873\n",
      "Epoch [16/30], Step [11/1067], D_A_loss: 0.0697, D_B_loss: 0.0203, G_A_loss: 0.5073, G_B_loss: 0.3483\n",
      "Epoch [16/30], Step [21/1067], D_A_loss: 0.1967, D_B_loss: 0.2731, G_A_loss: 0.1601, G_B_loss: 1.2271\n",
      "Epoch [16/30], Step [31/1067], D_A_loss: 0.1045, D_B_loss: 0.0213, G_A_loss: 0.3109, G_B_loss: 0.5341\n",
      "Epoch [16/30], Step [41/1067], D_A_loss: 0.0769, D_B_loss: 0.0312, G_A_loss: 0.3683, G_B_loss: 0.3537\n",
      "Epoch [16/30], Step [51/1067], D_A_loss: 0.1506, D_B_loss: 0.1754, G_A_loss: 0.3648, G_B_loss: 0.2929\n",
      "Epoch [16/30], Step [61/1067], D_A_loss: 0.0615, D_B_loss: 0.0203, G_A_loss: 0.3727, G_B_loss: 0.7314\n",
      "Epoch [16/30], Step [71/1067], D_A_loss: 0.0958, D_B_loss: 0.0299, G_A_loss: 0.4464, G_B_loss: 0.5179\n",
      "Epoch [16/30], Step [81/1067], D_A_loss: 0.1734, D_B_loss: 0.2565, G_A_loss: 0.4051, G_B_loss: 0.1352\n",
      "Epoch [16/30], Step [91/1067], D_A_loss: 0.0983, D_B_loss: 0.1074, G_A_loss: 0.7627, G_B_loss: 0.4308\n",
      "Epoch [16/30], Step [101/1067], D_A_loss: 0.0615, D_B_loss: 0.1246, G_A_loss: 0.3012, G_B_loss: 0.4622\n",
      "Epoch [16/30], Step [111/1067], D_A_loss: 0.1071, D_B_loss: 0.0305, G_A_loss: 0.6093, G_B_loss: 0.5282\n",
      "Epoch [16/30], Step [121/1067], D_A_loss: 0.0310, D_B_loss: 0.0621, G_A_loss: 0.3343, G_B_loss: 0.2865\n",
      "Epoch [16/30], Step [131/1067], D_A_loss: 0.1434, D_B_loss: 0.1025, G_A_loss: 0.4994, G_B_loss: 0.2859\n",
      "Epoch [16/30], Step [141/1067], D_A_loss: 0.0829, D_B_loss: 0.0897, G_A_loss: 0.5906, G_B_loss: 0.6340\n",
      "Epoch [16/30], Step [151/1067], D_A_loss: 0.1136, D_B_loss: 0.0468, G_A_loss: 0.5705, G_B_loss: 0.4524\n",
      "Epoch [16/30], Step [161/1067], D_A_loss: 0.1022, D_B_loss: 0.1063, G_A_loss: 0.3666, G_B_loss: 0.7408\n",
      "Epoch [16/30], Step [171/1067], D_A_loss: 0.0207, D_B_loss: 0.0346, G_A_loss: 1.0351, G_B_loss: 0.3916\n",
      "Epoch [16/30], Step [181/1067], D_A_loss: 0.0625, D_B_loss: 0.0603, G_A_loss: 0.6149, G_B_loss: 0.2974\n",
      "Epoch [16/30], Step [191/1067], D_A_loss: 0.2373, D_B_loss: 0.0693, G_A_loss: 0.5511, G_B_loss: 0.4650\n",
      "Epoch [16/30], Step [201/1067], D_A_loss: 0.1110, D_B_loss: 0.1058, G_A_loss: 0.4317, G_B_loss: 0.2029\n",
      "Epoch [16/30], Step [211/1067], D_A_loss: 0.0263, D_B_loss: 0.1139, G_A_loss: 0.3649, G_B_loss: 0.9748\n",
      "Epoch [16/30], Step [221/1067], D_A_loss: 0.2569, D_B_loss: 0.0348, G_A_loss: 0.1643, G_B_loss: 0.4790\n",
      "Epoch [16/30], Step [231/1067], D_A_loss: 0.0269, D_B_loss: 0.0267, G_A_loss: 0.6853, G_B_loss: 0.9432\n",
      "Epoch [16/30], Step [241/1067], D_A_loss: 0.1717, D_B_loss: 0.1037, G_A_loss: 0.9361, G_B_loss: 0.4822\n",
      "Epoch [16/30], Step [251/1067], D_A_loss: 0.0960, D_B_loss: 0.0509, G_A_loss: 0.8021, G_B_loss: 0.4407\n",
      "Epoch [16/30], Step [261/1067], D_A_loss: 0.1234, D_B_loss: 0.0302, G_A_loss: 0.9434, G_B_loss: 0.3678\n",
      "Epoch [16/30], Step [271/1067], D_A_loss: 0.2063, D_B_loss: 0.0328, G_A_loss: 0.7169, G_B_loss: 0.6595\n",
      "Epoch [16/30], Step [281/1067], D_A_loss: 0.1445, D_B_loss: 0.1015, G_A_loss: 0.5284, G_B_loss: 0.5665\n",
      "Epoch [16/30], Step [291/1067], D_A_loss: 0.0570, D_B_loss: 0.0489, G_A_loss: 0.3078, G_B_loss: 0.7836\n",
      "Epoch [16/30], Step [301/1067], D_A_loss: 0.0678, D_B_loss: 0.0305, G_A_loss: 0.4973, G_B_loss: 0.5871\n",
      "Epoch [16/30], Step [311/1067], D_A_loss: 0.2721, D_B_loss: 0.4781, G_A_loss: 0.0810, G_B_loss: 0.4866\n",
      "Epoch [16/30], Step [321/1067], D_A_loss: 0.1465, D_B_loss: 0.0710, G_A_loss: 0.3043, G_B_loss: 0.4278\n",
      "Epoch [16/30], Step [331/1067], D_A_loss: 0.0727, D_B_loss: 0.0205, G_A_loss: 0.7622, G_B_loss: 0.4075\n",
      "Epoch [16/30], Step [341/1067], D_A_loss: 0.0881, D_B_loss: 0.0531, G_A_loss: 0.5531, G_B_loss: 0.2737\n",
      "Epoch [16/30], Step [351/1067], D_A_loss: 0.1197, D_B_loss: 0.2088, G_A_loss: 0.4865, G_B_loss: 0.7227\n",
      "Epoch [16/30], Step [361/1067], D_A_loss: 0.0713, D_B_loss: 0.1509, G_A_loss: 1.0996, G_B_loss: 0.6854\n",
      "Epoch [16/30], Step [371/1067], D_A_loss: 0.0837, D_B_loss: 0.1003, G_A_loss: 0.3578, G_B_loss: 0.3639\n",
      "Epoch [16/30], Step [381/1067], D_A_loss: 0.0435, D_B_loss: 0.0551, G_A_loss: 0.5531, G_B_loss: 0.4302\n",
      "Epoch [16/30], Step [391/1067], D_A_loss: 0.1925, D_B_loss: 0.0767, G_A_loss: 0.5493, G_B_loss: 0.3649\n",
      "Epoch [16/30], Step [401/1067], D_A_loss: 0.2045, D_B_loss: 0.1487, G_A_loss: 0.4162, G_B_loss: 0.4677\n",
      "Epoch [16/30], Step [411/1067], D_A_loss: 0.0599, D_B_loss: 0.1216, G_A_loss: 0.3831, G_B_loss: 0.5963\n",
      "Epoch [16/30], Step [421/1067], D_A_loss: 0.0800, D_B_loss: 0.0503, G_A_loss: 0.8094, G_B_loss: 0.5273\n",
      "Epoch [16/30], Step [431/1067], D_A_loss: 0.0472, D_B_loss: 0.0647, G_A_loss: 0.8345, G_B_loss: 0.4455\n",
      "Epoch [16/30], Step [441/1067], D_A_loss: 0.1546, D_B_loss: 0.1452, G_A_loss: 0.3152, G_B_loss: 0.1143\n",
      "Epoch [16/30], Step [451/1067], D_A_loss: 0.1610, D_B_loss: 0.0589, G_A_loss: 0.6314, G_B_loss: 0.5568\n",
      "Epoch [16/30], Step [461/1067], D_A_loss: 0.0521, D_B_loss: 0.0454, G_A_loss: 0.9515, G_B_loss: 1.0290\n",
      "Epoch [16/30], Step [471/1067], D_A_loss: 0.0253, D_B_loss: 0.2501, G_A_loss: 1.3158, G_B_loss: 0.5123\n",
      "Epoch [16/30], Step [481/1067], D_A_loss: 0.0610, D_B_loss: 0.1252, G_A_loss: 0.3271, G_B_loss: 0.3030\n",
      "Epoch [16/30], Step [491/1067], D_A_loss: 0.0619, D_B_loss: 0.0725, G_A_loss: 0.3754, G_B_loss: 0.4225\n",
      "Epoch [16/30], Step [501/1067], D_A_loss: 0.1365, D_B_loss: 0.2358, G_A_loss: 0.7765, G_B_loss: 0.3819\n",
      "Epoch [16/30], Step [511/1067], D_A_loss: 0.1489, D_B_loss: 0.1148, G_A_loss: 0.4029, G_B_loss: 0.4204\n",
      "Epoch [16/30], Step [521/1067], D_A_loss: 0.0960, D_B_loss: 0.0664, G_A_loss: 0.6633, G_B_loss: 0.4371\n",
      "Epoch [16/30], Step [531/1067], D_A_loss: 0.1418, D_B_loss: 0.2859, G_A_loss: 0.5078, G_B_loss: 0.3093\n",
      "Epoch [16/30], Step [541/1067], D_A_loss: 0.1178, D_B_loss: 0.0701, G_A_loss: 0.5788, G_B_loss: 0.3282\n",
      "Epoch [16/30], Step [551/1067], D_A_loss: 0.0379, D_B_loss: 0.0743, G_A_loss: 0.7044, G_B_loss: 0.4823\n",
      "Epoch [16/30], Step [561/1067], D_A_loss: 0.0908, D_B_loss: 0.0814, G_A_loss: 0.6798, G_B_loss: 0.6603\n",
      "Epoch [16/30], Step [571/1067], D_A_loss: 0.0613, D_B_loss: 0.1554, G_A_loss: 0.3833, G_B_loss: 0.3929\n",
      "Epoch [16/30], Step [581/1067], D_A_loss: 0.0596, D_B_loss: 0.0929, G_A_loss: 1.0344, G_B_loss: 0.5958\n",
      "Epoch [16/30], Step [591/1067], D_A_loss: 0.1448, D_B_loss: 0.0529, G_A_loss: 0.7846, G_B_loss: 0.3900\n",
      "Epoch [16/30], Step [601/1067], D_A_loss: 0.0376, D_B_loss: 0.0824, G_A_loss: 0.8100, G_B_loss: 0.3813\n",
      "Epoch [16/30], Step [611/1067], D_A_loss: 0.0327, D_B_loss: 0.0467, G_A_loss: 1.1056, G_B_loss: 0.4020\n",
      "Epoch [16/30], Step [621/1067], D_A_loss: 0.0905, D_B_loss: 0.1051, G_A_loss: 0.2131, G_B_loss: 0.4113\n",
      "Epoch [16/30], Step [631/1067], D_A_loss: 0.1360, D_B_loss: 0.0706, G_A_loss: 0.4491, G_B_loss: 0.7744\n",
      "Epoch [16/30], Step [641/1067], D_A_loss: 0.0547, D_B_loss: 0.1453, G_A_loss: 1.2179, G_B_loss: 0.2306\n",
      "Epoch [16/30], Step [651/1067], D_A_loss: 0.0966, D_B_loss: 0.0678, G_A_loss: 0.5141, G_B_loss: 0.6617\n",
      "Epoch [16/30], Step [661/1067], D_A_loss: 0.1154, D_B_loss: 0.0325, G_A_loss: 0.7273, G_B_loss: 1.1004\n",
      "Epoch [16/30], Step [671/1067], D_A_loss: 0.0759, D_B_loss: 0.0973, G_A_loss: 0.6424, G_B_loss: 0.3580\n",
      "Epoch [16/30], Step [681/1067], D_A_loss: 0.1577, D_B_loss: 0.0336, G_A_loss: 0.6289, G_B_loss: 1.0383\n",
      "Epoch [16/30], Step [691/1067], D_A_loss: 0.1574, D_B_loss: 0.0701, G_A_loss: 0.6719, G_B_loss: 0.2952\n",
      "Epoch [16/30], Step [701/1067], D_A_loss: 0.3702, D_B_loss: 0.0358, G_A_loss: 0.8244, G_B_loss: 0.5043\n",
      "Epoch [16/30], Step [711/1067], D_A_loss: 0.1165, D_B_loss: 0.0630, G_A_loss: 0.4227, G_B_loss: 0.7346\n",
      "Epoch [16/30], Step [721/1067], D_A_loss: 0.0824, D_B_loss: 0.1013, G_A_loss: 0.7979, G_B_loss: 0.5133\n",
      "Epoch [16/30], Step [731/1067], D_A_loss: 0.1585, D_B_loss: 0.0755, G_A_loss: 0.4660, G_B_loss: 0.4177\n",
      "Epoch [16/30], Step [741/1067], D_A_loss: 0.0258, D_B_loss: 0.0371, G_A_loss: 0.7645, G_B_loss: 0.7547\n",
      "Epoch [16/30], Step [751/1067], D_A_loss: 0.1565, D_B_loss: 0.0276, G_A_loss: 0.4328, G_B_loss: 0.3759\n",
      "Epoch [16/30], Step [761/1067], D_A_loss: 0.0394, D_B_loss: 0.0955, G_A_loss: 0.3919, G_B_loss: 0.7339\n",
      "Epoch [16/30], Step [771/1067], D_A_loss: 0.0404, D_B_loss: 0.1352, G_A_loss: 0.7898, G_B_loss: 0.2662\n",
      "Epoch [16/30], Step [781/1067], D_A_loss: 0.2251, D_B_loss: 0.1072, G_A_loss: 1.0008, G_B_loss: 0.1830\n",
      "Epoch [16/30], Step [791/1067], D_A_loss: 0.1349, D_B_loss: 0.0303, G_A_loss: 0.2817, G_B_loss: 0.3969\n",
      "Epoch [16/30], Step [801/1067], D_A_loss: 0.2062, D_B_loss: 0.1735, G_A_loss: 0.2270, G_B_loss: 0.4295\n",
      "Epoch [16/30], Step [811/1067], D_A_loss: 0.1081, D_B_loss: 0.0972, G_A_loss: 0.3666, G_B_loss: 0.3788\n",
      "Epoch [16/30], Step [821/1067], D_A_loss: 0.0472, D_B_loss: 0.0496, G_A_loss: 0.7573, G_B_loss: 0.4669\n",
      "Epoch [16/30], Step [831/1067], D_A_loss: 0.0770, D_B_loss: 0.0859, G_A_loss: 0.4013, G_B_loss: 0.4253\n",
      "Epoch [16/30], Step [841/1067], D_A_loss: 0.0874, D_B_loss: 0.0420, G_A_loss: 0.8959, G_B_loss: 0.4960\n",
      "Epoch [16/30], Step [851/1067], D_A_loss: 0.1034, D_B_loss: 0.0893, G_A_loss: 0.2663, G_B_loss: 0.4103\n",
      "Epoch [16/30], Step [861/1067], D_A_loss: 0.0259, D_B_loss: 0.1181, G_A_loss: 0.9251, G_B_loss: 0.4504\n",
      "Epoch [16/30], Step [871/1067], D_A_loss: 0.1234, D_B_loss: 0.0455, G_A_loss: 0.6730, G_B_loss: 0.5168\n",
      "Epoch [16/30], Step [881/1067], D_A_loss: 0.0823, D_B_loss: 0.0279, G_A_loss: 0.1354, G_B_loss: 0.3876\n",
      "Epoch [16/30], Step [891/1067], D_A_loss: 0.1865, D_B_loss: 0.1124, G_A_loss: 0.5100, G_B_loss: 0.2030\n",
      "Epoch [16/30], Step [901/1067], D_A_loss: 0.0711, D_B_loss: 0.0347, G_A_loss: 0.5235, G_B_loss: 0.3987\n",
      "Epoch [16/30], Step [911/1067], D_A_loss: 0.1560, D_B_loss: 0.0215, G_A_loss: 0.8316, G_B_loss: 0.5490\n",
      "Epoch [16/30], Step [921/1067], D_A_loss: 0.1707, D_B_loss: 0.2350, G_A_loss: 0.2359, G_B_loss: 0.5708\n",
      "Epoch [16/30], Step [931/1067], D_A_loss: 0.1040, D_B_loss: 0.1723, G_A_loss: 0.4244, G_B_loss: 0.4175\n",
      "Epoch [16/30], Step [941/1067], D_A_loss: 0.2546, D_B_loss: 0.0276, G_A_loss: 0.1974, G_B_loss: 0.2828\n",
      "Epoch [16/30], Step [951/1067], D_A_loss: 0.1533, D_B_loss: 0.1036, G_A_loss: 0.5321, G_B_loss: 0.6865\n",
      "Epoch [16/30], Step [961/1067], D_A_loss: 0.0545, D_B_loss: 0.0476, G_A_loss: 0.7670, G_B_loss: 0.5472\n",
      "Epoch [16/30], Step [971/1067], D_A_loss: 0.1568, D_B_loss: 0.0504, G_A_loss: 0.7392, G_B_loss: 0.3248\n",
      "Epoch [16/30], Step [981/1067], D_A_loss: 0.1710, D_B_loss: 0.0625, G_A_loss: 0.6886, G_B_loss: 0.7116\n",
      "Epoch [16/30], Step [991/1067], D_A_loss: 0.1464, D_B_loss: 0.0502, G_A_loss: 0.8875, G_B_loss: 0.2728\n",
      "Epoch [16/30], Step [1001/1067], D_A_loss: 0.1310, D_B_loss: 0.0911, G_A_loss: 0.7448, G_B_loss: 0.5737\n",
      "Epoch [16/30], Step [1011/1067], D_A_loss: 0.1350, D_B_loss: 0.0428, G_A_loss: 0.8306, G_B_loss: 0.3987\n",
      "Epoch [16/30], Step [1021/1067], D_A_loss: 0.0644, D_B_loss: 0.0572, G_A_loss: 0.5842, G_B_loss: 0.7841\n",
      "Epoch [16/30], Step [1031/1067], D_A_loss: 0.1609, D_B_loss: 0.0431, G_A_loss: 0.8992, G_B_loss: 0.5132\n",
      "Epoch [16/30], Step [1041/1067], D_A_loss: 0.0908, D_B_loss: 0.0550, G_A_loss: 0.2914, G_B_loss: 0.8243\n",
      "Epoch [16/30], Step [1051/1067], D_A_loss: 0.0705, D_B_loss: 0.0390, G_A_loss: 0.9071, G_B_loss: 0.4734\n",
      "Epoch [16/30], Step [1061/1067], D_A_loss: 0.0304, D_B_loss: 0.1658, G_A_loss: 0.8767, G_B_loss: 0.6048\n",
      "Epoch [17/30], Step [1/1067], D_A_loss: 0.0984, D_B_loss: 0.2417, G_A_loss: 0.5848, G_B_loss: 0.6670\n",
      "Epoch [17/30], Step [11/1067], D_A_loss: 0.0583, D_B_loss: 0.2584, G_A_loss: 0.6881, G_B_loss: 0.4519\n",
      "Epoch [17/30], Step [21/1067], D_A_loss: 0.0685, D_B_loss: 0.1279, G_A_loss: 0.2981, G_B_loss: 0.5417\n",
      "Epoch [17/30], Step [31/1067], D_A_loss: 0.1325, D_B_loss: 0.0746, G_A_loss: 0.7924, G_B_loss: 0.4356\n",
      "Epoch [17/30], Step [41/1067], D_A_loss: 0.1014, D_B_loss: 0.0668, G_A_loss: 0.8635, G_B_loss: 0.5705\n",
      "Epoch [17/30], Step [51/1067], D_A_loss: 0.1215, D_B_loss: 0.2168, G_A_loss: 0.7297, G_B_loss: 0.3789\n",
      "Epoch [17/30], Step [61/1067], D_A_loss: 0.1963, D_B_loss: 0.0603, G_A_loss: 0.5692, G_B_loss: 0.3429\n",
      "Epoch [17/30], Step [71/1067], D_A_loss: 0.0813, D_B_loss: 0.2429, G_A_loss: 1.0795, G_B_loss: 0.3625\n",
      "Epoch [17/30], Step [81/1067], D_A_loss: 0.0295, D_B_loss: 0.0772, G_A_loss: 1.1697, G_B_loss: 0.6267\n",
      "Epoch [17/30], Step [91/1067], D_A_loss: 0.0449, D_B_loss: 0.1142, G_A_loss: 0.3773, G_B_loss: 0.4946\n",
      "Epoch [17/30], Step [101/1067], D_A_loss: 0.0478, D_B_loss: 0.1288, G_A_loss: 1.3793, G_B_loss: 0.5105\n",
      "Epoch [17/30], Step [111/1067], D_A_loss: 0.1527, D_B_loss: 0.1139, G_A_loss: 1.0445, G_B_loss: 0.4415\n",
      "Epoch [17/30], Step [121/1067], D_A_loss: 0.1292, D_B_loss: 0.1090, G_A_loss: 0.3387, G_B_loss: 0.5790\n",
      "Epoch [17/30], Step [131/1067], D_A_loss: 0.1826, D_B_loss: 0.0390, G_A_loss: 0.2544, G_B_loss: 0.7892\n",
      "Epoch [17/30], Step [141/1067], D_A_loss: 0.0853, D_B_loss: 0.0543, G_A_loss: 0.8609, G_B_loss: 0.4902\n",
      "Epoch [17/30], Step [151/1067], D_A_loss: 0.2507, D_B_loss: 0.0446, G_A_loss: 0.5446, G_B_loss: 1.2293\n",
      "Epoch [17/30], Step [161/1067], D_A_loss: 0.0426, D_B_loss: 0.0545, G_A_loss: 0.8915, G_B_loss: 0.5072\n",
      "Epoch [17/30], Step [171/1067], D_A_loss: 0.0699, D_B_loss: 0.0475, G_A_loss: 0.5539, G_B_loss: 0.7616\n",
      "Epoch [17/30], Step [181/1067], D_A_loss: 0.1397, D_B_loss: 0.0151, G_A_loss: 0.6799, G_B_loss: 0.3846\n",
      "Epoch [17/30], Step [191/1067], D_A_loss: 0.0819, D_B_loss: 0.2158, G_A_loss: 0.7976, G_B_loss: 0.3828\n",
      "Epoch [17/30], Step [201/1067], D_A_loss: 0.2273, D_B_loss: 0.0733, G_A_loss: 0.3106, G_B_loss: 0.1990\n",
      "Epoch [17/30], Step [211/1067], D_A_loss: 0.0789, D_B_loss: 0.0453, G_A_loss: 0.5964, G_B_loss: 0.3717\n",
      "Epoch [17/30], Step [221/1067], D_A_loss: 0.0380, D_B_loss: 0.0235, G_A_loss: 0.9863, G_B_loss: 0.6886\n",
      "Epoch [17/30], Step [231/1067], D_A_loss: 0.1391, D_B_loss: 0.0328, G_A_loss: 0.7001, G_B_loss: 0.2854\n",
      "Epoch [17/30], Step [241/1067], D_A_loss: 0.1381, D_B_loss: 0.0354, G_A_loss: 1.0772, G_B_loss: 0.3964\n",
      "Epoch [17/30], Step [251/1067], D_A_loss: 0.1070, D_B_loss: 0.1271, G_A_loss: 0.5555, G_B_loss: 0.2237\n",
      "Epoch [17/30], Step [261/1067], D_A_loss: 0.1810, D_B_loss: 0.0158, G_A_loss: 0.4094, G_B_loss: 0.3041\n",
      "Epoch [17/30], Step [271/1067], D_A_loss: 0.1128, D_B_loss: 0.0850, G_A_loss: 0.3279, G_B_loss: 0.4911\n",
      "Epoch [17/30], Step [281/1067], D_A_loss: 0.0813, D_B_loss: 0.0867, G_A_loss: 0.2686, G_B_loss: 0.6388\n",
      "Epoch [17/30], Step [291/1067], D_A_loss: 0.0864, D_B_loss: 0.0385, G_A_loss: 0.6827, G_B_loss: 0.8746\n",
      "Epoch [17/30], Step [301/1067], D_A_loss: 0.0537, D_B_loss: 0.0344, G_A_loss: 0.7187, G_B_loss: 0.5798\n",
      "Epoch [17/30], Step [311/1067], D_A_loss: 0.1150, D_B_loss: 0.0433, G_A_loss: 1.0806, G_B_loss: 0.4375\n",
      "Epoch [17/30], Step [321/1067], D_A_loss: 0.0851, D_B_loss: 0.0631, G_A_loss: 0.4218, G_B_loss: 0.4485\n",
      "Epoch [17/30], Step [331/1067], D_A_loss: 0.1017, D_B_loss: 0.0694, G_A_loss: 0.4782, G_B_loss: 0.4432\n",
      "Epoch [17/30], Step [341/1067], D_A_loss: 0.1622, D_B_loss: 0.2063, G_A_loss: 0.2598, G_B_loss: 0.0640\n",
      "Epoch [17/30], Step [351/1067], D_A_loss: 0.0620, D_B_loss: 0.0201, G_A_loss: 0.8405, G_B_loss: 0.6403\n",
      "Epoch [17/30], Step [361/1067], D_A_loss: 0.0799, D_B_loss: 0.0747, G_A_loss: 1.0775, G_B_loss: 0.3933\n",
      "Epoch [17/30], Step [371/1067], D_A_loss: 0.0524, D_B_loss: 0.0273, G_A_loss: 0.6915, G_B_loss: 0.7952\n",
      "Epoch [17/30], Step [381/1067], D_A_loss: 0.1228, D_B_loss: 0.0848, G_A_loss: 0.6946, G_B_loss: 0.3709\n",
      "Epoch [17/30], Step [391/1067], D_A_loss: 0.0708, D_B_loss: 0.1566, G_A_loss: 0.4277, G_B_loss: 0.5426\n",
      "Epoch [17/30], Step [401/1067], D_A_loss: 0.0283, D_B_loss: 0.0477, G_A_loss: 0.5807, G_B_loss: 0.5779\n",
      "Epoch [17/30], Step [411/1067], D_A_loss: 0.1181, D_B_loss: 0.0578, G_A_loss: 0.7825, G_B_loss: 0.4276\n",
      "Epoch [17/30], Step [421/1067], D_A_loss: 0.1615, D_B_loss: 0.0204, G_A_loss: 0.3863, G_B_loss: 0.3151\n",
      "Epoch [17/30], Step [431/1067], D_A_loss: 0.0580, D_B_loss: 0.1010, G_A_loss: 0.8497, G_B_loss: 0.5930\n",
      "Epoch [17/30], Step [441/1067], D_A_loss: 0.0148, D_B_loss: 0.0364, G_A_loss: 0.7420, G_B_loss: 0.6880\n",
      "Epoch [17/30], Step [451/1067], D_A_loss: 0.0968, D_B_loss: 0.2657, G_A_loss: 1.1261, G_B_loss: 0.5936\n",
      "Epoch [17/30], Step [461/1067], D_A_loss: 0.1020, D_B_loss: 0.0359, G_A_loss: 0.2840, G_B_loss: 1.1356\n",
      "Epoch [17/30], Step [471/1067], D_A_loss: 0.0719, D_B_loss: 0.0976, G_A_loss: 0.6030, G_B_loss: 0.5001\n",
      "Epoch [17/30], Step [481/1067], D_A_loss: 0.0271, D_B_loss: 0.1812, G_A_loss: 0.8013, G_B_loss: 0.6225\n",
      "Epoch [17/30], Step [491/1067], D_A_loss: 0.2326, D_B_loss: 0.0832, G_A_loss: 0.6902, G_B_loss: 1.1310\n",
      "Epoch [17/30], Step [501/1067], D_A_loss: 0.0559, D_B_loss: 0.1321, G_A_loss: 0.7373, G_B_loss: 0.5976\n",
      "Epoch [17/30], Step [511/1067], D_A_loss: 0.2951, D_B_loss: 0.2081, G_A_loss: 1.2829, G_B_loss: 0.6625\n",
      "Epoch [17/30], Step [521/1067], D_A_loss: 0.2963, D_B_loss: 0.0568, G_A_loss: 0.5788, G_B_loss: 0.2407\n",
      "Epoch [17/30], Step [531/1067], D_A_loss: 0.1138, D_B_loss: 0.1143, G_A_loss: 0.3512, G_B_loss: 0.5592\n",
      "Epoch [17/30], Step [541/1067], D_A_loss: 0.0535, D_B_loss: 0.0649, G_A_loss: 0.5824, G_B_loss: 0.7032\n",
      "Epoch [17/30], Step [551/1067], D_A_loss: 0.0555, D_B_loss: 0.0991, G_A_loss: 0.4475, G_B_loss: 0.3887\n",
      "Epoch [17/30], Step [561/1067], D_A_loss: 0.1207, D_B_loss: 0.1077, G_A_loss: 0.1352, G_B_loss: 0.3776\n",
      "Epoch [17/30], Step [571/1067], D_A_loss: 0.2511, D_B_loss: 0.0555, G_A_loss: 0.6432, G_B_loss: 0.1380\n",
      "Epoch [17/30], Step [581/1067], D_A_loss: 0.0226, D_B_loss: 0.0886, G_A_loss: 0.3907, G_B_loss: 0.6080\n",
      "Epoch [17/30], Step [591/1067], D_A_loss: 0.0356, D_B_loss: 0.1115, G_A_loss: 0.3704, G_B_loss: 0.7738\n",
      "Epoch [17/30], Step [601/1067], D_A_loss: 0.3269, D_B_loss: 0.1443, G_A_loss: 0.7560, G_B_loss: 0.1020\n",
      "Epoch [17/30], Step [611/1067], D_A_loss: 0.1710, D_B_loss: 0.0592, G_A_loss: 0.4033, G_B_loss: 0.4016\n",
      "Epoch [17/30], Step [621/1067], D_A_loss: 0.1423, D_B_loss: 0.0956, G_A_loss: 0.4055, G_B_loss: 0.3149\n",
      "Epoch [17/30], Step [631/1067], D_A_loss: 0.0920, D_B_loss: 0.2416, G_A_loss: 0.2141, G_B_loss: 0.4305\n",
      "Epoch [17/30], Step [641/1067], D_A_loss: 0.0979, D_B_loss: 0.0456, G_A_loss: 0.7511, G_B_loss: 0.4606\n",
      "Epoch [17/30], Step [651/1067], D_A_loss: 0.0752, D_B_loss: 0.1280, G_A_loss: 0.3203, G_B_loss: 0.8099\n",
      "Epoch [17/30], Step [661/1067], D_A_loss: 0.1412, D_B_loss: 0.2276, G_A_loss: 0.8010, G_B_loss: 0.4686\n",
      "Epoch [17/30], Step [671/1067], D_A_loss: 0.0361, D_B_loss: 0.2136, G_A_loss: 0.7414, G_B_loss: 0.6091\n",
      "Epoch [17/30], Step [681/1067], D_A_loss: 0.4754, D_B_loss: 0.0352, G_A_loss: 0.8352, G_B_loss: 0.0630\n",
      "Epoch [17/30], Step [691/1067], D_A_loss: 0.2024, D_B_loss: 0.2118, G_A_loss: 0.4164, G_B_loss: 0.2631\n",
      "Epoch [17/30], Step [701/1067], D_A_loss: 0.0968, D_B_loss: 0.0707, G_A_loss: 0.4910, G_B_loss: 0.8625\n",
      "Epoch [17/30], Step [711/1067], D_A_loss: 0.0317, D_B_loss: 0.0742, G_A_loss: 0.3759, G_B_loss: 0.6649\n",
      "Epoch [17/30], Step [721/1067], D_A_loss: 0.0882, D_B_loss: 0.0702, G_A_loss: 0.5647, G_B_loss: 0.4335\n",
      "Epoch [17/30], Step [731/1067], D_A_loss: 0.0551, D_B_loss: 0.1873, G_A_loss: 0.8913, G_B_loss: 0.5475\n",
      "Epoch [17/30], Step [741/1067], D_A_loss: 0.0751, D_B_loss: 0.0269, G_A_loss: 0.6864, G_B_loss: 0.6190\n",
      "Epoch [17/30], Step [751/1067], D_A_loss: 0.1159, D_B_loss: 0.0766, G_A_loss: 0.5003, G_B_loss: 0.4661\n",
      "Epoch [17/30], Step [761/1067], D_A_loss: 0.0457, D_B_loss: 0.0533, G_A_loss: 0.6214, G_B_loss: 0.8433\n",
      "Epoch [17/30], Step [771/1067], D_A_loss: 0.1085, D_B_loss: 0.0858, G_A_loss: 0.4150, G_B_loss: 0.9535\n",
      "Epoch [17/30], Step [781/1067], D_A_loss: 0.1124, D_B_loss: 0.1335, G_A_loss: 1.2535, G_B_loss: 0.9371\n",
      "Epoch [17/30], Step [791/1067], D_A_loss: 0.0334, D_B_loss: 0.0587, G_A_loss: 0.2715, G_B_loss: 0.6149\n",
      "Epoch [17/30], Step [801/1067], D_A_loss: 0.1319, D_B_loss: 0.1358, G_A_loss: 0.3981, G_B_loss: 0.3668\n",
      "Epoch [17/30], Step [811/1067], D_A_loss: 0.1766, D_B_loss: 0.0583, G_A_loss: 0.5444, G_B_loss: 0.4948\n",
      "Epoch [17/30], Step [821/1067], D_A_loss: 0.0997, D_B_loss: 0.1796, G_A_loss: 0.2284, G_B_loss: 0.7549\n",
      "Epoch [17/30], Step [831/1067], D_A_loss: 0.1372, D_B_loss: 0.1509, G_A_loss: 0.2579, G_B_loss: 0.6633\n",
      "Epoch [17/30], Step [841/1067], D_A_loss: 0.1341, D_B_loss: 0.1912, G_A_loss: 0.6250, G_B_loss: 0.2708\n",
      "Epoch [17/30], Step [851/1067], D_A_loss: 0.1702, D_B_loss: 0.0384, G_A_loss: 0.6469, G_B_loss: 0.3030\n",
      "Epoch [17/30], Step [861/1067], D_A_loss: 0.0724, D_B_loss: 0.0716, G_A_loss: 0.7179, G_B_loss: 0.5657\n",
      "Epoch [17/30], Step [871/1067], D_A_loss: 0.1157, D_B_loss: 0.0486, G_A_loss: 0.6632, G_B_loss: 0.6899\n",
      "Epoch [17/30], Step [881/1067], D_A_loss: 0.0332, D_B_loss: 0.0721, G_A_loss: 0.7982, G_B_loss: 0.3205\n",
      "Epoch [17/30], Step [891/1067], D_A_loss: 0.0761, D_B_loss: 0.0470, G_A_loss: 0.7928, G_B_loss: 0.3685\n",
      "Epoch [17/30], Step [901/1067], D_A_loss: 0.0668, D_B_loss: 0.0286, G_A_loss: 0.7184, G_B_loss: 0.5839\n",
      "Epoch [17/30], Step [911/1067], D_A_loss: 0.1008, D_B_loss: 0.0554, G_A_loss: 0.5845, G_B_loss: 0.3788\n",
      "Epoch [17/30], Step [921/1067], D_A_loss: 0.0564, D_B_loss: 0.1399, G_A_loss: 0.2799, G_B_loss: 0.6530\n",
      "Epoch [17/30], Step [931/1067], D_A_loss: 0.0939, D_B_loss: 0.0720, G_A_loss: 0.8114, G_B_loss: 0.6963\n",
      "Epoch [17/30], Step [941/1067], D_A_loss: 0.1820, D_B_loss: 0.1021, G_A_loss: 0.3058, G_B_loss: 0.3446\n",
      "Epoch [17/30], Step [951/1067], D_A_loss: 0.1556, D_B_loss: 0.0668, G_A_loss: 0.9922, G_B_loss: 0.7079\n",
      "Epoch [17/30], Step [961/1067], D_A_loss: 0.1324, D_B_loss: 0.0948, G_A_loss: 0.1944, G_B_loss: 0.4365\n",
      "Epoch [17/30], Step [971/1067], D_A_loss: 0.1031, D_B_loss: 0.0663, G_A_loss: 0.6808, G_B_loss: 0.1605\n",
      "Epoch [17/30], Step [981/1067], D_A_loss: 0.0232, D_B_loss: 0.0251, G_A_loss: 0.5344, G_B_loss: 0.4908\n",
      "Epoch [17/30], Step [991/1067], D_A_loss: 0.2239, D_B_loss: 0.1641, G_A_loss: 0.2305, G_B_loss: 0.3766\n",
      "Epoch [17/30], Step [1001/1067], D_A_loss: 0.0734, D_B_loss: 0.0285, G_A_loss: 0.8251, G_B_loss: 0.5080\n",
      "Epoch [17/30], Step [1011/1067], D_A_loss: 0.0812, D_B_loss: 0.0299, G_A_loss: 0.7921, G_B_loss: 0.2355\n",
      "Epoch [17/30], Step [1021/1067], D_A_loss: 0.0276, D_B_loss: 0.0544, G_A_loss: 0.3963, G_B_loss: 0.5284\n",
      "Epoch [17/30], Step [1031/1067], D_A_loss: 0.0593, D_B_loss: 0.0306, G_A_loss: 0.8036, G_B_loss: 0.8820\n",
      "Epoch [17/30], Step [1041/1067], D_A_loss: 0.0841, D_B_loss: 0.0667, G_A_loss: 0.8507, G_B_loss: 0.2298\n",
      "Epoch [17/30], Step [1051/1067], D_A_loss: 0.1009, D_B_loss: 0.0688, G_A_loss: 0.6488, G_B_loss: 0.8911\n",
      "Epoch [17/30], Step [1061/1067], D_A_loss: 0.0286, D_B_loss: 0.1158, G_A_loss: 0.3454, G_B_loss: 0.2618\n",
      "Epoch [18/30], Step [1/1067], D_A_loss: 0.1676, D_B_loss: 0.1018, G_A_loss: 0.6210, G_B_loss: 0.3718\n",
      "Epoch [18/30], Step [11/1067], D_A_loss: 0.1521, D_B_loss: 0.0711, G_A_loss: 0.7668, G_B_loss: 0.3640\n",
      "Epoch [18/30], Step [21/1067], D_A_loss: 0.0852, D_B_loss: 0.0540, G_A_loss: 0.5304, G_B_loss: 0.5345\n",
      "Epoch [18/30], Step [31/1067], D_A_loss: 0.1084, D_B_loss: 0.0823, G_A_loss: 0.6314, G_B_loss: 0.8887\n",
      "Epoch [18/30], Step [41/1067], D_A_loss: 0.1709, D_B_loss: 0.0308, G_A_loss: 0.4100, G_B_loss: 0.2810\n",
      "Epoch [18/30], Step [51/1067], D_A_loss: 0.0641, D_B_loss: 0.0319, G_A_loss: 0.6604, G_B_loss: 0.3982\n",
      "Epoch [18/30], Step [61/1067], D_A_loss: 0.0978, D_B_loss: 0.0450, G_A_loss: 0.8371, G_B_loss: 0.5538\n",
      "Epoch [18/30], Step [71/1067], D_A_loss: 0.1656, D_B_loss: 0.0641, G_A_loss: 0.1185, G_B_loss: 0.5726\n",
      "Epoch [18/30], Step [81/1067], D_A_loss: 0.0954, D_B_loss: 0.0455, G_A_loss: 0.7636, G_B_loss: 0.4644\n",
      "Epoch [18/30], Step [91/1067], D_A_loss: 0.1859, D_B_loss: 0.0419, G_A_loss: 0.2283, G_B_loss: 0.7790\n",
      "Epoch [18/30], Step [101/1067], D_A_loss: 0.1705, D_B_loss: 0.1182, G_A_loss: 0.3803, G_B_loss: 0.2355\n",
      "Epoch [18/30], Step [111/1067], D_A_loss: 0.0809, D_B_loss: 0.0499, G_A_loss: 1.1043, G_B_loss: 0.4460\n",
      "Epoch [18/30], Step [121/1067], D_A_loss: 0.2665, D_B_loss: 0.1437, G_A_loss: 0.8924, G_B_loss: 0.1226\n",
      "Epoch [18/30], Step [131/1067], D_A_loss: 0.0889, D_B_loss: 0.0612, G_A_loss: 0.8274, G_B_loss: 0.7769\n",
      "Epoch [18/30], Step [141/1067], D_A_loss: 0.1697, D_B_loss: 0.1259, G_A_loss: 0.3415, G_B_loss: 0.3791\n",
      "Epoch [18/30], Step [151/1067], D_A_loss: 0.1411, D_B_loss: 0.0294, G_A_loss: 0.4604, G_B_loss: 0.2640\n",
      "Epoch [18/30], Step [161/1067], D_A_loss: 0.0825, D_B_loss: 0.0206, G_A_loss: 0.4339, G_B_loss: 0.4973\n",
      "Epoch [18/30], Step [171/1067], D_A_loss: 0.1503, D_B_loss: 0.0234, G_A_loss: 0.6186, G_B_loss: 0.7025\n",
      "Epoch [18/30], Step [181/1067], D_A_loss: 0.0889, D_B_loss: 0.0933, G_A_loss: 0.5734, G_B_loss: 0.7121\n",
      "Epoch [18/30], Step [191/1067], D_A_loss: 0.1044, D_B_loss: 0.2763, G_A_loss: 0.1933, G_B_loss: 0.4468\n",
      "Epoch [18/30], Step [201/1067], D_A_loss: 0.1029, D_B_loss: 0.1907, G_A_loss: 0.7350, G_B_loss: 0.4721\n",
      "Epoch [18/30], Step [211/1067], D_A_loss: 0.2265, D_B_loss: 0.0294, G_A_loss: 0.5759, G_B_loss: 0.8447\n",
      "Epoch [18/30], Step [221/1067], D_A_loss: 0.0735, D_B_loss: 0.0704, G_A_loss: 0.4763, G_B_loss: 0.4232\n",
      "Epoch [18/30], Step [231/1067], D_A_loss: 0.0629, D_B_loss: 0.0228, G_A_loss: 0.6822, G_B_loss: 0.6559\n",
      "Epoch [18/30], Step [241/1067], D_A_loss: 0.0776, D_B_loss: 0.1908, G_A_loss: 0.6861, G_B_loss: 1.1583\n",
      "Epoch [18/30], Step [251/1067], D_A_loss: 0.0627, D_B_loss: 0.0602, G_A_loss: 0.7530, G_B_loss: 0.5509\n",
      "Epoch [18/30], Step [261/1067], D_A_loss: 0.0563, D_B_loss: 0.0950, G_A_loss: 0.7581, G_B_loss: 0.3006\n",
      "Epoch [18/30], Step [271/1067], D_A_loss: 0.1882, D_B_loss: 0.0534, G_A_loss: 0.6568, G_B_loss: 0.3982\n",
      "Epoch [18/30], Step [281/1067], D_A_loss: 0.0755, D_B_loss: 0.0799, G_A_loss: 0.2035, G_B_loss: 0.8784\n",
      "Epoch [18/30], Step [291/1067], D_A_loss: 0.0559, D_B_loss: 0.1134, G_A_loss: 1.0342, G_B_loss: 0.8476\n",
      "Epoch [18/30], Step [301/1067], D_A_loss: 0.0772, D_B_loss: 0.1440, G_A_loss: 0.3739, G_B_loss: 0.2102\n",
      "Epoch [18/30], Step [311/1067], D_A_loss: 0.0990, D_B_loss: 0.0255, G_A_loss: 0.5164, G_B_loss: 0.9819\n",
      "Epoch [18/30], Step [321/1067], D_A_loss: 0.4109, D_B_loss: 0.1107, G_A_loss: 0.3514, G_B_loss: 0.1072\n",
      "Epoch [18/30], Step [331/1067], D_A_loss: 0.4536, D_B_loss: 0.1558, G_A_loss: 1.0310, G_B_loss: 0.0488\n",
      "Epoch [18/30], Step [341/1067], D_A_loss: 0.1077, D_B_loss: 0.0719, G_A_loss: 0.1991, G_B_loss: 0.7776\n",
      "Epoch [18/30], Step [351/1067], D_A_loss: 0.0927, D_B_loss: 0.0504, G_A_loss: 0.4948, G_B_loss: 0.4309\n",
      "Epoch [18/30], Step [361/1067], D_A_loss: 0.2197, D_B_loss: 0.1301, G_A_loss: 0.9704, G_B_loss: 0.1915\n",
      "Epoch [18/30], Step [371/1067], D_A_loss: 0.1595, D_B_loss: 0.1717, G_A_loss: 0.3475, G_B_loss: 0.4821\n",
      "Epoch [18/30], Step [381/1067], D_A_loss: 0.0908, D_B_loss: 0.0822, G_A_loss: 0.9212, G_B_loss: 1.0746\n",
      "Epoch [18/30], Step [391/1067], D_A_loss: 0.1650, D_B_loss: 0.1268, G_A_loss: 0.3233, G_B_loss: 0.4944\n",
      "Epoch [18/30], Step [401/1067], D_A_loss: 0.1460, D_B_loss: 0.1050, G_A_loss: 0.4615, G_B_loss: 0.4934\n",
      "Epoch [18/30], Step [411/1067], D_A_loss: 0.1782, D_B_loss: 0.0934, G_A_loss: 0.3156, G_B_loss: 0.2762\n",
      "Epoch [18/30], Step [421/1067], D_A_loss: 0.2512, D_B_loss: 0.1002, G_A_loss: 0.3793, G_B_loss: 0.1640\n",
      "Epoch [18/30], Step [431/1067], D_A_loss: 0.0629, D_B_loss: 0.0568, G_A_loss: 0.7900, G_B_loss: 0.7814\n",
      "Epoch [18/30], Step [441/1067], D_A_loss: 0.1225, D_B_loss: 0.0758, G_A_loss: 1.0097, G_B_loss: 0.3194\n",
      "Epoch [18/30], Step [451/1067], D_A_loss: 0.1777, D_B_loss: 0.0625, G_A_loss: 0.6388, G_B_loss: 0.7394\n",
      "Epoch [18/30], Step [461/1067], D_A_loss: 0.0372, D_B_loss: 0.1728, G_A_loss: 0.9734, G_B_loss: 0.9430\n",
      "Epoch [18/30], Step [471/1067], D_A_loss: 0.0443, D_B_loss: 0.0178, G_A_loss: 0.7931, G_B_loss: 1.0903\n",
      "Epoch [18/30], Step [481/1067], D_A_loss: 0.0412, D_B_loss: 0.2599, G_A_loss: 0.9702, G_B_loss: 0.9018\n",
      "Epoch [18/30], Step [491/1067], D_A_loss: 0.0445, D_B_loss: 0.0429, G_A_loss: 0.8771, G_B_loss: 0.5683\n",
      "Epoch [18/30], Step [501/1067], D_A_loss: 0.0244, D_B_loss: 0.0197, G_A_loss: 0.9315, G_B_loss: 0.9349\n",
      "Epoch [18/30], Step [511/1067], D_A_loss: 0.1309, D_B_loss: 0.4397, G_A_loss: 0.0364, G_B_loss: 0.3061\n",
      "Epoch [18/30], Step [521/1067], D_A_loss: 0.0898, D_B_loss: 0.2002, G_A_loss: 0.1728, G_B_loss: 0.4144\n",
      "Epoch [18/30], Step [531/1067], D_A_loss: 0.0299, D_B_loss: 0.0412, G_A_loss: 0.6307, G_B_loss: 1.0402\n",
      "Epoch [18/30], Step [541/1067], D_A_loss: 0.1713, D_B_loss: 0.0792, G_A_loss: 0.3982, G_B_loss: 0.2390\n",
      "Epoch [18/30], Step [551/1067], D_A_loss: 0.1646, D_B_loss: 0.1376, G_A_loss: 0.4953, G_B_loss: 0.2917\n",
      "Epoch [18/30], Step [561/1067], D_A_loss: 0.1119, D_B_loss: 0.0568, G_A_loss: 0.6158, G_B_loss: 0.3924\n",
      "Epoch [18/30], Step [571/1067], D_A_loss: 0.1499, D_B_loss: 0.1460, G_A_loss: 0.2704, G_B_loss: 1.3043\n",
      "Epoch [18/30], Step [581/1067], D_A_loss: 0.1686, D_B_loss: 0.0676, G_A_loss: 0.4636, G_B_loss: 0.2505\n",
      "Epoch [18/30], Step [591/1067], D_A_loss: 0.2100, D_B_loss: 0.0750, G_A_loss: 0.4907, G_B_loss: 0.2967\n",
      "Epoch [18/30], Step [601/1067], D_A_loss: 0.2126, D_B_loss: 0.0263, G_A_loss: 0.7263, G_B_loss: 0.2072\n",
      "Epoch [18/30], Step [611/1067], D_A_loss: 0.1818, D_B_loss: 0.0238, G_A_loss: 0.8796, G_B_loss: 1.6161\n",
      "Epoch [18/30], Step [621/1067], D_A_loss: 0.1702, D_B_loss: 0.0571, G_A_loss: 0.4910, G_B_loss: 0.4286\n",
      "Epoch [18/30], Step [631/1067], D_A_loss: 0.1379, D_B_loss: 0.0595, G_A_loss: 0.2142, G_B_loss: 0.6976\n",
      "Epoch [18/30], Step [641/1067], D_A_loss: 0.1002, D_B_loss: 0.0633, G_A_loss: 1.0832, G_B_loss: 0.5169\n",
      "Epoch [18/30], Step [651/1067], D_A_loss: 0.1167, D_B_loss: 0.0778, G_A_loss: 0.5377, G_B_loss: 0.7084\n",
      "Epoch [18/30], Step [661/1067], D_A_loss: 0.0747, D_B_loss: 0.0508, G_A_loss: 0.7134, G_B_loss: 0.3338\n",
      "Epoch [18/30], Step [671/1067], D_A_loss: 0.1474, D_B_loss: 0.0281, G_A_loss: 0.4879, G_B_loss: 0.5850\n",
      "Epoch [18/30], Step [681/1067], D_A_loss: 0.1659, D_B_loss: 0.0522, G_A_loss: 0.5788, G_B_loss: 0.6877\n",
      "Epoch [18/30], Step [691/1067], D_A_loss: 0.0879, D_B_loss: 0.2816, G_A_loss: 0.6789, G_B_loss: 0.7358\n",
      "Epoch [18/30], Step [701/1067], D_A_loss: 0.1109, D_B_loss: 0.2165, G_A_loss: 1.5777, G_B_loss: 0.4303\n",
      "Epoch [18/30], Step [711/1067], D_A_loss: 0.1593, D_B_loss: 0.0642, G_A_loss: 0.5540, G_B_loss: 0.6216\n",
      "Epoch [18/30], Step [721/1067], D_A_loss: 0.1268, D_B_loss: 0.0708, G_A_loss: 0.2982, G_B_loss: 0.5089\n",
      "Epoch [18/30], Step [731/1067], D_A_loss: 0.0845, D_B_loss: 0.1955, G_A_loss: 0.7148, G_B_loss: 0.8393\n",
      "Epoch [18/30], Step [741/1067], D_A_loss: 0.0418, D_B_loss: 0.0421, G_A_loss: 0.6121, G_B_loss: 0.4586\n",
      "Epoch [18/30], Step [751/1067], D_A_loss: 0.1259, D_B_loss: 0.0701, G_A_loss: 0.4586, G_B_loss: 0.9373\n",
      "Epoch [18/30], Step [761/1067], D_A_loss: 0.0973, D_B_loss: 0.0402, G_A_loss: 0.7335, G_B_loss: 0.5337\n",
      "Epoch [18/30], Step [771/1067], D_A_loss: 0.1362, D_B_loss: 0.0350, G_A_loss: 0.3582, G_B_loss: 0.1555\n",
      "Epoch [18/30], Step [781/1067], D_A_loss: 0.1012, D_B_loss: 0.0716, G_A_loss: 0.7302, G_B_loss: 0.6119\n",
      "Epoch [18/30], Step [791/1067], D_A_loss: 0.3760, D_B_loss: 0.0291, G_A_loss: 0.6635, G_B_loss: 0.0667\n",
      "Epoch [18/30], Step [801/1067], D_A_loss: 0.1276, D_B_loss: 0.0329, G_A_loss: 0.8083, G_B_loss: 0.4964\n",
      "Epoch [18/30], Step [811/1067], D_A_loss: 0.0819, D_B_loss: 0.0411, G_A_loss: 0.8313, G_B_loss: 0.4427\n",
      "Epoch [18/30], Step [821/1067], D_A_loss: 0.1485, D_B_loss: 0.1254, G_A_loss: 0.4206, G_B_loss: 0.8214\n",
      "Epoch [18/30], Step [831/1067], D_A_loss: 0.1097, D_B_loss: 0.0793, G_A_loss: 0.9000, G_B_loss: 0.2073\n",
      "Epoch [18/30], Step [841/1067], D_A_loss: 0.1001, D_B_loss: 0.0603, G_A_loss: 0.6786, G_B_loss: 1.2131\n",
      "Epoch [18/30], Step [851/1067], D_A_loss: 0.0735, D_B_loss: 0.1105, G_A_loss: 0.4086, G_B_loss: 0.4670\n",
      "Epoch [18/30], Step [861/1067], D_A_loss: 0.0624, D_B_loss: 0.0500, G_A_loss: 0.7256, G_B_loss: 0.7715\n",
      "Epoch [18/30], Step [871/1067], D_A_loss: 0.0946, D_B_loss: 0.1318, G_A_loss: 0.1957, G_B_loss: 0.2768\n",
      "Epoch [18/30], Step [881/1067], D_A_loss: 0.0397, D_B_loss: 0.1192, G_A_loss: 0.4897, G_B_loss: 0.4196\n",
      "Epoch [18/30], Step [891/1067], D_A_loss: 0.1285, D_B_loss: 0.0703, G_A_loss: 0.6433, G_B_loss: 0.5151\n",
      "Epoch [18/30], Step [901/1067], D_A_loss: 0.2805, D_B_loss: 0.0387, G_A_loss: 1.0682, G_B_loss: 0.4053\n",
      "Epoch [18/30], Step [911/1067], D_A_loss: 0.1382, D_B_loss: 0.0329, G_A_loss: 0.7676, G_B_loss: 1.1017\n",
      "Epoch [18/30], Step [921/1067], D_A_loss: 0.0738, D_B_loss: 0.0643, G_A_loss: 0.5299, G_B_loss: 0.5458\n",
      "Epoch [18/30], Step [931/1067], D_A_loss: 0.1683, D_B_loss: 0.1268, G_A_loss: 0.7803, G_B_loss: 0.0999\n",
      "Epoch [18/30], Step [941/1067], D_A_loss: 0.1153, D_B_loss: 0.0859, G_A_loss: 0.4870, G_B_loss: 0.5655\n",
      "Epoch [18/30], Step [951/1067], D_A_loss: 0.2864, D_B_loss: 0.0558, G_A_loss: 0.7040, G_B_loss: 0.1259\n",
      "Epoch [18/30], Step [961/1067], D_A_loss: 0.0893, D_B_loss: 0.1257, G_A_loss: 0.3600, G_B_loss: 0.3835\n",
      "Epoch [18/30], Step [971/1067], D_A_loss: 0.1073, D_B_loss: 0.1449, G_A_loss: 0.3382, G_B_loss: 0.4407\n",
      "Epoch [18/30], Step [981/1067], D_A_loss: 0.2249, D_B_loss: 0.1196, G_A_loss: 0.3785, G_B_loss: 0.4801\n",
      "Epoch [18/30], Step [991/1067], D_A_loss: 0.1873, D_B_loss: 0.0574, G_A_loss: 0.7221, G_B_loss: 0.3930\n",
      "Epoch [18/30], Step [1001/1067], D_A_loss: 0.1028, D_B_loss: 0.0843, G_A_loss: 0.4202, G_B_loss: 0.3802\n",
      "Epoch [18/30], Step [1011/1067], D_A_loss: 0.0508, D_B_loss: 0.0490, G_A_loss: 0.2679, G_B_loss: 0.4311\n",
      "Epoch [18/30], Step [1021/1067], D_A_loss: 0.1261, D_B_loss: 0.1251, G_A_loss: 0.1208, G_B_loss: 0.3589\n",
      "Epoch [18/30], Step [1031/1067], D_A_loss: 0.1013, D_B_loss: 0.0779, G_A_loss: 0.4751, G_B_loss: 0.3992\n",
      "Epoch [18/30], Step [1041/1067], D_A_loss: 0.2623, D_B_loss: 0.0259, G_A_loss: 0.4564, G_B_loss: 0.7380\n",
      "Epoch [18/30], Step [1051/1067], D_A_loss: 0.0800, D_B_loss: 0.0593, G_A_loss: 0.7244, G_B_loss: 0.2352\n",
      "Epoch [18/30], Step [1061/1067], D_A_loss: 0.0880, D_B_loss: 0.0671, G_A_loss: 0.7683, G_B_loss: 0.7502\n",
      "Epoch [19/30], Step [1/1067], D_A_loss: 0.1363, D_B_loss: 0.1253, G_A_loss: 0.7115, G_B_loss: 0.5113\n",
      "Epoch [19/30], Step [11/1067], D_A_loss: 0.0482, D_B_loss: 0.0462, G_A_loss: 0.6788, G_B_loss: 0.6036\n",
      "Epoch [19/30], Step [21/1067], D_A_loss: 0.2004, D_B_loss: 0.0969, G_A_loss: 0.6950, G_B_loss: 0.8910\n",
      "Epoch [19/30], Step [31/1067], D_A_loss: 0.2132, D_B_loss: 0.0711, G_A_loss: 0.1505, G_B_loss: 0.2880\n",
      "Epoch [19/30], Step [41/1067], D_A_loss: 0.0881, D_B_loss: 0.0404, G_A_loss: 0.7794, G_B_loss: 0.3318\n",
      "Epoch [19/30], Step [51/1067], D_A_loss: 0.1878, D_B_loss: 0.0497, G_A_loss: 0.2447, G_B_loss: 0.2436\n",
      "Epoch [19/30], Step [61/1067], D_A_loss: 0.0673, D_B_loss: 0.0453, G_A_loss: 0.7064, G_B_loss: 0.9567\n",
      "Epoch [19/30], Step [71/1067], D_A_loss: 0.2881, D_B_loss: 0.1047, G_A_loss: 0.4029, G_B_loss: 1.4478\n",
      "Epoch [19/30], Step [81/1067], D_A_loss: 0.0849, D_B_loss: 0.0702, G_A_loss: 0.4645, G_B_loss: 0.5693\n",
      "Epoch [19/30], Step [91/1067], D_A_loss: 0.0407, D_B_loss: 0.1062, G_A_loss: 0.4525, G_B_loss: 0.5877\n",
      "Epoch [19/30], Step [101/1067], D_A_loss: 0.0735, D_B_loss: 0.0519, G_A_loss: 1.0353, G_B_loss: 0.2519\n",
      "Epoch [19/30], Step [111/1067], D_A_loss: 0.2417, D_B_loss: 0.1724, G_A_loss: 0.3115, G_B_loss: 0.5350\n",
      "Epoch [19/30], Step [121/1067], D_A_loss: 0.0823, D_B_loss: 0.0501, G_A_loss: 0.5679, G_B_loss: 0.6659\n",
      "Epoch [19/30], Step [131/1067], D_A_loss: 0.0532, D_B_loss: 0.0400, G_A_loss: 0.7412, G_B_loss: 0.6554\n",
      "Epoch [19/30], Step [141/1067], D_A_loss: 0.1400, D_B_loss: 0.0749, G_A_loss: 0.7017, G_B_loss: 0.7618\n",
      "Epoch [19/30], Step [151/1067], D_A_loss: 0.0355, D_B_loss: 0.0337, G_A_loss: 0.6281, G_B_loss: 0.6101\n",
      "Epoch [19/30], Step [161/1067], D_A_loss: 0.2132, D_B_loss: 0.0311, G_A_loss: 0.6618, G_B_loss: 0.6084\n",
      "Epoch [19/30], Step [171/1067], D_A_loss: 0.0577, D_B_loss: 0.0833, G_A_loss: 0.9267, G_B_loss: 0.5261\n",
      "Epoch [19/30], Step [181/1067], D_A_loss: 0.0508, D_B_loss: 0.0459, G_A_loss: 0.9685, G_B_loss: 0.6186\n",
      "Epoch [19/30], Step [191/1067], D_A_loss: 0.0569, D_B_loss: 0.0266, G_A_loss: 0.3821, G_B_loss: 0.7425\n",
      "Epoch [19/30], Step [201/1067], D_A_loss: 0.0406, D_B_loss: 0.0374, G_A_loss: 0.4138, G_B_loss: 0.3968\n",
      "Epoch [19/30], Step [211/1067], D_A_loss: 0.2601, D_B_loss: 0.0433, G_A_loss: 0.6274, G_B_loss: 0.9936\n",
      "Epoch [19/30], Step [221/1067], D_A_loss: 0.0680, D_B_loss: 0.1498, G_A_loss: 0.6955, G_B_loss: 0.6544\n",
      "Epoch [19/30], Step [231/1067], D_A_loss: 0.1033, D_B_loss: 0.1579, G_A_loss: 0.6278, G_B_loss: 0.5320\n",
      "Epoch [19/30], Step [241/1067], D_A_loss: 0.0528, D_B_loss: 0.1010, G_A_loss: 0.4011, G_B_loss: 0.7294\n",
      "Epoch [19/30], Step [251/1067], D_A_loss: 0.1289, D_B_loss: 0.0957, G_A_loss: 0.4231, G_B_loss: 0.3525\n",
      "Epoch [19/30], Step [261/1067], D_A_loss: 0.1719, D_B_loss: 0.0645, G_A_loss: 0.5915, G_B_loss: 0.3184\n",
      "Epoch [19/30], Step [271/1067], D_A_loss: 0.1858, D_B_loss: 0.0502, G_A_loss: 0.3579, G_B_loss: 0.4228\n",
      "Epoch [19/30], Step [281/1067], D_A_loss: 0.0900, D_B_loss: 0.0926, G_A_loss: 0.7658, G_B_loss: 0.5444\n",
      "Epoch [19/30], Step [291/1067], D_A_loss: 0.0569, D_B_loss: 0.0877, G_A_loss: 0.3726, G_B_loss: 0.5083\n",
      "Epoch [19/30], Step [301/1067], D_A_loss: 0.1811, D_B_loss: 0.0391, G_A_loss: 0.5587, G_B_loss: 0.6371\n",
      "Epoch [19/30], Step [311/1067], D_A_loss: 0.0488, D_B_loss: 0.1162, G_A_loss: 0.9083, G_B_loss: 0.5972\n",
      "Epoch [19/30], Step [321/1067], D_A_loss: 0.1909, D_B_loss: 0.0993, G_A_loss: 0.5179, G_B_loss: 0.8482\n",
      "Epoch [19/30], Step [331/1067], D_A_loss: 0.1876, D_B_loss: 0.0483, G_A_loss: 0.5547, G_B_loss: 0.2455\n",
      "Epoch [19/30], Step [341/1067], D_A_loss: 0.1919, D_B_loss: 0.0311, G_A_loss: 1.1085, G_B_loss: 0.1745\n",
      "Epoch [19/30], Step [351/1067], D_A_loss: 0.1233, D_B_loss: 0.0401, G_A_loss: 0.3807, G_B_loss: 0.5028\n",
      "Epoch [19/30], Step [361/1067], D_A_loss: 0.0860, D_B_loss: 0.1156, G_A_loss: 0.5319, G_B_loss: 0.2970\n",
      "Epoch [19/30], Step [371/1067], D_A_loss: 0.1534, D_B_loss: 0.1143, G_A_loss: 0.5056, G_B_loss: 0.7038\n",
      "Epoch [19/30], Step [381/1067], D_A_loss: 0.1577, D_B_loss: 0.0280, G_A_loss: 0.7208, G_B_loss: 0.2559\n",
      "Epoch [19/30], Step [391/1067], D_A_loss: 0.2158, D_B_loss: 0.0887, G_A_loss: 0.7630, G_B_loss: 0.1651\n",
      "Epoch [19/30], Step [401/1067], D_A_loss: 0.2680, D_B_loss: 0.0551, G_A_loss: 0.6940, G_B_loss: 1.0546\n",
      "Epoch [19/30], Step [411/1067], D_A_loss: 0.1762, D_B_loss: 0.1272, G_A_loss: 0.4551, G_B_loss: 0.2333\n",
      "Epoch [19/30], Step [421/1067], D_A_loss: 0.0282, D_B_loss: 0.2177, G_A_loss: 0.2493, G_B_loss: 0.7802\n",
      "Epoch [19/30], Step [431/1067], D_A_loss: 0.1534, D_B_loss: 0.0536, G_A_loss: 0.5246, G_B_loss: 0.2603\n",
      "Epoch [19/30], Step [441/1067], D_A_loss: 0.0815, D_B_loss: 0.0361, G_A_loss: 0.6443, G_B_loss: 0.4943\n",
      "Epoch [19/30], Step [451/1067], D_A_loss: 0.0617, D_B_loss: 0.1900, G_A_loss: 1.5555, G_B_loss: 0.4241\n",
      "Epoch [19/30], Step [461/1067], D_A_loss: 0.2718, D_B_loss: 0.0630, G_A_loss: 0.7783, G_B_loss: 0.2524\n",
      "Epoch [19/30], Step [471/1067], D_A_loss: 0.1077, D_B_loss: 0.1382, G_A_loss: 0.8975, G_B_loss: 0.4354\n",
      "Epoch [19/30], Step [481/1067], D_A_loss: 0.1032, D_B_loss: 0.1422, G_A_loss: 1.0137, G_B_loss: 0.4438\n",
      "Epoch [19/30], Step [491/1067], D_A_loss: 0.0723, D_B_loss: 0.3040, G_A_loss: 1.2718, G_B_loss: 0.6129\n",
      "Epoch [19/30], Step [501/1067], D_A_loss: 0.1347, D_B_loss: 0.1389, G_A_loss: 0.7140, G_B_loss: 0.6374\n",
      "Epoch [19/30], Step [511/1067], D_A_loss: 0.0365, D_B_loss: 0.1003, G_A_loss: 0.5710, G_B_loss: 0.2495\n",
      "Epoch [19/30], Step [521/1067], D_A_loss: 0.1208, D_B_loss: 0.0725, G_A_loss: 0.6084, G_B_loss: 0.4204\n",
      "Epoch [19/30], Step [531/1067], D_A_loss: 0.2936, D_B_loss: 0.0891, G_A_loss: 0.7977, G_B_loss: 0.4527\n",
      "Epoch [19/30], Step [541/1067], D_A_loss: 0.0972, D_B_loss: 0.0972, G_A_loss: 0.4862, G_B_loss: 0.4696\n",
      "Epoch [19/30], Step [551/1067], D_A_loss: 0.0979, D_B_loss: 0.0339, G_A_loss: 0.7089, G_B_loss: 0.8611\n",
      "Epoch [19/30], Step [561/1067], D_A_loss: 0.1080, D_B_loss: 0.0509, G_A_loss: 1.3329, G_B_loss: 0.3830\n",
      "Epoch [19/30], Step [571/1067], D_A_loss: 0.2752, D_B_loss: 0.1870, G_A_loss: 0.3883, G_B_loss: 0.1485\n",
      "Epoch [19/30], Step [581/1067], D_A_loss: 0.0803, D_B_loss: 0.0190, G_A_loss: 0.4218, G_B_loss: 0.5494\n",
      "Epoch [19/30], Step [591/1067], D_A_loss: 0.0367, D_B_loss: 0.0671, G_A_loss: 0.5147, G_B_loss: 0.1451\n",
      "Epoch [19/30], Step [601/1067], D_A_loss: 0.0576, D_B_loss: 0.0579, G_A_loss: 0.6395, G_B_loss: 0.5651\n",
      "Epoch [19/30], Step [611/1067], D_A_loss: 0.0945, D_B_loss: 0.0234, G_A_loss: 0.1331, G_B_loss: 0.2845\n",
      "Epoch [19/30], Step [621/1067], D_A_loss: 0.0860, D_B_loss: 0.0213, G_A_loss: 1.0020, G_B_loss: 0.6674\n",
      "Epoch [19/30], Step [631/1067], D_A_loss: 0.0984, D_B_loss: 0.0478, G_A_loss: 0.6218, G_B_loss: 0.4294\n",
      "Epoch [19/30], Step [641/1067], D_A_loss: 0.0855, D_B_loss: 0.0201, G_A_loss: 0.6927, G_B_loss: 0.4566\n",
      "Epoch [19/30], Step [651/1067], D_A_loss: 0.0289, D_B_loss: 0.0943, G_A_loss: 0.4966, G_B_loss: 0.5795\n",
      "Epoch [19/30], Step [661/1067], D_A_loss: 0.1045, D_B_loss: 0.2213, G_A_loss: 0.5039, G_B_loss: 0.3526\n",
      "Epoch [19/30], Step [671/1067], D_A_loss: 0.0587, D_B_loss: 0.0158, G_A_loss: 0.4377, G_B_loss: 0.7798\n",
      "Epoch [19/30], Step [681/1067], D_A_loss: 0.0451, D_B_loss: 0.1223, G_A_loss: 0.3128, G_B_loss: 0.6415\n",
      "Epoch [19/30], Step [691/1067], D_A_loss: 0.0611, D_B_loss: 0.2552, G_A_loss: 0.6100, G_B_loss: 0.1171\n",
      "Epoch [19/30], Step [701/1067], D_A_loss: 0.0332, D_B_loss: 0.1946, G_A_loss: 0.9241, G_B_loss: 0.4361\n",
      "Epoch [19/30], Step [711/1067], D_A_loss: 0.1590, D_B_loss: 0.1483, G_A_loss: 0.2965, G_B_loss: 0.2855\n",
      "Epoch [19/30], Step [721/1067], D_A_loss: 0.2379, D_B_loss: 0.1802, G_A_loss: 0.3201, G_B_loss: 0.2369\n",
      "Epoch [19/30], Step [731/1067], D_A_loss: 0.1249, D_B_loss: 0.0568, G_A_loss: 0.5146, G_B_loss: 0.4005\n",
      "Epoch [19/30], Step [741/1067], D_A_loss: 0.1304, D_B_loss: 0.2530, G_A_loss: 1.3880, G_B_loss: 0.3343\n",
      "Epoch [19/30], Step [751/1067], D_A_loss: 0.1029, D_B_loss: 0.0332, G_A_loss: 0.7765, G_B_loss: 0.4152\n",
      "Epoch [19/30], Step [761/1067], D_A_loss: 0.0602, D_B_loss: 0.2594, G_A_loss: 0.8035, G_B_loss: 0.2376\n",
      "Epoch [19/30], Step [771/1067], D_A_loss: 0.1159, D_B_loss: 0.0407, G_A_loss: 0.6531, G_B_loss: 0.5192\n",
      "Epoch [19/30], Step [781/1067], D_A_loss: 0.3091, D_B_loss: 0.0724, G_A_loss: 0.6940, G_B_loss: 1.0520\n",
      "Epoch [19/30], Step [791/1067], D_A_loss: 0.1911, D_B_loss: 0.0524, G_A_loss: 0.5368, G_B_loss: 1.2744\n",
      "Epoch [19/30], Step [801/1067], D_A_loss: 0.0363, D_B_loss: 0.0137, G_A_loss: 0.6240, G_B_loss: 0.5494\n",
      "Epoch [19/30], Step [811/1067], D_A_loss: 0.1450, D_B_loss: 0.1267, G_A_loss: 0.7173, G_B_loss: 0.5328\n",
      "Epoch [19/30], Step [821/1067], D_A_loss: 0.0560, D_B_loss: 0.0541, G_A_loss: 0.8126, G_B_loss: 0.5624\n",
      "Epoch [19/30], Step [831/1067], D_A_loss: 0.0550, D_B_loss: 0.1114, G_A_loss: 0.3518, G_B_loss: 0.4461\n",
      "Epoch [19/30], Step [841/1067], D_A_loss: 0.0640, D_B_loss: 0.1224, G_A_loss: 0.7463, G_B_loss: 0.6373\n",
      "Epoch [19/30], Step [851/1067], D_A_loss: 0.2601, D_B_loss: 0.0489, G_A_loss: 0.6444, G_B_loss: 0.4928\n",
      "Epoch [19/30], Step [861/1067], D_A_loss: 0.0670, D_B_loss: 0.0237, G_A_loss: 0.3370, G_B_loss: 0.5661\n",
      "Epoch [19/30], Step [871/1067], D_A_loss: 0.3141, D_B_loss: 0.1477, G_A_loss: 0.6840, G_B_loss: 0.2031\n",
      "Epoch [19/30], Step [881/1067], D_A_loss: 0.2289, D_B_loss: 0.0625, G_A_loss: 1.3349, G_B_loss: 0.8219\n",
      "Epoch [19/30], Step [891/1067], D_A_loss: 0.0440, D_B_loss: 0.0802, G_A_loss: 0.5466, G_B_loss: 0.4788\n",
      "Epoch [19/30], Step [901/1067], D_A_loss: 0.0358, D_B_loss: 0.0890, G_A_loss: 0.5894, G_B_loss: 0.3092\n",
      "Epoch [19/30], Step [911/1067], D_A_loss: 0.0706, D_B_loss: 0.0522, G_A_loss: 0.4662, G_B_loss: 0.1843\n",
      "Epoch [19/30], Step [921/1067], D_A_loss: 0.0383, D_B_loss: 0.1495, G_A_loss: 0.2767, G_B_loss: 0.7867\n",
      "Epoch [19/30], Step [931/1067], D_A_loss: 0.1275, D_B_loss: 0.0404, G_A_loss: 0.5914, G_B_loss: 0.5310\n",
      "Epoch [19/30], Step [941/1067], D_A_loss: 0.0480, D_B_loss: 0.0492, G_A_loss: 0.3355, G_B_loss: 0.1985\n",
      "Epoch [19/30], Step [951/1067], D_A_loss: 0.2390, D_B_loss: 0.0214, G_A_loss: 0.9172, G_B_loss: 0.9981\n",
      "Epoch [19/30], Step [961/1067], D_A_loss: 0.0450, D_B_loss: 0.0294, G_A_loss: 0.3994, G_B_loss: 0.4806\n",
      "Epoch [19/30], Step [971/1067], D_A_loss: 0.2107, D_B_loss: 0.0305, G_A_loss: 0.6667, G_B_loss: 0.5530\n",
      "Epoch [19/30], Step [981/1067], D_A_loss: 0.1730, D_B_loss: 0.1456, G_A_loss: 0.7102, G_B_loss: 0.4771\n",
      "Epoch [19/30], Step [991/1067], D_A_loss: 0.0912, D_B_loss: 0.0177, G_A_loss: 0.5608, G_B_loss: 0.5432\n",
      "Epoch [19/30], Step [1001/1067], D_A_loss: 0.0739, D_B_loss: 0.0470, G_A_loss: 0.6246, G_B_loss: 0.6468\n",
      "Epoch [19/30], Step [1011/1067], D_A_loss: 0.0548, D_B_loss: 0.0769, G_A_loss: 1.0370, G_B_loss: 0.5213\n",
      "Epoch [19/30], Step [1021/1067], D_A_loss: 0.1161, D_B_loss: 0.2207, G_A_loss: 0.6690, G_B_loss: 0.3561\n",
      "Epoch [19/30], Step [1031/1067], D_A_loss: 0.0842, D_B_loss: 0.0717, G_A_loss: 0.7472, G_B_loss: 0.5128\n",
      "Epoch [19/30], Step [1041/1067], D_A_loss: 0.0499, D_B_loss: 0.2597, G_A_loss: 1.1180, G_B_loss: 0.3876\n",
      "Epoch [19/30], Step [1051/1067], D_A_loss: 0.1726, D_B_loss: 0.1173, G_A_loss: 0.4705, G_B_loss: 0.2207\n",
      "Epoch [19/30], Step [1061/1067], D_A_loss: 0.0693, D_B_loss: 0.0678, G_A_loss: 0.3705, G_B_loss: 0.0903\n",
      "Epoch [20/30], Step [1/1067], D_A_loss: 0.0240, D_B_loss: 0.0635, G_A_loss: 0.5358, G_B_loss: 0.9666\n",
      "Epoch [20/30], Step [11/1067], D_A_loss: 0.1315, D_B_loss: 0.0844, G_A_loss: 0.8189, G_B_loss: 0.5089\n",
      "Epoch [20/30], Step [21/1067], D_A_loss: 0.0140, D_B_loss: 0.1119, G_A_loss: 0.9659, G_B_loss: 0.2584\n",
      "Epoch [20/30], Step [31/1067], D_A_loss: 0.1848, D_B_loss: 0.0646, G_A_loss: 0.4594, G_B_loss: 0.8390\n",
      "Epoch [20/30], Step [41/1067], D_A_loss: 0.0443, D_B_loss: 0.0190, G_A_loss: 0.9485, G_B_loss: 0.6619\n",
      "Epoch [20/30], Step [51/1067], D_A_loss: 0.1829, D_B_loss: 0.1540, G_A_loss: 0.4562, G_B_loss: 0.2969\n",
      "Epoch [20/30], Step [61/1067], D_A_loss: 0.0850, D_B_loss: 0.0636, G_A_loss: 0.5434, G_B_loss: 0.3466\n",
      "Epoch [20/30], Step [71/1067], D_A_loss: 0.0361, D_B_loss: 0.0259, G_A_loss: 1.1074, G_B_loss: 0.4863\n",
      "Epoch [20/30], Step [81/1067], D_A_loss: 0.1784, D_B_loss: 0.0500, G_A_loss: 0.5355, G_B_loss: 0.2526\n",
      "Epoch [20/30], Step [91/1067], D_A_loss: 0.1705, D_B_loss: 0.0282, G_A_loss: 0.8787, G_B_loss: 0.3560\n",
      "Epoch [20/30], Step [101/1067], D_A_loss: 0.0515, D_B_loss: 0.2098, G_A_loss: 0.2626, G_B_loss: 0.4597\n",
      "Epoch [20/30], Step [111/1067], D_A_loss: 0.3372, D_B_loss: 0.0425, G_A_loss: 0.3645, G_B_loss: 0.3036\n",
      "Epoch [20/30], Step [121/1067], D_A_loss: 0.0656, D_B_loss: 0.0393, G_A_loss: 0.7169, G_B_loss: 0.7660\n",
      "Epoch [20/30], Step [131/1067], D_A_loss: 0.1482, D_B_loss: 0.0297, G_A_loss: 0.8376, G_B_loss: 0.3574\n",
      "Epoch [20/30], Step [141/1067], D_A_loss: 0.0817, D_B_loss: 0.3408, G_A_loss: 0.0695, G_B_loss: 0.3108\n",
      "Epoch [20/30], Step [151/1067], D_A_loss: 0.1820, D_B_loss: 0.0149, G_A_loss: 0.6954, G_B_loss: 0.3127\n",
      "Epoch [20/30], Step [161/1067], D_A_loss: 0.1565, D_B_loss: 0.1700, G_A_loss: 0.2984, G_B_loss: 0.4212\n",
      "Epoch [20/30], Step [171/1067], D_A_loss: 0.2317, D_B_loss: 0.0456, G_A_loss: 0.5223, G_B_loss: 0.1724\n",
      "Epoch [20/30], Step [181/1067], D_A_loss: 0.0273, D_B_loss: 0.0163, G_A_loss: 0.9101, G_B_loss: 0.6039\n",
      "Epoch [20/30], Step [191/1067], D_A_loss: 0.0386, D_B_loss: 0.0551, G_A_loss: 0.5168, G_B_loss: 0.2583\n",
      "Epoch [20/30], Step [201/1067], D_A_loss: 0.0998, D_B_loss: 0.3300, G_A_loss: 0.6563, G_B_loss: 0.0619\n",
      "Epoch [20/30], Step [211/1067], D_A_loss: 0.0689, D_B_loss: 0.0763, G_A_loss: 0.3793, G_B_loss: 0.8963\n",
      "Epoch [20/30], Step [221/1067], D_A_loss: 0.1028, D_B_loss: 0.0719, G_A_loss: 0.7363, G_B_loss: 0.5331\n",
      "Epoch [20/30], Step [231/1067], D_A_loss: 0.0665, D_B_loss: 0.0255, G_A_loss: 0.7200, G_B_loss: 0.5581\n",
      "Epoch [20/30], Step [241/1067], D_A_loss: 0.0625, D_B_loss: 0.0244, G_A_loss: 0.7310, G_B_loss: 0.4583\n",
      "Epoch [20/30], Step [251/1067], D_A_loss: 0.1038, D_B_loss: 0.0520, G_A_loss: 0.5249, G_B_loss: 0.4410\n",
      "Epoch [20/30], Step [261/1067], D_A_loss: 0.0534, D_B_loss: 0.0185, G_A_loss: 0.8181, G_B_loss: 0.5050\n",
      "Epoch [20/30], Step [271/1067], D_A_loss: 0.1129, D_B_loss: 0.0944, G_A_loss: 0.4820, G_B_loss: 0.8990\n",
      "Epoch [20/30], Step [281/1067], D_A_loss: 0.0542, D_B_loss: 0.0976, G_A_loss: 0.4376, G_B_loss: 0.6606\n",
      "Epoch [20/30], Step [291/1067], D_A_loss: 0.1427, D_B_loss: 0.0914, G_A_loss: 0.4647, G_B_loss: 1.2434\n",
      "Epoch [20/30], Step [301/1067], D_A_loss: 0.0414, D_B_loss: 0.1027, G_A_loss: 0.8246, G_B_loss: 0.3600\n",
      "Epoch [20/30], Step [311/1067], D_A_loss: 0.0509, D_B_loss: 0.0647, G_A_loss: 0.6926, G_B_loss: 0.6153\n",
      "Epoch [20/30], Step [321/1067], D_A_loss: 0.4627, D_B_loss: 0.0270, G_A_loss: 0.5424, G_B_loss: 0.7328\n",
      "Epoch [20/30], Step [331/1067], D_A_loss: 0.1130, D_B_loss: 0.0259, G_A_loss: 0.9861, G_B_loss: 0.6653\n",
      "Epoch [20/30], Step [341/1067], D_A_loss: 0.1408, D_B_loss: 0.0712, G_A_loss: 0.8521, G_B_loss: 0.6888\n",
      "Epoch [20/30], Step [351/1067], D_A_loss: 0.1254, D_B_loss: 0.0353, G_A_loss: 0.9220, G_B_loss: 0.4964\n",
      "Epoch [20/30], Step [361/1067], D_A_loss: 0.0993, D_B_loss: 0.0414, G_A_loss: 0.9779, G_B_loss: 0.2842\n",
      "Epoch [20/30], Step [371/1067], D_A_loss: 0.1785, D_B_loss: 0.0692, G_A_loss: 0.7679, G_B_loss: 0.2254\n",
      "Epoch [20/30], Step [381/1067], D_A_loss: 0.0508, D_B_loss: 0.0257, G_A_loss: 0.4461, G_B_loss: 0.3453\n",
      "Epoch [20/30], Step [391/1067], D_A_loss: 0.1541, D_B_loss: 0.0680, G_A_loss: 0.8348, G_B_loss: 0.2929\n",
      "Epoch [20/30], Step [401/1067], D_A_loss: 0.1814, D_B_loss: 0.1493, G_A_loss: 0.7221, G_B_loss: 0.6979\n",
      "Epoch [20/30], Step [411/1067], D_A_loss: 0.1022, D_B_loss: 0.0413, G_A_loss: 0.6901, G_B_loss: 0.3261\n",
      "Epoch [20/30], Step [421/1067], D_A_loss: 0.0402, D_B_loss: 0.0376, G_A_loss: 0.4287, G_B_loss: 0.4005\n",
      "Epoch [20/30], Step [431/1067], D_A_loss: 0.0726, D_B_loss: 0.0320, G_A_loss: 0.7196, G_B_loss: 0.6047\n",
      "Epoch [20/30], Step [441/1067], D_A_loss: 0.1094, D_B_loss: 0.1750, G_A_loss: 0.4566, G_B_loss: 0.2588\n",
      "Epoch [20/30], Step [451/1067], D_A_loss: 0.2551, D_B_loss: 0.2526, G_A_loss: 0.3349, G_B_loss: 0.2022\n",
      "Epoch [20/30], Step [461/1067], D_A_loss: 0.0522, D_B_loss: 0.0874, G_A_loss: 1.1706, G_B_loss: 0.5650\n",
      "Epoch [20/30], Step [471/1067], D_A_loss: 0.1103, D_B_loss: 0.1222, G_A_loss: 0.6750, G_B_loss: 0.3865\n",
      "Epoch [20/30], Step [481/1067], D_A_loss: 0.1155, D_B_loss: 0.0465, G_A_loss: 0.4399, G_B_loss: 0.3581\n",
      "Epoch [20/30], Step [491/1067], D_A_loss: 0.1481, D_B_loss: 0.1111, G_A_loss: 0.4348, G_B_loss: 0.2855\n",
      "Epoch [20/30], Step [501/1067], D_A_loss: 0.1671, D_B_loss: 0.0764, G_A_loss: 0.7588, G_B_loss: 0.6007\n",
      "Epoch [20/30], Step [511/1067], D_A_loss: 0.1833, D_B_loss: 0.0555, G_A_loss: 0.5759, G_B_loss: 0.9824\n",
      "Epoch [20/30], Step [521/1067], D_A_loss: 0.0863, D_B_loss: 0.0631, G_A_loss: 0.5150, G_B_loss: 0.3009\n",
      "Epoch [20/30], Step [531/1067], D_A_loss: 0.0839, D_B_loss: 0.0425, G_A_loss: 0.7164, G_B_loss: 0.8337\n",
      "Epoch [20/30], Step [541/1067], D_A_loss: 0.1230, D_B_loss: 0.0343, G_A_loss: 0.5678, G_B_loss: 0.3292\n",
      "Epoch [20/30], Step [551/1067], D_A_loss: 0.2299, D_B_loss: 0.1130, G_A_loss: 0.8414, G_B_loss: 0.6706\n",
      "Epoch [20/30], Step [561/1067], D_A_loss: 0.0353, D_B_loss: 0.0745, G_A_loss: 0.1802, G_B_loss: 0.3277\n",
      "Epoch [20/30], Step [571/1067], D_A_loss: 0.0761, D_B_loss: 0.0254, G_A_loss: 0.5383, G_B_loss: 0.5535\n",
      "Epoch [20/30], Step [581/1067], D_A_loss: 0.1057, D_B_loss: 0.0230, G_A_loss: 0.6928, G_B_loss: 0.6050\n",
      "Epoch [20/30], Step [591/1067], D_A_loss: 0.3488, D_B_loss: 0.0599, G_A_loss: 0.8013, G_B_loss: 0.7000\n",
      "Epoch [20/30], Step [601/1067], D_A_loss: 0.1130, D_B_loss: 0.0269, G_A_loss: 0.7290, G_B_loss: 0.3447\n",
      "Epoch [20/30], Step [611/1067], D_A_loss: 0.1137, D_B_loss: 0.0237, G_A_loss: 0.4884, G_B_loss: 0.3630\n",
      "Epoch [20/30], Step [621/1067], D_A_loss: 0.0496, D_B_loss: 0.0502, G_A_loss: 0.4051, G_B_loss: 0.6532\n",
      "Epoch [20/30], Step [631/1067], D_A_loss: 0.1094, D_B_loss: 0.0537, G_A_loss: 0.8962, G_B_loss: 0.4871\n",
      "Epoch [20/30], Step [641/1067], D_A_loss: 0.1667, D_B_loss: 0.0600, G_A_loss: 1.2434, G_B_loss: 0.7934\n",
      "Epoch [20/30], Step [651/1067], D_A_loss: 0.1258, D_B_loss: 0.0325, G_A_loss: 0.6659, G_B_loss: 0.6121\n",
      "Epoch [20/30], Step [661/1067], D_A_loss: 0.0648, D_B_loss: 0.0481, G_A_loss: 1.6320, G_B_loss: 0.3781\n",
      "Epoch [20/30], Step [671/1067], D_A_loss: 0.0768, D_B_loss: 0.0335, G_A_loss: 0.5325, G_B_loss: 0.2216\n",
      "Epoch [20/30], Step [681/1067], D_A_loss: 0.0352, D_B_loss: 0.0726, G_A_loss: 0.8425, G_B_loss: 0.4791\n",
      "Epoch [20/30], Step [691/1067], D_A_loss: 0.1032, D_B_loss: 0.0616, G_A_loss: 0.8632, G_B_loss: 0.3755\n",
      "Epoch [20/30], Step [701/1067], D_A_loss: 0.0468, D_B_loss: 0.1356, G_A_loss: 0.3287, G_B_loss: 0.9229\n",
      "Epoch [20/30], Step [711/1067], D_A_loss: 0.0890, D_B_loss: 0.0616, G_A_loss: 0.7626, G_B_loss: 0.3314\n",
      "Epoch [20/30], Step [721/1067], D_A_loss: 0.0904, D_B_loss: 0.0647, G_A_loss: 0.8551, G_B_loss: 0.5263\n",
      "Epoch [20/30], Step [731/1067], D_A_loss: 0.0757, D_B_loss: 0.0551, G_A_loss: 1.2038, G_B_loss: 0.4705\n",
      "Epoch [20/30], Step [741/1067], D_A_loss: 0.0765, D_B_loss: 0.0839, G_A_loss: 0.2768, G_B_loss: 0.5626\n",
      "Epoch [20/30], Step [751/1067], D_A_loss: 0.0537, D_B_loss: 0.0307, G_A_loss: 0.7034, G_B_loss: 0.8068\n",
      "Epoch [20/30], Step [761/1067], D_A_loss: 0.1441, D_B_loss: 0.0784, G_A_loss: 0.6201, G_B_loss: 0.3235\n",
      "Epoch [20/30], Step [771/1067], D_A_loss: 0.0648, D_B_loss: 0.0238, G_A_loss: 0.3929, G_B_loss: 0.6876\n",
      "Epoch [20/30], Step [781/1067], D_A_loss: 0.0690, D_B_loss: 0.0220, G_A_loss: 0.6491, G_B_loss: 0.5895\n",
      "Epoch [20/30], Step [791/1067], D_A_loss: 0.1993, D_B_loss: 0.0368, G_A_loss: 0.4370, G_B_loss: 0.2464\n",
      "Epoch [20/30], Step [801/1067], D_A_loss: 0.1049, D_B_loss: 0.0949, G_A_loss: 0.5701, G_B_loss: 0.5057\n",
      "Epoch [20/30], Step [811/1067], D_A_loss: 0.0469, D_B_loss: 0.0764, G_A_loss: 0.7745, G_B_loss: 0.6955\n",
      "Epoch [20/30], Step [821/1067], D_A_loss: 0.0508, D_B_loss: 0.0459, G_A_loss: 1.1196, G_B_loss: 0.6304\n",
      "Epoch [20/30], Step [831/1067], D_A_loss: 0.0828, D_B_loss: 0.0491, G_A_loss: 0.5919, G_B_loss: 0.5742\n",
      "Epoch [20/30], Step [841/1067], D_A_loss: 0.2328, D_B_loss: 0.0563, G_A_loss: 0.7041, G_B_loss: 0.5339\n",
      "Epoch [20/30], Step [851/1067], D_A_loss: 0.0750, D_B_loss: 0.0243, G_A_loss: 0.8272, G_B_loss: 0.5365\n",
      "Epoch [20/30], Step [861/1067], D_A_loss: 0.0991, D_B_loss: 0.0207, G_A_loss: 0.6881, G_B_loss: 1.0914\n",
      "Epoch [20/30], Step [871/1067], D_A_loss: 0.0804, D_B_loss: 0.0325, G_A_loss: 1.1540, G_B_loss: 0.4524\n",
      "Epoch [20/30], Step [881/1067], D_A_loss: 0.0414, D_B_loss: 0.0387, G_A_loss: 1.2232, G_B_loss: 0.6563\n",
      "Epoch [20/30], Step [891/1067], D_A_loss: 0.1082, D_B_loss: 0.2105, G_A_loss: 0.8171, G_B_loss: 0.4154\n",
      "Epoch [20/30], Step [901/1067], D_A_loss: 0.0884, D_B_loss: 0.3225, G_A_loss: 0.7072, G_B_loss: 0.4832\n",
      "Epoch [20/30], Step [911/1067], D_A_loss: 0.1086, D_B_loss: 0.1725, G_A_loss: 0.5325, G_B_loss: 0.5761\n",
      "Epoch [20/30], Step [921/1067], D_A_loss: 0.0724, D_B_loss: 0.0909, G_A_loss: 0.8910, G_B_loss: 0.3184\n",
      "Epoch [20/30], Step [931/1067], D_A_loss: 0.0585, D_B_loss: 0.0763, G_A_loss: 0.4562, G_B_loss: 0.8176\n",
      "Epoch [20/30], Step [941/1067], D_A_loss: 0.1400, D_B_loss: 0.0522, G_A_loss: 0.5140, G_B_loss: 0.2985\n",
      "Epoch [20/30], Step [951/1067], D_A_loss: 0.3315, D_B_loss: 0.0686, G_A_loss: 0.8366, G_B_loss: 0.9067\n",
      "Epoch [20/30], Step [961/1067], D_A_loss: 0.2300, D_B_loss: 0.2742, G_A_loss: 0.3863, G_B_loss: 0.2417\n",
      "Epoch [20/30], Step [971/1067], D_A_loss: 0.0761, D_B_loss: 0.1618, G_A_loss: 0.4600, G_B_loss: 0.4921\n",
      "Epoch [20/30], Step [981/1067], D_A_loss: 0.1397, D_B_loss: 0.0756, G_A_loss: 0.9743, G_B_loss: 0.4076\n",
      "Epoch [20/30], Step [991/1067], D_A_loss: 0.2714, D_B_loss: 0.0436, G_A_loss: 0.6423, G_B_loss: 0.1903\n",
      "Epoch [20/30], Step [1001/1067], D_A_loss: 0.0817, D_B_loss: 0.1737, G_A_loss: 0.5293, G_B_loss: 0.4739\n",
      "Epoch [20/30], Step [1011/1067], D_A_loss: 0.0319, D_B_loss: 0.0861, G_A_loss: 0.7086, G_B_loss: 0.9668\n",
      "Epoch [20/30], Step [1021/1067], D_A_loss: 0.0833, D_B_loss: 0.1238, G_A_loss: 0.5803, G_B_loss: 0.6555\n",
      "Epoch [20/30], Step [1031/1067], D_A_loss: 0.0978, D_B_loss: 0.1215, G_A_loss: 0.8394, G_B_loss: 0.9458\n",
      "Epoch [20/30], Step [1041/1067], D_A_loss: 0.1599, D_B_loss: 0.0351, G_A_loss: 0.5942, G_B_loss: 0.3408\n",
      "Epoch [20/30], Step [1051/1067], D_A_loss: 0.0781, D_B_loss: 0.0377, G_A_loss: 1.0112, G_B_loss: 0.5353\n",
      "Epoch [20/30], Step [1061/1067], D_A_loss: 0.0549, D_B_loss: 0.0603, G_A_loss: 1.1149, G_B_loss: 1.1053\n",
      "Epoch [21/30], Step [1/1067], D_A_loss: 0.0869, D_B_loss: 0.0521, G_A_loss: 0.6674, G_B_loss: 0.9383\n",
      "Epoch [21/30], Step [11/1067], D_A_loss: 0.1168, D_B_loss: 0.1252, G_A_loss: 0.5479, G_B_loss: 0.3487\n",
      "Epoch [21/30], Step [21/1067], D_A_loss: 0.2058, D_B_loss: 0.0290, G_A_loss: 0.5865, G_B_loss: 0.4540\n",
      "Epoch [21/30], Step [31/1067], D_A_loss: 0.0498, D_B_loss: 0.0748, G_A_loss: 0.5999, G_B_loss: 0.7977\n",
      "Epoch [21/30], Step [41/1067], D_A_loss: 0.0970, D_B_loss: 0.0245, G_A_loss: 0.6799, G_B_loss: 0.9687\n",
      "Epoch [21/30], Step [51/1067], D_A_loss: 0.1051, D_B_loss: 0.0275, G_A_loss: 1.0039, G_B_loss: 0.4892\n",
      "Epoch [21/30], Step [61/1067], D_A_loss: 0.0750, D_B_loss: 0.0274, G_A_loss: 0.9448, G_B_loss: 0.6588\n",
      "Epoch [21/30], Step [71/1067], D_A_loss: 0.0252, D_B_loss: 0.0274, G_A_loss: 0.8497, G_B_loss: 0.7844\n",
      "Epoch [21/30], Step [81/1067], D_A_loss: 0.1076, D_B_loss: 0.0267, G_A_loss: 0.4023, G_B_loss: 0.4112\n",
      "Epoch [21/30], Step [91/1067], D_A_loss: 0.0946, D_B_loss: 0.0471, G_A_loss: 0.5577, G_B_loss: 0.4917\n",
      "Epoch [21/30], Step [101/1067], D_A_loss: 0.2099, D_B_loss: 0.0675, G_A_loss: 0.6094, G_B_loss: 0.4437\n",
      "Epoch [21/30], Step [111/1067], D_A_loss: 0.1397, D_B_loss: 0.0773, G_A_loss: 0.6151, G_B_loss: 0.3229\n",
      "Epoch [21/30], Step [121/1067], D_A_loss: 0.0603, D_B_loss: 0.0719, G_A_loss: 0.4988, G_B_loss: 0.7415\n",
      "Epoch [21/30], Step [131/1067], D_A_loss: 0.0435, D_B_loss: 0.0485, G_A_loss: 0.5993, G_B_loss: 0.6175\n",
      "Epoch [21/30], Step [141/1067], D_A_loss: 0.0723, D_B_loss: 0.1153, G_A_loss: 0.5194, G_B_loss: 0.5187\n",
      "Epoch [21/30], Step [151/1067], D_A_loss: 0.0528, D_B_loss: 0.0355, G_A_loss: 0.8958, G_B_loss: 0.7897\n",
      "Epoch [21/30], Step [161/1067], D_A_loss: 0.0647, D_B_loss: 0.0891, G_A_loss: 1.0604, G_B_loss: 0.2292\n",
      "Epoch [21/30], Step [171/1067], D_A_loss: 0.1084, D_B_loss: 0.0487, G_A_loss: 0.6832, G_B_loss: 0.6616\n",
      "Epoch [21/30], Step [181/1067], D_A_loss: 0.0329, D_B_loss: 0.0992, G_A_loss: 0.3937, G_B_loss: 0.2916\n",
      "Epoch [21/30], Step [191/1067], D_A_loss: 0.0413, D_B_loss: 0.0670, G_A_loss: 0.5777, G_B_loss: 0.6541\n",
      "Epoch [21/30], Step [201/1067], D_A_loss: 0.0346, D_B_loss: 0.0651, G_A_loss: 0.6587, G_B_loss: 0.8342\n",
      "Epoch [21/30], Step [211/1067], D_A_loss: 0.0709, D_B_loss: 0.0501, G_A_loss: 0.5836, G_B_loss: 0.6200\n",
      "Epoch [21/30], Step [221/1067], D_A_loss: 0.0587, D_B_loss: 0.0531, G_A_loss: 0.5641, G_B_loss: 0.8259\n",
      "Epoch [21/30], Step [231/1067], D_A_loss: 0.0674, D_B_loss: 0.1992, G_A_loss: 1.5471, G_B_loss: 0.3721\n",
      "Epoch [21/30], Step [241/1067], D_A_loss: 0.1164, D_B_loss: 0.0556, G_A_loss: 0.7939, G_B_loss: 0.3673\n",
      "Epoch [21/30], Step [251/1067], D_A_loss: 0.0726, D_B_loss: 0.2382, G_A_loss: 0.2615, G_B_loss: 0.3240\n",
      "Epoch [21/30], Step [261/1067], D_A_loss: 0.0291, D_B_loss: 0.0989, G_A_loss: 0.3738, G_B_loss: 0.3948\n",
      "Epoch [21/30], Step [271/1067], D_A_loss: 0.1048, D_B_loss: 0.0522, G_A_loss: 0.9825, G_B_loss: 0.7339\n",
      "Epoch [21/30], Step [281/1067], D_A_loss: 0.0483, D_B_loss: 0.2202, G_A_loss: 0.9412, G_B_loss: 0.5306\n",
      "Epoch [21/30], Step [291/1067], D_A_loss: 0.0655, D_B_loss: 0.0434, G_A_loss: 0.5550, G_B_loss: 0.7965\n",
      "Epoch [21/30], Step [301/1067], D_A_loss: 0.0979, D_B_loss: 0.0315, G_A_loss: 0.3721, G_B_loss: 0.9688\n",
      "Epoch [21/30], Step [311/1067], D_A_loss: 0.0709, D_B_loss: 0.0488, G_A_loss: 1.0896, G_B_loss: 0.9818\n",
      "Epoch [21/30], Step [321/1067], D_A_loss: 0.0278, D_B_loss: 0.0346, G_A_loss: 0.6700, G_B_loss: 0.8735\n",
      "Epoch [21/30], Step [331/1067], D_A_loss: 0.1133, D_B_loss: 0.0516, G_A_loss: 0.4157, G_B_loss: 0.3519\n",
      "Epoch [21/30], Step [341/1067], D_A_loss: 0.1008, D_B_loss: 0.1678, G_A_loss: 0.4668, G_B_loss: 0.6999\n",
      "Epoch [21/30], Step [351/1067], D_A_loss: 0.1253, D_B_loss: 0.0279, G_A_loss: 0.4542, G_B_loss: 0.3342\n",
      "Epoch [21/30], Step [361/1067], D_A_loss: 0.1086, D_B_loss: 0.0751, G_A_loss: 0.7768, G_B_loss: 0.5886\n",
      "Epoch [21/30], Step [371/1067], D_A_loss: 0.0463, D_B_loss: 0.1470, G_A_loss: 0.4589, G_B_loss: 0.6470\n",
      "Epoch [21/30], Step [381/1067], D_A_loss: 0.1674, D_B_loss: 0.0897, G_A_loss: 0.7485, G_B_loss: 0.4770\n",
      "Epoch [21/30], Step [391/1067], D_A_loss: 0.0877, D_B_loss: 0.2056, G_A_loss: 0.9145, G_B_loss: 0.6147\n",
      "Epoch [21/30], Step [401/1067], D_A_loss: 0.0594, D_B_loss: 0.1468, G_A_loss: 0.3445, G_B_loss: 0.2858\n",
      "Epoch [21/30], Step [411/1067], D_A_loss: 0.0767, D_B_loss: 0.0403, G_A_loss: 0.5982, G_B_loss: 0.4916\n",
      "Epoch [21/30], Step [421/1067], D_A_loss: 0.1365, D_B_loss: 0.0394, G_A_loss: 0.6189, G_B_loss: 0.5350\n",
      "Epoch [21/30], Step [431/1067], D_A_loss: 0.0522, D_B_loss: 0.1663, G_A_loss: 0.2536, G_B_loss: 0.4205\n",
      "Epoch [21/30], Step [441/1067], D_A_loss: 0.1993, D_B_loss: 0.0273, G_A_loss: 1.2539, G_B_loss: 1.0268\n",
      "Epoch [21/30], Step [451/1067], D_A_loss: 0.0610, D_B_loss: 0.0408, G_A_loss: 0.8113, G_B_loss: 0.5153\n",
      "Epoch [21/30], Step [461/1067], D_A_loss: 0.0446, D_B_loss: 0.0929, G_A_loss: 0.9754, G_B_loss: 0.6676\n",
      "Epoch [21/30], Step [471/1067], D_A_loss: 0.1678, D_B_loss: 0.0432, G_A_loss: 0.7039, G_B_loss: 0.4055\n",
      "Epoch [21/30], Step [481/1067], D_A_loss: 0.0483, D_B_loss: 0.0245, G_A_loss: 0.4159, G_B_loss: 0.4290\n",
      "Epoch [21/30], Step [491/1067], D_A_loss: 0.0469, D_B_loss: 0.1327, G_A_loss: 0.4976, G_B_loss: 0.6085\n",
      "Epoch [21/30], Step [501/1067], D_A_loss: 0.0718, D_B_loss: 0.0435, G_A_loss: 0.6328, G_B_loss: 0.7194\n",
      "Epoch [21/30], Step [511/1067], D_A_loss: 0.1717, D_B_loss: 0.0493, G_A_loss: 0.9122, G_B_loss: 0.4814\n",
      "Epoch [21/30], Step [521/1067], D_A_loss: 0.0642, D_B_loss: 0.1618, G_A_loss: 0.3929, G_B_loss: 0.4931\n",
      "Epoch [21/30], Step [531/1067], D_A_loss: 0.1945, D_B_loss: 0.0560, G_A_loss: 0.4857, G_B_loss: 0.8174\n",
      "Epoch [21/30], Step [541/1067], D_A_loss: 0.1867, D_B_loss: 0.0472, G_A_loss: 0.7393, G_B_loss: 0.6696\n",
      "Epoch [21/30], Step [551/1067], D_A_loss: 0.1085, D_B_loss: 0.1303, G_A_loss: 0.4234, G_B_loss: 0.5125\n",
      "Epoch [21/30], Step [561/1067], D_A_loss: 0.1110, D_B_loss: 0.0850, G_A_loss: 1.0908, G_B_loss: 0.7429\n",
      "Epoch [21/30], Step [571/1067], D_A_loss: 0.0654, D_B_loss: 0.0157, G_A_loss: 0.8937, G_B_loss: 0.7422\n",
      "Epoch [21/30], Step [581/1067], D_A_loss: 0.1242, D_B_loss: 0.1115, G_A_loss: 0.3593, G_B_loss: 0.3433\n",
      "Epoch [21/30], Step [591/1067], D_A_loss: 0.0240, D_B_loss: 0.0443, G_A_loss: 0.6799, G_B_loss: 0.4235\n",
      "Epoch [21/30], Step [601/1067], D_A_loss: 0.0942, D_B_loss: 0.0485, G_A_loss: 0.7680, G_B_loss: 0.4207\n",
      "Epoch [21/30], Step [611/1067], D_A_loss: 0.0175, D_B_loss: 0.0358, G_A_loss: 0.7482, G_B_loss: 1.0751\n",
      "Epoch [21/30], Step [621/1067], D_A_loss: 0.2287, D_B_loss: 0.0242, G_A_loss: 0.5618, G_B_loss: 0.1465\n",
      "Epoch [21/30], Step [631/1067], D_A_loss: 0.0409, D_B_loss: 0.0601, G_A_loss: 0.5039, G_B_loss: 0.3810\n",
      "Epoch [21/30], Step [641/1067], D_A_loss: 0.0857, D_B_loss: 0.0832, G_A_loss: 0.3998, G_B_loss: 1.0348\n",
      "Epoch [21/30], Step [651/1067], D_A_loss: 0.0701, D_B_loss: 0.0499, G_A_loss: 0.5712, G_B_loss: 0.5404\n",
      "Epoch [21/30], Step [661/1067], D_A_loss: 0.0984, D_B_loss: 0.0259, G_A_loss: 0.2978, G_B_loss: 0.3483\n",
      "Epoch [21/30], Step [671/1067], D_A_loss: 0.0171, D_B_loss: 0.1182, G_A_loss: 0.9476, G_B_loss: 0.1766\n",
      "Epoch [21/30], Step [681/1067], D_A_loss: 0.0845, D_B_loss: 0.0552, G_A_loss: 0.8160, G_B_loss: 0.4591\n",
      "Epoch [21/30], Step [691/1067], D_A_loss: 0.1988, D_B_loss: 0.1518, G_A_loss: 0.3653, G_B_loss: 0.1958\n",
      "Epoch [21/30], Step [701/1067], D_A_loss: 0.1016, D_B_loss: 0.0672, G_A_loss: 0.5055, G_B_loss: 0.4669\n",
      "Epoch [21/30], Step [711/1067], D_A_loss: 0.0968, D_B_loss: 0.0541, G_A_loss: 0.5251, G_B_loss: 0.7115\n",
      "Epoch [21/30], Step [721/1067], D_A_loss: 0.0525, D_B_loss: 0.0276, G_A_loss: 0.6020, G_B_loss: 0.9344\n",
      "Epoch [21/30], Step [731/1067], D_A_loss: 0.0571, D_B_loss: 0.1431, G_A_loss: 0.3778, G_B_loss: 0.7431\n",
      "Epoch [21/30], Step [741/1067], D_A_loss: 0.0466, D_B_loss: 0.2414, G_A_loss: 0.2020, G_B_loss: 0.5795\n",
      "Epoch [21/30], Step [751/1067], D_A_loss: 0.0955, D_B_loss: 0.0785, G_A_loss: 1.2556, G_B_loss: 0.4005\n",
      "Epoch [21/30], Step [761/1067], D_A_loss: 0.1170, D_B_loss: 0.0283, G_A_loss: 0.8030, G_B_loss: 0.9889\n",
      "Epoch [21/30], Step [771/1067], D_A_loss: 0.1433, D_B_loss: 0.1549, G_A_loss: 0.3934, G_B_loss: 0.6971\n",
      "Epoch [21/30], Step [781/1067], D_A_loss: 0.0490, D_B_loss: 0.1797, G_A_loss: 0.2742, G_B_loss: 0.1891\n",
      "Epoch [21/30], Step [791/1067], D_A_loss: 0.0713, D_B_loss: 0.0779, G_A_loss: 1.2101, G_B_loss: 0.5812\n",
      "Epoch [21/30], Step [801/1067], D_A_loss: 0.1428, D_B_loss: 0.0318, G_A_loss: 1.0400, G_B_loss: 0.3525\n",
      "Epoch [21/30], Step [811/1067], D_A_loss: 0.1566, D_B_loss: 0.1492, G_A_loss: 1.0788, G_B_loss: 0.2762\n",
      "Epoch [21/30], Step [821/1067], D_A_loss: 0.1863, D_B_loss: 0.0296, G_A_loss: 0.2368, G_B_loss: 0.7035\n",
      "Epoch [21/30], Step [831/1067], D_A_loss: 0.1300, D_B_loss: 0.0222, G_A_loss: 0.4354, G_B_loss: 0.5746\n",
      "Epoch [21/30], Step [841/1067], D_A_loss: 0.0933, D_B_loss: 0.0904, G_A_loss: 1.1209, G_B_loss: 0.4982\n",
      "Epoch [21/30], Step [851/1067], D_A_loss: 0.1006, D_B_loss: 0.0291, G_A_loss: 0.8111, G_B_loss: 0.4021\n",
      "Epoch [21/30], Step [861/1067], D_A_loss: 0.0690, D_B_loss: 0.0178, G_A_loss: 0.6103, G_B_loss: 0.8434\n",
      "Epoch [21/30], Step [871/1067], D_A_loss: 0.0212, D_B_loss: 0.1604, G_A_loss: 0.2887, G_B_loss: 0.6796\n",
      "Epoch [21/30], Step [881/1067], D_A_loss: 0.0283, D_B_loss: 0.0923, G_A_loss: 0.8453, G_B_loss: 0.7408\n",
      "Epoch [21/30], Step [891/1067], D_A_loss: 0.1116, D_B_loss: 0.0433, G_A_loss: 0.9624, G_B_loss: 0.8874\n",
      "Epoch [21/30], Step [901/1067], D_A_loss: 0.0490, D_B_loss: 0.0461, G_A_loss: 0.6174, G_B_loss: 0.7677\n",
      "Epoch [21/30], Step [911/1067], D_A_loss: 0.1506, D_B_loss: 0.1505, G_A_loss: 0.2871, G_B_loss: 0.5480\n",
      "Epoch [21/30], Step [921/1067], D_A_loss: 0.0954, D_B_loss: 0.0200, G_A_loss: 0.8433, G_B_loss: 0.6451\n",
      "Epoch [21/30], Step [931/1067], D_A_loss: 0.0322, D_B_loss: 0.0638, G_A_loss: 0.4793, G_B_loss: 0.6661\n",
      "Epoch [21/30], Step [941/1067], D_A_loss: 0.0892, D_B_loss: 0.1106, G_A_loss: 0.4357, G_B_loss: 0.4904\n",
      "Epoch [21/30], Step [951/1067], D_A_loss: 0.0545, D_B_loss: 0.0698, G_A_loss: 0.4372, G_B_loss: 0.7593\n",
      "Epoch [21/30], Step [961/1067], D_A_loss: 0.0422, D_B_loss: 0.1362, G_A_loss: 0.3693, G_B_loss: 0.7718\n",
      "Epoch [21/30], Step [971/1067], D_A_loss: 0.0402, D_B_loss: 0.0921, G_A_loss: 0.4806, G_B_loss: 0.3336\n",
      "Epoch [21/30], Step [981/1067], D_A_loss: 0.1134, D_B_loss: 0.1748, G_A_loss: 1.3178, G_B_loss: 0.5333\n",
      "Epoch [21/30], Step [991/1067], D_A_loss: 0.1313, D_B_loss: 0.0393, G_A_loss: 0.7320, G_B_loss: 0.5030\n",
      "Epoch [21/30], Step [1001/1067], D_A_loss: 0.0441, D_B_loss: 0.0348, G_A_loss: 1.1504, G_B_loss: 0.7088\n",
      "Epoch [21/30], Step [1011/1067], D_A_loss: 0.0236, D_B_loss: 0.1573, G_A_loss: 0.6645, G_B_loss: 0.6578\n",
      "Epoch [21/30], Step [1021/1067], D_A_loss: 0.0862, D_B_loss: 0.0759, G_A_loss: 0.8591, G_B_loss: 0.4211\n",
      "Epoch [21/30], Step [1031/1067], D_A_loss: 0.0427, D_B_loss: 0.0712, G_A_loss: 0.5063, G_B_loss: 0.7666\n",
      "Epoch [21/30], Step [1041/1067], D_A_loss: 0.0473, D_B_loss: 0.0412, G_A_loss: 0.3185, G_B_loss: 0.7460\n",
      "Epoch [21/30], Step [1051/1067], D_A_loss: 0.0480, D_B_loss: 0.1489, G_A_loss: 1.0693, G_B_loss: 0.7052\n",
      "Epoch [21/30], Step [1061/1067], D_A_loss: 0.0863, D_B_loss: 0.0460, G_A_loss: 0.9969, G_B_loss: 0.5112\n",
      "Epoch [22/30], Step [1/1067], D_A_loss: 0.0588, D_B_loss: 0.0405, G_A_loss: 0.7408, G_B_loss: 0.5753\n",
      "Epoch [22/30], Step [11/1067], D_A_loss: 0.0371, D_B_loss: 0.0439, G_A_loss: 0.8169, G_B_loss: 0.9316\n",
      "Epoch [22/30], Step [21/1067], D_A_loss: 0.2118, D_B_loss: 0.0492, G_A_loss: 0.7756, G_B_loss: 1.0245\n",
      "Epoch [22/30], Step [31/1067], D_A_loss: 0.0350, D_B_loss: 0.0865, G_A_loss: 1.1265, G_B_loss: 1.2394\n",
      "Epoch [22/30], Step [41/1067], D_A_loss: 0.0508, D_B_loss: 0.0657, G_A_loss: 0.4769, G_B_loss: 0.3161\n",
      "Epoch [22/30], Step [51/1067], D_A_loss: 0.1469, D_B_loss: 0.2011, G_A_loss: 0.1733, G_B_loss: 0.5075\n",
      "Epoch [22/30], Step [61/1067], D_A_loss: 0.0484, D_B_loss: 0.0155, G_A_loss: 0.5600, G_B_loss: 0.7719\n",
      "Epoch [22/30], Step [71/1067], D_A_loss: 0.0437, D_B_loss: 0.0367, G_A_loss: 0.6904, G_B_loss: 0.5312\n",
      "Epoch [22/30], Step [81/1067], D_A_loss: 0.0911, D_B_loss: 0.0174, G_A_loss: 1.2121, G_B_loss: 0.4370\n",
      "Epoch [22/30], Step [91/1067], D_A_loss: 0.0394, D_B_loss: 0.0954, G_A_loss: 0.8535, G_B_loss: 0.5844\n",
      "Epoch [22/30], Step [101/1067], D_A_loss: 0.0564, D_B_loss: 0.0753, G_A_loss: 0.5174, G_B_loss: 0.7204\n",
      "Epoch [22/30], Step [111/1067], D_A_loss: 0.0450, D_B_loss: 0.0386, G_A_loss: 0.7161, G_B_loss: 0.6242\n",
      "Epoch [22/30], Step [121/1067], D_A_loss: 0.0375, D_B_loss: 0.1816, G_A_loss: 0.9149, G_B_loss: 0.6740\n",
      "Epoch [22/30], Step [131/1067], D_A_loss: 0.2243, D_B_loss: 0.1655, G_A_loss: 0.2118, G_B_loss: 0.2773\n",
      "Epoch [22/30], Step [141/1067], D_A_loss: 0.0865, D_B_loss: 0.0919, G_A_loss: 0.3326, G_B_loss: 0.3354\n",
      "Epoch [22/30], Step [151/1067], D_A_loss: 0.0837, D_B_loss: 0.0157, G_A_loss: 0.9275, G_B_loss: 0.4481\n",
      "Epoch [22/30], Step [161/1067], D_A_loss: 0.2008, D_B_loss: 0.0902, G_A_loss: 0.4431, G_B_loss: 0.2283\n",
      "Epoch [22/30], Step [171/1067], D_A_loss: 0.0861, D_B_loss: 0.0569, G_A_loss: 0.3925, G_B_loss: 0.5402\n",
      "Epoch [22/30], Step [181/1067], D_A_loss: 0.0968, D_B_loss: 0.0720, G_A_loss: 0.4843, G_B_loss: 0.4633\n",
      "Epoch [22/30], Step [191/1067], D_A_loss: 0.0725, D_B_loss: 0.0216, G_A_loss: 1.1799, G_B_loss: 0.4281\n",
      "Epoch [22/30], Step [201/1067], D_A_loss: 0.0358, D_B_loss: 0.0421, G_A_loss: 0.7342, G_B_loss: 0.5500\n",
      "Epoch [22/30], Step [211/1067], D_A_loss: 0.0812, D_B_loss: 0.1425, G_A_loss: 0.3047, G_B_loss: 0.4971\n",
      "Epoch [22/30], Step [221/1067], D_A_loss: 0.1346, D_B_loss: 0.0516, G_A_loss: 0.3374, G_B_loss: 0.3756\n",
      "Epoch [22/30], Step [231/1067], D_A_loss: 0.0427, D_B_loss: 0.0943, G_A_loss: 0.4021, G_B_loss: 0.2693\n",
      "Epoch [22/30], Step [241/1067], D_A_loss: 0.0600, D_B_loss: 0.1103, G_A_loss: 0.6624, G_B_loss: 0.7783\n",
      "Epoch [22/30], Step [251/1067], D_A_loss: 0.0747, D_B_loss: 0.1100, G_A_loss: 0.3584, G_B_loss: 0.5453\n",
      "Epoch [22/30], Step [261/1067], D_A_loss: 0.1623, D_B_loss: 0.2212, G_A_loss: 0.9650, G_B_loss: 0.2556\n",
      "Epoch [22/30], Step [271/1067], D_A_loss: 0.0654, D_B_loss: 0.1049, G_A_loss: 0.4630, G_B_loss: 0.5681\n",
      "Epoch [22/30], Step [281/1067], D_A_loss: 0.1294, D_B_loss: 0.0750, G_A_loss: 0.7507, G_B_loss: 0.5791\n",
      "Epoch [22/30], Step [291/1067], D_A_loss: 0.1222, D_B_loss: 0.1267, G_A_loss: 0.4525, G_B_loss: 0.8541\n",
      "Epoch [22/30], Step [301/1067], D_A_loss: 0.0302, D_B_loss: 0.0214, G_A_loss: 0.6943, G_B_loss: 0.4819\n",
      "Epoch [22/30], Step [311/1067], D_A_loss: 0.0405, D_B_loss: 0.0174, G_A_loss: 0.6896, G_B_loss: 0.7001\n",
      "Epoch [22/30], Step [321/1067], D_A_loss: 0.0227, D_B_loss: 0.0669, G_A_loss: 0.6629, G_B_loss: 1.2030\n",
      "Epoch [22/30], Step [331/1067], D_A_loss: 0.1827, D_B_loss: 0.2205, G_A_loss: 0.7409, G_B_loss: 0.7284\n",
      "Epoch [22/30], Step [341/1067], D_A_loss: 0.1661, D_B_loss: 0.0515, G_A_loss: 0.4475, G_B_loss: 0.3845\n",
      "Epoch [22/30], Step [351/1067], D_A_loss: 0.0442, D_B_loss: 0.2074, G_A_loss: 0.7710, G_B_loss: 0.3834\n",
      "Epoch [22/30], Step [361/1067], D_A_loss: 0.1391, D_B_loss: 0.0425, G_A_loss: 1.0338, G_B_loss: 0.7230\n",
      "Epoch [22/30], Step [371/1067], D_A_loss: 0.0378, D_B_loss: 0.0558, G_A_loss: 0.6264, G_B_loss: 0.7999\n",
      "Epoch [22/30], Step [381/1067], D_A_loss: 0.0666, D_B_loss: 0.0107, G_A_loss: 1.0419, G_B_loss: 0.2173\n",
      "Epoch [22/30], Step [391/1067], D_A_loss: 0.0866, D_B_loss: 0.0642, G_A_loss: 0.5410, G_B_loss: 0.5117\n",
      "Epoch [22/30], Step [401/1067], D_A_loss: 0.0491, D_B_loss: 0.0461, G_A_loss: 0.8060, G_B_loss: 0.2694\n",
      "Epoch [22/30], Step [411/1067], D_A_loss: 0.1761, D_B_loss: 0.1708, G_A_loss: 0.5502, G_B_loss: 0.3232\n",
      "Epoch [22/30], Step [421/1067], D_A_loss: 0.0750, D_B_loss: 0.3250, G_A_loss: 0.6081, G_B_loss: 0.5006\n",
      "Epoch [22/30], Step [431/1067], D_A_loss: 0.1666, D_B_loss: 0.0861, G_A_loss: 0.4486, G_B_loss: 1.1904\n",
      "Epoch [22/30], Step [441/1067], D_A_loss: 0.1591, D_B_loss: 0.0380, G_A_loss: 0.6965, G_B_loss: 0.8298\n",
      "Epoch [22/30], Step [451/1067], D_A_loss: 0.1304, D_B_loss: 0.0806, G_A_loss: 0.4557, G_B_loss: 0.8065\n",
      "Epoch [22/30], Step [461/1067], D_A_loss: 0.0828, D_B_loss: 0.1386, G_A_loss: 0.3628, G_B_loss: 0.4521\n",
      "Epoch [22/30], Step [471/1067], D_A_loss: 0.1869, D_B_loss: 0.0383, G_A_loss: 0.7135, G_B_loss: 0.6834\n",
      "Epoch [22/30], Step [481/1067], D_A_loss: 0.1187, D_B_loss: 0.0715, G_A_loss: 0.4743, G_B_loss: 0.4404\n",
      "Epoch [22/30], Step [491/1067], D_A_loss: 0.2096, D_B_loss: 0.1130, G_A_loss: 0.3952, G_B_loss: 0.8078\n",
      "Epoch [22/30], Step [501/1067], D_A_loss: 0.0264, D_B_loss: 0.0365, G_A_loss: 0.6363, G_B_loss: 0.9829\n",
      "Epoch [22/30], Step [511/1067], D_A_loss: 0.2157, D_B_loss: 0.0252, G_A_loss: 0.9040, G_B_loss: 0.2237\n",
      "Epoch [22/30], Step [521/1067], D_A_loss: 0.1114, D_B_loss: 0.1370, G_A_loss: 0.5140, G_B_loss: 0.2617\n",
      "Epoch [22/30], Step [531/1067], D_A_loss: 0.1505, D_B_loss: 0.0420, G_A_loss: 0.6125, G_B_loss: 0.3926\n",
      "Epoch [22/30], Step [541/1067], D_A_loss: 0.1811, D_B_loss: 0.0314, G_A_loss: 0.9104, G_B_loss: 0.2814\n",
      "Epoch [22/30], Step [551/1067], D_A_loss: 0.1150, D_B_loss: 0.0339, G_A_loss: 0.6719, G_B_loss: 0.8794\n",
      "Epoch [22/30], Step [561/1067], D_A_loss: 0.0171, D_B_loss: 0.0842, G_A_loss: 0.4831, G_B_loss: 0.1060\n",
      "Epoch [22/30], Step [571/1067], D_A_loss: 0.1443, D_B_loss: 0.1219, G_A_loss: 0.3476, G_B_loss: 0.4306\n",
      "Epoch [22/30], Step [581/1067], D_A_loss: 0.1001, D_B_loss: 0.0645, G_A_loss: 0.9935, G_B_loss: 0.2955\n",
      "Epoch [22/30], Step [591/1067], D_A_loss: 0.1128, D_B_loss: 0.0465, G_A_loss: 0.6770, G_B_loss: 0.3837\n",
      "Epoch [22/30], Step [601/1067], D_A_loss: 0.0709, D_B_loss: 0.0408, G_A_loss: 0.7255, G_B_loss: 0.4892\n",
      "Epoch [22/30], Step [611/1067], D_A_loss: 0.0593, D_B_loss: 0.0177, G_A_loss: 0.4061, G_B_loss: 0.3743\n",
      "Epoch [22/30], Step [621/1067], D_A_loss: 0.1510, D_B_loss: 0.0780, G_A_loss: 0.5927, G_B_loss: 1.2642\n",
      "Epoch [22/30], Step [631/1067], D_A_loss: 0.0404, D_B_loss: 0.0479, G_A_loss: 0.8602, G_B_loss: 0.8657\n",
      "Epoch [22/30], Step [641/1067], D_A_loss: 0.0592, D_B_loss: 0.1565, G_A_loss: 0.4554, G_B_loss: 0.5279\n",
      "Epoch [22/30], Step [651/1067], D_A_loss: 0.1063, D_B_loss: 0.0540, G_A_loss: 0.6792, G_B_loss: 0.4240\n",
      "Epoch [22/30], Step [661/1067], D_A_loss: 0.1765, D_B_loss: 0.1482, G_A_loss: 0.7009, G_B_loss: 0.6556\n",
      "Epoch [22/30], Step [671/1067], D_A_loss: 0.1985, D_B_loss: 0.0685, G_A_loss: 1.2028, G_B_loss: 0.2813\n",
      "Epoch [22/30], Step [681/1067], D_A_loss: 0.0426, D_B_loss: 0.0617, G_A_loss: 0.4680, G_B_loss: 0.6503\n",
      "Epoch [22/30], Step [691/1067], D_A_loss: 0.0676, D_B_loss: 0.0441, G_A_loss: 1.1398, G_B_loss: 0.3979\n",
      "Epoch [22/30], Step [701/1067], D_A_loss: 0.0696, D_B_loss: 0.1381, G_A_loss: 0.2264, G_B_loss: 0.3369\n",
      "Epoch [22/30], Step [711/1067], D_A_loss: 0.1204, D_B_loss: 0.0236, G_A_loss: 0.7844, G_B_loss: 0.7288\n",
      "Epoch [22/30], Step [721/1067], D_A_loss: 0.0746, D_B_loss: 0.0920, G_A_loss: 0.4156, G_B_loss: 0.2647\n",
      "Epoch [22/30], Step [731/1067], D_A_loss: 0.0321, D_B_loss: 0.0539, G_A_loss: 0.5843, G_B_loss: 0.3355\n",
      "Epoch [22/30], Step [741/1067], D_A_loss: 0.0155, D_B_loss: 0.0157, G_A_loss: 0.8644, G_B_loss: 1.0476\n",
      "Epoch [22/30], Step [751/1067], D_A_loss: 0.1278, D_B_loss: 0.0712, G_A_loss: 0.4837, G_B_loss: 0.2920\n",
      "Epoch [22/30], Step [761/1067], D_A_loss: 0.0184, D_B_loss: 0.0342, G_A_loss: 0.6939, G_B_loss: 0.2577\n",
      "Epoch [22/30], Step [771/1067], D_A_loss: 0.0376, D_B_loss: 0.0828, G_A_loss: 1.0522, G_B_loss: 0.7391\n",
      "Epoch [22/30], Step [781/1067], D_A_loss: 0.0825, D_B_loss: 0.0948, G_A_loss: 0.9318, G_B_loss: 0.6094\n",
      "Epoch [22/30], Step [791/1067], D_A_loss: 0.2502, D_B_loss: 0.0433, G_A_loss: 0.6280, G_B_loss: 0.5115\n",
      "Epoch [22/30], Step [801/1067], D_A_loss: 0.5823, D_B_loss: 0.1235, G_A_loss: 0.1287, G_B_loss: 0.9668\n",
      "Epoch [22/30], Step [811/1067], D_A_loss: 0.2024, D_B_loss: 0.0517, G_A_loss: 0.5157, G_B_loss: 0.2640\n",
      "Epoch [22/30], Step [821/1067], D_A_loss: 0.0426, D_B_loss: 0.1370, G_A_loss: 0.9534, G_B_loss: 0.6487\n",
      "Epoch [22/30], Step [831/1067], D_A_loss: 0.0537, D_B_loss: 0.0813, G_A_loss: 0.2248, G_B_loss: 0.6464\n",
      "Epoch [22/30], Step [841/1067], D_A_loss: 0.1565, D_B_loss: 0.0697, G_A_loss: 0.5423, G_B_loss: 0.2617\n",
      "Epoch [22/30], Step [851/1067], D_A_loss: 0.0927, D_B_loss: 0.0602, G_A_loss: 0.7031, G_B_loss: 0.5586\n",
      "Epoch [22/30], Step [861/1067], D_A_loss: 0.0829, D_B_loss: 0.0928, G_A_loss: 0.3914, G_B_loss: 0.6227\n",
      "Epoch [22/30], Step [871/1067], D_A_loss: 0.1079, D_B_loss: 0.0941, G_A_loss: 0.3757, G_B_loss: 0.7225\n",
      "Epoch [22/30], Step [881/1067], D_A_loss: 0.1646, D_B_loss: 0.0501, G_A_loss: 0.6964, G_B_loss: 0.7574\n",
      "Epoch [22/30], Step [891/1067], D_A_loss: 0.0479, D_B_loss: 0.1120, G_A_loss: 0.7628, G_B_loss: 0.6429\n",
      "Epoch [22/30], Step [901/1067], D_A_loss: 0.0461, D_B_loss: 0.0607, G_A_loss: 0.6616, G_B_loss: 0.6125\n",
      "Epoch [22/30], Step [911/1067], D_A_loss: 0.0580, D_B_loss: 0.0283, G_A_loss: 0.9134, G_B_loss: 0.1316\n",
      "Epoch [22/30], Step [921/1067], D_A_loss: 0.2063, D_B_loss: 0.0956, G_A_loss: 0.3826, G_B_loss: 0.3420\n",
      "Epoch [22/30], Step [931/1067], D_A_loss: 0.0887, D_B_loss: 0.0367, G_A_loss: 0.6851, G_B_loss: 0.5027\n",
      "Epoch [22/30], Step [941/1067], D_A_loss: 0.0530, D_B_loss: 0.0500, G_A_loss: 0.6896, G_B_loss: 0.7065\n",
      "Epoch [22/30], Step [951/1067], D_A_loss: 0.1496, D_B_loss: 0.0308, G_A_loss: 0.6691, G_B_loss: 0.6047\n",
      "Epoch [22/30], Step [961/1067], D_A_loss: 0.0275, D_B_loss: 0.0699, G_A_loss: 0.5547, G_B_loss: 0.8633\n",
      "Epoch [22/30], Step [971/1067], D_A_loss: 0.0548, D_B_loss: 0.0756, G_A_loss: 0.5778, G_B_loss: 0.5583\n",
      "Epoch [22/30], Step [981/1067], D_A_loss: 0.0224, D_B_loss: 0.0259, G_A_loss: 0.5765, G_B_loss: 0.8842\n",
      "Epoch [22/30], Step [991/1067], D_A_loss: 0.2049, D_B_loss: 0.1745, G_A_loss: 0.3199, G_B_loss: 0.2261\n",
      "Epoch [22/30], Step [1001/1067], D_A_loss: 0.1810, D_B_loss: 0.1398, G_A_loss: 0.4249, G_B_loss: 0.3551\n",
      "Epoch [22/30], Step [1011/1067], D_A_loss: 0.0724, D_B_loss: 0.0689, G_A_loss: 0.4674, G_B_loss: 0.6472\n",
      "Epoch [22/30], Step [1021/1067], D_A_loss: 0.0268, D_B_loss: 0.0172, G_A_loss: 0.5368, G_B_loss: 0.8469\n",
      "Epoch [22/30], Step [1031/1067], D_A_loss: 0.1188, D_B_loss: 0.0543, G_A_loss: 0.5811, G_B_loss: 0.3211\n",
      "Epoch [22/30], Step [1041/1067], D_A_loss: 0.0950, D_B_loss: 0.1998, G_A_loss: 0.8811, G_B_loss: 0.4905\n",
      "Epoch [22/30], Step [1051/1067], D_A_loss: 0.0805, D_B_loss: 0.0401, G_A_loss: 0.8400, G_B_loss: 0.6037\n",
      "Epoch [22/30], Step [1061/1067], D_A_loss: 0.1235, D_B_loss: 0.0649, G_A_loss: 0.4578, G_B_loss: 0.8217\n",
      "Epoch [23/30], Step [1/1067], D_A_loss: 0.1167, D_B_loss: 0.0323, G_A_loss: 0.7254, G_B_loss: 0.4996\n",
      "Epoch [23/30], Step [11/1067], D_A_loss: 0.0545, D_B_loss: 0.0274, G_A_loss: 0.4849, G_B_loss: 0.5720\n",
      "Epoch [23/30], Step [21/1067], D_A_loss: 0.0567, D_B_loss: 0.0491, G_A_loss: 1.2074, G_B_loss: 1.0417\n",
      "Epoch [23/30], Step [31/1067], D_A_loss: 0.0903, D_B_loss: 0.1103, G_A_loss: 0.3911, G_B_loss: 0.6802\n",
      "Epoch [23/30], Step [41/1067], D_A_loss: 0.0940, D_B_loss: 0.0858, G_A_loss: 0.4588, G_B_loss: 0.4262\n",
      "Epoch [23/30], Step [51/1067], D_A_loss: 0.1183, D_B_loss: 0.1179, G_A_loss: 0.2415, G_B_loss: 0.7688\n",
      "Epoch [23/30], Step [61/1067], D_A_loss: 0.2899, D_B_loss: 0.0876, G_A_loss: 0.7092, G_B_loss: 1.2504\n",
      "Epoch [23/30], Step [71/1067], D_A_loss: 0.0628, D_B_loss: 0.0677, G_A_loss: 0.8558, G_B_loss: 0.5692\n",
      "Epoch [23/30], Step [81/1067], D_A_loss: 0.1027, D_B_loss: 0.0408, G_A_loss: 0.4008, G_B_loss: 0.3580\n",
      "Epoch [23/30], Step [91/1067], D_A_loss: 0.0397, D_B_loss: 0.0865, G_A_loss: 0.4372, G_B_loss: 0.6769\n",
      "Epoch [23/30], Step [101/1067], D_A_loss: 0.0275, D_B_loss: 0.0474, G_A_loss: 0.5478, G_B_loss: 0.7911\n",
      "Epoch [23/30], Step [111/1067], D_A_loss: 0.0324, D_B_loss: 0.1101, G_A_loss: 0.9715, G_B_loss: 0.2676\n",
      "Epoch [23/30], Step [121/1067], D_A_loss: 0.0548, D_B_loss: 0.0186, G_A_loss: 0.8137, G_B_loss: 0.8065\n",
      "Epoch [23/30], Step [131/1067], D_A_loss: 0.0953, D_B_loss: 0.2158, G_A_loss: 0.9840, G_B_loss: 0.4648\n",
      "Epoch [23/30], Step [141/1067], D_A_loss: 0.1253, D_B_loss: 0.0266, G_A_loss: 0.8535, G_B_loss: 0.4987\n",
      "Epoch [23/30], Step [151/1067], D_A_loss: 0.0274, D_B_loss: 0.0465, G_A_loss: 0.7534, G_B_loss: 0.5385\n",
      "Epoch [23/30], Step [161/1067], D_A_loss: 0.0569, D_B_loss: 0.1016, G_A_loss: 0.8348, G_B_loss: 0.4658\n",
      "Epoch [23/30], Step [171/1067], D_A_loss: 0.1046, D_B_loss: 0.0477, G_A_loss: 0.9869, G_B_loss: 0.9492\n",
      "Epoch [23/30], Step [181/1067], D_A_loss: 0.0501, D_B_loss: 0.0317, G_A_loss: 0.6850, G_B_loss: 0.9105\n",
      "Epoch [23/30], Step [191/1067], D_A_loss: 0.2286, D_B_loss: 0.0595, G_A_loss: 0.6938, G_B_loss: 0.7441\n",
      "Epoch [23/30], Step [201/1067], D_A_loss: 0.0855, D_B_loss: 0.0804, G_A_loss: 0.4624, G_B_loss: 0.5919\n",
      "Epoch [23/30], Step [211/1067], D_A_loss: 0.0725, D_B_loss: 0.1198, G_A_loss: 0.3181, G_B_loss: 0.3273\n",
      "Epoch [23/30], Step [221/1067], D_A_loss: 0.0468, D_B_loss: 0.0173, G_A_loss: 0.4281, G_B_loss: 0.7512\n",
      "Epoch [23/30], Step [231/1067], D_A_loss: 0.0668, D_B_loss: 0.0687, G_A_loss: 0.5175, G_B_loss: 0.6551\n",
      "Epoch [23/30], Step [241/1067], D_A_loss: 0.1758, D_B_loss: 0.0297, G_A_loss: 0.4928, G_B_loss: 0.4084\n",
      "Epoch [23/30], Step [251/1067], D_A_loss: 0.1087, D_B_loss: 0.1028, G_A_loss: 0.4574, G_B_loss: 0.7000\n",
      "Epoch [23/30], Step [261/1067], D_A_loss: 0.0806, D_B_loss: 0.1535, G_A_loss: 0.7613, G_B_loss: 0.6387\n",
      "Epoch [23/30], Step [271/1067], D_A_loss: 0.2114, D_B_loss: 0.0638, G_A_loss: 0.5570, G_B_loss: 0.2805\n",
      "Epoch [23/30], Step [281/1067], D_A_loss: 0.0544, D_B_loss: 0.0755, G_A_loss: 0.4385, G_B_loss: 0.6794\n",
      "Epoch [23/30], Step [291/1067], D_A_loss: 0.0272, D_B_loss: 0.1033, G_A_loss: 0.5838, G_B_loss: 0.8029\n",
      "Epoch [23/30], Step [301/1067], D_A_loss: 0.1185, D_B_loss: 0.0302, G_A_loss: 0.8177, G_B_loss: 0.4172\n",
      "Epoch [23/30], Step [311/1067], D_A_loss: 0.0863, D_B_loss: 0.0635, G_A_loss: 0.5190, G_B_loss: 0.5086\n",
      "Epoch [23/30], Step [321/1067], D_A_loss: 0.0480, D_B_loss: 0.0201, G_A_loss: 1.0214, G_B_loss: 0.4683\n",
      "Epoch [23/30], Step [331/1067], D_A_loss: 0.1212, D_B_loss: 0.0233, G_A_loss: 1.1134, G_B_loss: 0.8700\n",
      "Epoch [23/30], Step [341/1067], D_A_loss: 0.0259, D_B_loss: 0.0331, G_A_loss: 0.8858, G_B_loss: 0.6119\n",
      "Epoch [23/30], Step [351/1067], D_A_loss: 0.0820, D_B_loss: 0.0163, G_A_loss: 1.0138, G_B_loss: 0.9511\n",
      "Epoch [23/30], Step [361/1067], D_A_loss: 0.0502, D_B_loss: 0.0458, G_A_loss: 1.4063, G_B_loss: 0.5098\n",
      "Epoch [23/30], Step [371/1067], D_A_loss: 0.0452, D_B_loss: 0.0604, G_A_loss: 0.6594, G_B_loss: 0.7429\n",
      "Epoch [23/30], Step [381/1067], D_A_loss: 0.2217, D_B_loss: 0.0769, G_A_loss: 0.3621, G_B_loss: 0.1762\n",
      "Epoch [23/30], Step [391/1067], D_A_loss: 0.1017, D_B_loss: 0.0552, G_A_loss: 0.6615, G_B_loss: 0.3960\n",
      "Epoch [23/30], Step [401/1067], D_A_loss: 0.1613, D_B_loss: 0.0755, G_A_loss: 0.4728, G_B_loss: 0.3195\n",
      "Epoch [23/30], Step [411/1067], D_A_loss: 0.0721, D_B_loss: 0.1089, G_A_loss: 0.5323, G_B_loss: 0.4631\n",
      "Epoch [23/30], Step [421/1067], D_A_loss: 0.0477, D_B_loss: 0.0844, G_A_loss: 0.6644, G_B_loss: 0.7938\n",
      "Epoch [23/30], Step [431/1067], D_A_loss: 0.1915, D_B_loss: 0.1458, G_A_loss: 1.0101, G_B_loss: 0.2186\n",
      "Epoch [23/30], Step [441/1067], D_A_loss: 0.0898, D_B_loss: 0.0275, G_A_loss: 0.9364, G_B_loss: 0.4940\n",
      "Epoch [23/30], Step [451/1067], D_A_loss: 0.0980, D_B_loss: 0.2221, G_A_loss: 0.2798, G_B_loss: 0.3933\n",
      "Epoch [23/30], Step [461/1067], D_A_loss: 0.0294, D_B_loss: 0.0162, G_A_loss: 0.8959, G_B_loss: 0.4658\n",
      "Epoch [23/30], Step [471/1067], D_A_loss: 0.0637, D_B_loss: 0.0489, G_A_loss: 0.5926, G_B_loss: 0.7103\n",
      "Epoch [23/30], Step [481/1067], D_A_loss: 0.0774, D_B_loss: 0.0356, G_A_loss: 0.9363, G_B_loss: 0.8651\n",
      "Epoch [23/30], Step [491/1067], D_A_loss: 0.0364, D_B_loss: 0.1043, G_A_loss: 0.8184, G_B_loss: 0.4320\n",
      "Epoch [23/30], Step [501/1067], D_A_loss: 0.0384, D_B_loss: 0.2450, G_A_loss: 0.1495, G_B_loss: 0.4785\n",
      "Epoch [23/30], Step [511/1067], D_A_loss: 0.0694, D_B_loss: 0.0134, G_A_loss: 0.9031, G_B_loss: 0.7561\n",
      "Epoch [23/30], Step [521/1067], D_A_loss: 0.1552, D_B_loss: 0.1418, G_A_loss: 0.9679, G_B_loss: 0.2663\n",
      "Epoch [23/30], Step [531/1067], D_A_loss: 0.0543, D_B_loss: 0.0667, G_A_loss: 0.7490, G_B_loss: 0.5156\n",
      "Epoch [23/30], Step [541/1067], D_A_loss: 0.0599, D_B_loss: 0.0545, G_A_loss: 0.3713, G_B_loss: 0.7684\n",
      "Epoch [23/30], Step [551/1067], D_A_loss: 0.0759, D_B_loss: 0.0313, G_A_loss: 0.6666, G_B_loss: 1.0049\n",
      "Epoch [23/30], Step [561/1067], D_A_loss: 0.2015, D_B_loss: 0.0794, G_A_loss: 1.2727, G_B_loss: 0.2268\n",
      "Epoch [23/30], Step [571/1067], D_A_loss: 0.2878, D_B_loss: 0.0478, G_A_loss: 0.5748, G_B_loss: 0.1557\n",
      "Epoch [23/30], Step [581/1067], D_A_loss: 0.3104, D_B_loss: 0.1211, G_A_loss: 0.5232, G_B_loss: 0.1429\n",
      "Epoch [23/30], Step [591/1067], D_A_loss: 0.0566, D_B_loss: 0.0341, G_A_loss: 1.4036, G_B_loss: 1.0769\n",
      "Epoch [23/30], Step [601/1067], D_A_loss: 0.0764, D_B_loss: 0.0227, G_A_loss: 0.3843, G_B_loss: 0.5925\n",
      "Epoch [23/30], Step [611/1067], D_A_loss: 0.0336, D_B_loss: 0.0545, G_A_loss: 0.5657, G_B_loss: 0.5953\n",
      "Epoch [23/30], Step [621/1067], D_A_loss: 0.1508, D_B_loss: 0.0338, G_A_loss: 0.6202, G_B_loss: 0.2715\n",
      "Epoch [23/30], Step [631/1067], D_A_loss: 0.1348, D_B_loss: 0.0832, G_A_loss: 0.5228, G_B_loss: 0.6238\n",
      "Epoch [23/30], Step [641/1067], D_A_loss: 0.2087, D_B_loss: 0.0310, G_A_loss: 0.7438, G_B_loss: 0.3740\n",
      "Epoch [23/30], Step [651/1067], D_A_loss: 0.0817, D_B_loss: 0.0279, G_A_loss: 0.8748, G_B_loss: 0.3199\n",
      "Epoch [23/30], Step [661/1067], D_A_loss: 0.0667, D_B_loss: 0.1989, G_A_loss: 0.8490, G_B_loss: 0.5656\n",
      "Epoch [23/30], Step [671/1067], D_A_loss: 0.1366, D_B_loss: 0.0532, G_A_loss: 0.6579, G_B_loss: 0.5549\n",
      "Epoch [23/30], Step [681/1067], D_A_loss: 0.1601, D_B_loss: 0.0883, G_A_loss: 0.4085, G_B_loss: 0.6863\n",
      "Epoch [23/30], Step [691/1067], D_A_loss: 0.0367, D_B_loss: 0.0208, G_A_loss: 0.4647, G_B_loss: 0.3920\n",
      "Epoch [23/30], Step [701/1067], D_A_loss: 0.0339, D_B_loss: 0.0547, G_A_loss: 0.6205, G_B_loss: 0.7761\n",
      "Epoch [23/30], Step [711/1067], D_A_loss: 0.0422, D_B_loss: 0.1714, G_A_loss: 0.2504, G_B_loss: 0.7131\n",
      "Epoch [23/30], Step [721/1067], D_A_loss: 0.0681, D_B_loss: 0.0358, G_A_loss: 0.6436, G_B_loss: 0.5707\n",
      "Epoch [23/30], Step [731/1067], D_A_loss: 0.0638, D_B_loss: 0.1094, G_A_loss: 0.3296, G_B_loss: 0.2729\n",
      "Epoch [23/30], Step [741/1067], D_A_loss: 0.2482, D_B_loss: 0.0592, G_A_loss: 0.5771, G_B_loss: 0.3257\n",
      "Epoch [23/30], Step [751/1067], D_A_loss: 0.0975, D_B_loss: 0.0711, G_A_loss: 0.6819, G_B_loss: 0.5986\n",
      "Epoch [23/30], Step [761/1067], D_A_loss: 0.0402, D_B_loss: 0.1044, G_A_loss: 0.3413, G_B_loss: 0.2981\n",
      "Epoch [23/30], Step [771/1067], D_A_loss: 0.1275, D_B_loss: 0.0311, G_A_loss: 0.8136, G_B_loss: 0.3590\n",
      "Epoch [23/30], Step [781/1067], D_A_loss: 0.1016, D_B_loss: 0.0526, G_A_loss: 0.4172, G_B_loss: 0.4168\n",
      "Epoch [23/30], Step [791/1067], D_A_loss: 0.1281, D_B_loss: 0.0380, G_A_loss: 0.6464, G_B_loss: 0.2929\n",
      "Epoch [23/30], Step [801/1067], D_A_loss: 0.2190, D_B_loss: 0.1455, G_A_loss: 0.6204, G_B_loss: 0.8336\n",
      "Epoch [23/30], Step [811/1067], D_A_loss: 0.0940, D_B_loss: 0.0335, G_A_loss: 0.6546, G_B_loss: 0.6660\n",
      "Epoch [23/30], Step [821/1067], D_A_loss: 0.2038, D_B_loss: 0.2483, G_A_loss: 0.2070, G_B_loss: 0.2737\n",
      "Epoch [23/30], Step [831/1067], D_A_loss: 0.0938, D_B_loss: 0.0641, G_A_loss: 0.2678, G_B_loss: 0.6203\n",
      "Epoch [23/30], Step [841/1067], D_A_loss: 0.0702, D_B_loss: 0.0579, G_A_loss: 0.9215, G_B_loss: 0.6776\n",
      "Epoch [23/30], Step [851/1067], D_A_loss: 0.0585, D_B_loss: 0.0154, G_A_loss: 0.6294, G_B_loss: 1.1063\n",
      "Epoch [23/30], Step [861/1067], D_A_loss: 0.2238, D_B_loss: 0.1534, G_A_loss: 0.6517, G_B_loss: 0.2511\n",
      "Epoch [23/30], Step [871/1067], D_A_loss: 0.2284, D_B_loss: 0.0896, G_A_loss: 1.2254, G_B_loss: 0.4771\n",
      "Epoch [23/30], Step [881/1067], D_A_loss: 0.1055, D_B_loss: 0.0795, G_A_loss: 1.0192, G_B_loss: 0.3729\n",
      "Epoch [23/30], Step [891/1067], D_A_loss: 0.1428, D_B_loss: 0.1282, G_A_loss: 0.5230, G_B_loss: 0.1720\n",
      "Epoch [23/30], Step [901/1067], D_A_loss: 0.0647, D_B_loss: 0.0519, G_A_loss: 0.6035, G_B_loss: 0.5258\n",
      "Epoch [23/30], Step [911/1067], D_A_loss: 0.1155, D_B_loss: 0.0476, G_A_loss: 0.7077, G_B_loss: 0.3928\n",
      "Epoch [23/30], Step [921/1067], D_A_loss: 0.0981, D_B_loss: 0.0360, G_A_loss: 0.6410, G_B_loss: 0.6226\n",
      "Epoch [23/30], Step [931/1067], D_A_loss: 0.2192, D_B_loss: 0.0465, G_A_loss: 0.6654, G_B_loss: 0.4155\n",
      "Epoch [23/30], Step [941/1067], D_A_loss: 0.0887, D_B_loss: 0.2481, G_A_loss: 0.1661, G_B_loss: 0.5666\n",
      "Epoch [23/30], Step [951/1067], D_A_loss: 0.1812, D_B_loss: 0.1382, G_A_loss: 0.5140, G_B_loss: 0.2229\n",
      "Epoch [23/30], Step [961/1067], D_A_loss: 0.0574, D_B_loss: 0.0725, G_A_loss: 0.8957, G_B_loss: 0.4773\n",
      "Epoch [23/30], Step [971/1067], D_A_loss: 0.0921, D_B_loss: 0.2383, G_A_loss: 0.6015, G_B_loss: 0.4209\n",
      "Epoch [23/30], Step [981/1067], D_A_loss: 0.1671, D_B_loss: 0.0741, G_A_loss: 0.6017, G_B_loss: 0.4878\n",
      "Epoch [23/30], Step [991/1067], D_A_loss: 0.1279, D_B_loss: 0.0605, G_A_loss: 0.9521, G_B_loss: 0.3371\n",
      "Epoch [23/30], Step [1001/1067], D_A_loss: 0.0909, D_B_loss: 0.0304, G_A_loss: 0.7021, G_B_loss: 0.4304\n",
      "Epoch [23/30], Step [1011/1067], D_A_loss: 0.1546, D_B_loss: 0.0214, G_A_loss: 0.8066, G_B_loss: 0.3711\n",
      "Epoch [23/30], Step [1021/1067], D_A_loss: 0.1731, D_B_loss: 0.0131, G_A_loss: 0.6110, G_B_loss: 0.5420\n",
      "Epoch [23/30], Step [1031/1067], D_A_loss: 0.0452, D_B_loss: 0.1883, G_A_loss: 0.2087, G_B_loss: 0.4657\n",
      "Epoch [23/30], Step [1041/1067], D_A_loss: 0.0314, D_B_loss: 0.1365, G_A_loss: 1.3362, G_B_loss: 0.5483\n",
      "Epoch [23/30], Step [1051/1067], D_A_loss: 0.2257, D_B_loss: 0.0437, G_A_loss: 0.6733, G_B_loss: 0.2541\n",
      "Epoch [23/30], Step [1061/1067], D_A_loss: 0.0308, D_B_loss: 0.0724, G_A_loss: 0.6242, G_B_loss: 0.7645\n",
      "Epoch [24/30], Step [1/1067], D_A_loss: 0.0270, D_B_loss: 0.0187, G_A_loss: 0.4258, G_B_loss: 0.7460\n",
      "Epoch [24/30], Step [11/1067], D_A_loss: 0.0528, D_B_loss: 0.0536, G_A_loss: 0.5602, G_B_loss: 0.9413\n",
      "Epoch [24/30], Step [21/1067], D_A_loss: 0.0839, D_B_loss: 0.0289, G_A_loss: 0.8747, G_B_loss: 0.5479\n",
      "Epoch [24/30], Step [31/1067], D_A_loss: 0.0880, D_B_loss: 0.0214, G_A_loss: 0.4755, G_B_loss: 0.7372\n",
      "Epoch [24/30], Step [41/1067], D_A_loss: 0.0431, D_B_loss: 0.0371, G_A_loss: 1.3289, G_B_loss: 0.4700\n",
      "Epoch [24/30], Step [51/1067], D_A_loss: 0.0852, D_B_loss: 0.1158, G_A_loss: 0.4995, G_B_loss: 0.7090\n",
      "Epoch [24/30], Step [61/1067], D_A_loss: 0.1019, D_B_loss: 0.0339, G_A_loss: 0.6394, G_B_loss: 0.6596\n",
      "Epoch [24/30], Step [71/1067], D_A_loss: 0.0959, D_B_loss: 0.0894, G_A_loss: 0.4615, G_B_loss: 0.4514\n",
      "Epoch [24/30], Step [81/1067], D_A_loss: 0.2099, D_B_loss: 0.0407, G_A_loss: 0.7432, G_B_loss: 0.1897\n",
      "Epoch [24/30], Step [91/1067], D_A_loss: 0.1677, D_B_loss: 0.0718, G_A_loss: 0.5274, G_B_loss: 0.9080\n",
      "Epoch [24/30], Step [101/1067], D_A_loss: 0.1021, D_B_loss: 0.0664, G_A_loss: 0.7313, G_B_loss: 0.3230\n",
      "Epoch [24/30], Step [111/1067], D_A_loss: 0.0921, D_B_loss: 0.0253, G_A_loss: 0.7907, G_B_loss: 0.4677\n",
      "Epoch [24/30], Step [121/1067], D_A_loss: 0.0909, D_B_loss: 0.0425, G_A_loss: 0.8592, G_B_loss: 0.6119\n",
      "Epoch [24/30], Step [131/1067], D_A_loss: 0.0330, D_B_loss: 0.0380, G_A_loss: 0.8748, G_B_loss: 0.3540\n",
      "Epoch [24/30], Step [141/1067], D_A_loss: 0.0909, D_B_loss: 0.1259, G_A_loss: 0.8803, G_B_loss: 0.8585\n",
      "Epoch [24/30], Step [151/1067], D_A_loss: 0.1038, D_B_loss: 0.0546, G_A_loss: 1.1723, G_B_loss: 0.3860\n",
      "Epoch [24/30], Step [161/1067], D_A_loss: 0.0415, D_B_loss: 0.0519, G_A_loss: 0.4904, G_B_loss: 0.5857\n",
      "Epoch [24/30], Step [171/1067], D_A_loss: 0.0488, D_B_loss: 0.0313, G_A_loss: 0.3830, G_B_loss: 0.5635\n",
      "Epoch [24/30], Step [181/1067], D_A_loss: 0.0612, D_B_loss: 0.1080, G_A_loss: 0.3557, G_B_loss: 0.3592\n",
      "Epoch [24/30], Step [191/1067], D_A_loss: 0.1426, D_B_loss: 0.0427, G_A_loss: 0.9134, G_B_loss: 0.3803\n",
      "Epoch [24/30], Step [201/1067], D_A_loss: 0.1230, D_B_loss: 0.0976, G_A_loss: 0.3914, G_B_loss: 1.0067\n",
      "Epoch [24/30], Step [211/1067], D_A_loss: 0.1139, D_B_loss: 0.0509, G_A_loss: 0.8907, G_B_loss: 0.3275\n",
      "Epoch [24/30], Step [221/1067], D_A_loss: 0.0444, D_B_loss: 0.3513, G_A_loss: 0.1146, G_B_loss: 0.5659\n",
      "Epoch [24/30], Step [231/1067], D_A_loss: 0.0492, D_B_loss: 0.1381, G_A_loss: 0.3091, G_B_loss: 0.5347\n",
      "Epoch [24/30], Step [241/1067], D_A_loss: 0.0492, D_B_loss: 0.1926, G_A_loss: 1.0593, G_B_loss: 0.2506\n",
      "Epoch [24/30], Step [251/1067], D_A_loss: 0.0638, D_B_loss: 0.0356, G_A_loss: 0.7364, G_B_loss: 0.5033\n",
      "Epoch [24/30], Step [261/1067], D_A_loss: 0.0497, D_B_loss: 0.0151, G_A_loss: 0.7330, G_B_loss: 0.4460\n",
      "Epoch [24/30], Step [271/1067], D_A_loss: 0.1156, D_B_loss: 0.0267, G_A_loss: 0.7359, G_B_loss: 0.3093\n",
      "Epoch [24/30], Step [281/1067], D_A_loss: 0.1417, D_B_loss: 0.0694, G_A_loss: 0.9412, G_B_loss: 0.2878\n",
      "Epoch [24/30], Step [291/1067], D_A_loss: 0.1415, D_B_loss: 0.0727, G_A_loss: 0.3872, G_B_loss: 0.8010\n",
      "Epoch [24/30], Step [301/1067], D_A_loss: 0.0299, D_B_loss: 0.0622, G_A_loss: 0.7374, G_B_loss: 0.3264\n",
      "Epoch [24/30], Step [311/1067], D_A_loss: 0.1060, D_B_loss: 0.0322, G_A_loss: 0.2229, G_B_loss: 0.4137\n",
      "Epoch [24/30], Step [321/1067], D_A_loss: 0.1350, D_B_loss: 0.0521, G_A_loss: 0.9358, G_B_loss: 0.2809\n",
      "Epoch [24/30], Step [331/1067], D_A_loss: 0.0905, D_B_loss: 0.1983, G_A_loss: 0.1932, G_B_loss: 0.4112\n",
      "Epoch [24/30], Step [341/1067], D_A_loss: 0.0660, D_B_loss: 0.0360, G_A_loss: 0.9097, G_B_loss: 0.5204\n",
      "Epoch [24/30], Step [351/1067], D_A_loss: 0.0278, D_B_loss: 0.0693, G_A_loss: 0.5016, G_B_loss: 0.4547\n",
      "Epoch [24/30], Step [361/1067], D_A_loss: 0.0215, D_B_loss: 0.0738, G_A_loss: 0.5228, G_B_loss: 0.9767\n",
      "Epoch [24/30], Step [371/1067], D_A_loss: 0.1035, D_B_loss: 0.0530, G_A_loss: 0.6262, G_B_loss: 1.0766\n",
      "Epoch [24/30], Step [381/1067], D_A_loss: 0.0981, D_B_loss: 0.0476, G_A_loss: 0.7086, G_B_loss: 0.7611\n",
      "Epoch [24/30], Step [391/1067], D_A_loss: 0.0576, D_B_loss: 0.0436, G_A_loss: 0.5033, G_B_loss: 0.1646\n",
      "Epoch [24/30], Step [401/1067], D_A_loss: 0.1599, D_B_loss: 0.1480, G_A_loss: 0.9121, G_B_loss: 0.2568\n",
      "Epoch [24/30], Step [411/1067], D_A_loss: 0.0830, D_B_loss: 0.0560, G_A_loss: 0.5209, G_B_loss: 0.3993\n",
      "Epoch [24/30], Step [421/1067], D_A_loss: 0.2137, D_B_loss: 0.0757, G_A_loss: 0.4128, G_B_loss: 0.7658\n",
      "Epoch [24/30], Step [431/1067], D_A_loss: 0.3352, D_B_loss: 0.0404, G_A_loss: 0.8177, G_B_loss: 0.0855\n",
      "Epoch [24/30], Step [441/1067], D_A_loss: 0.1455, D_B_loss: 0.1612, G_A_loss: 0.3160, G_B_loss: 0.4183\n",
      "Epoch [24/30], Step [451/1067], D_A_loss: 0.0636, D_B_loss: 0.0240, G_A_loss: 0.9109, G_B_loss: 0.6004\n",
      "Epoch [24/30], Step [461/1067], D_A_loss: 0.0344, D_B_loss: 0.0445, G_A_loss: 0.9629, G_B_loss: 0.5424\n",
      "Epoch [24/30], Step [471/1067], D_A_loss: 0.1701, D_B_loss: 0.0464, G_A_loss: 0.4262, G_B_loss: 0.5622\n",
      "Epoch [24/30], Step [481/1067], D_A_loss: 0.0416, D_B_loss: 0.0614, G_A_loss: 0.5028, G_B_loss: 0.6289\n",
      "Epoch [24/30], Step [491/1067], D_A_loss: 0.0402, D_B_loss: 0.1438, G_A_loss: 0.4030, G_B_loss: 0.5351\n",
      "Epoch [24/30], Step [501/1067], D_A_loss: 0.0749, D_B_loss: 0.0381, G_A_loss: 0.9255, G_B_loss: 0.6275\n",
      "Epoch [24/30], Step [511/1067], D_A_loss: 0.1118, D_B_loss: 0.0577, G_A_loss: 0.3662, G_B_loss: 0.4778\n",
      "Epoch [24/30], Step [521/1067], D_A_loss: 0.1138, D_B_loss: 0.0881, G_A_loss: 0.1306, G_B_loss: 0.4524\n",
      "Epoch [24/30], Step [531/1067], D_A_loss: 0.0397, D_B_loss: 0.1239, G_A_loss: 0.4569, G_B_loss: 0.6447\n",
      "Epoch [24/30], Step [541/1067], D_A_loss: 0.0873, D_B_loss: 0.0861, G_A_loss: 0.4678, G_B_loss: 0.4515\n",
      "Epoch [24/30], Step [551/1067], D_A_loss: 0.1563, D_B_loss: 0.0572, G_A_loss: 0.5466, G_B_loss: 1.3903\n",
      "Epoch [24/30], Step [561/1067], D_A_loss: 0.3742, D_B_loss: 0.1508, G_A_loss: 0.5239, G_B_loss: 0.4375\n",
      "Epoch [24/30], Step [571/1067], D_A_loss: 0.0598, D_B_loss: 0.0288, G_A_loss: 0.6608, G_B_loss: 0.3186\n",
      "Epoch [24/30], Step [581/1067], D_A_loss: 0.1137, D_B_loss: 0.0983, G_A_loss: 0.8407, G_B_loss: 0.4902\n",
      "Epoch [24/30], Step [591/1067], D_A_loss: 0.0351, D_B_loss: 0.0464, G_A_loss: 0.6609, G_B_loss: 1.2908\n",
      "Epoch [24/30], Step [601/1067], D_A_loss: 0.0559, D_B_loss: 0.0207, G_A_loss: 0.6346, G_B_loss: 0.4848\n",
      "Epoch [24/30], Step [611/1067], D_A_loss: 0.0204, D_B_loss: 0.0271, G_A_loss: 0.6980, G_B_loss: 0.6845\n",
      "Epoch [24/30], Step [621/1067], D_A_loss: 0.1527, D_B_loss: 0.1814, G_A_loss: 0.2413, G_B_loss: 0.3191\n",
      "Epoch [24/30], Step [631/1067], D_A_loss: 0.0515, D_B_loss: 0.0260, G_A_loss: 0.6559, G_B_loss: 0.7188\n",
      "Epoch [24/30], Step [641/1067], D_A_loss: 0.2812, D_B_loss: 0.0857, G_A_loss: 0.8517, G_B_loss: 0.1893\n",
      "Epoch [24/30], Step [651/1067], D_A_loss: 0.0614, D_B_loss: 0.0214, G_A_loss: 0.9253, G_B_loss: 0.8310\n",
      "Epoch [24/30], Step [661/1067], D_A_loss: 0.1061, D_B_loss: 0.0161, G_A_loss: 0.9021, G_B_loss: 0.7763\n",
      "Epoch [24/30], Step [671/1067], D_A_loss: 0.0505, D_B_loss: 0.0329, G_A_loss: 0.8607, G_B_loss: 0.7954\n",
      "Epoch [24/30], Step [681/1067], D_A_loss: 0.2280, D_B_loss: 0.0322, G_A_loss: 0.6741, G_B_loss: 0.2610\n",
      "Epoch [24/30], Step [691/1067], D_A_loss: 0.0518, D_B_loss: 0.0631, G_A_loss: 0.5187, G_B_loss: 0.5956\n",
      "Epoch [24/30], Step [701/1067], D_A_loss: 0.1105, D_B_loss: 0.1321, G_A_loss: 0.8055, G_B_loss: 0.5152\n",
      "Epoch [24/30], Step [711/1067], D_A_loss: 0.1071, D_B_loss: 0.0958, G_A_loss: 0.3785, G_B_loss: 0.2163\n",
      "Epoch [24/30], Step [721/1067], D_A_loss: 0.1616, D_B_loss: 0.0984, G_A_loss: 0.8889, G_B_loss: 0.2444\n",
      "Epoch [24/30], Step [731/1067], D_A_loss: 0.2758, D_B_loss: 0.0801, G_A_loss: 0.7214, G_B_loss: 0.2196\n",
      "Epoch [24/30], Step [741/1067], D_A_loss: 0.0229, D_B_loss: 0.0950, G_A_loss: 0.9286, G_B_loss: 0.6657\n",
      "Epoch [24/30], Step [751/1067], D_A_loss: 0.0271, D_B_loss: 0.1022, G_A_loss: 0.3729, G_B_loss: 0.9228\n",
      "Epoch [24/30], Step [761/1067], D_A_loss: 0.0237, D_B_loss: 0.0207, G_A_loss: 0.3644, G_B_loss: 0.5317\n",
      "Epoch [24/30], Step [771/1067], D_A_loss: 0.0308, D_B_loss: 0.2099, G_A_loss: 0.2609, G_B_loss: 0.6060\n",
      "Epoch [24/30], Step [781/1067], D_A_loss: 0.0510, D_B_loss: 0.0816, G_A_loss: 0.5538, G_B_loss: 0.3059\n",
      "Epoch [24/30], Step [791/1067], D_A_loss: 0.0162, D_B_loss: 0.0353, G_A_loss: 0.7247, G_B_loss: 0.6684\n",
      "Epoch [24/30], Step [801/1067], D_A_loss: 0.1245, D_B_loss: 0.0422, G_A_loss: 0.4428, G_B_loss: 0.3418\n",
      "Epoch [24/30], Step [811/1067], D_A_loss: 0.0312, D_B_loss: 0.0863, G_A_loss: 0.6181, G_B_loss: 0.5486\n",
      "Epoch [24/30], Step [821/1067], D_A_loss: 0.1245, D_B_loss: 0.1214, G_A_loss: 0.7687, G_B_loss: 0.6710\n",
      "Epoch [24/30], Step [831/1067], D_A_loss: 0.0503, D_B_loss: 0.0696, G_A_loss: 0.5347, G_B_loss: 0.3794\n",
      "Epoch [24/30], Step [841/1067], D_A_loss: 0.1707, D_B_loss: 0.0566, G_A_loss: 0.8066, G_B_loss: 0.6043\n",
      "Epoch [24/30], Step [851/1067], D_A_loss: 0.0498, D_B_loss: 0.0296, G_A_loss: 0.4091, G_B_loss: 0.7012\n",
      "Epoch [24/30], Step [861/1067], D_A_loss: 0.1241, D_B_loss: 0.0315, G_A_loss: 0.2114, G_B_loss: 0.4347\n",
      "Epoch [24/30], Step [871/1067], D_A_loss: 0.0609, D_B_loss: 0.0252, G_A_loss: 0.7848, G_B_loss: 0.7971\n",
      "Epoch [24/30], Step [881/1067], D_A_loss: 0.2747, D_B_loss: 0.0536, G_A_loss: 0.5497, G_B_loss: 0.1145\n",
      "Epoch [24/30], Step [891/1067], D_A_loss: 0.1528, D_B_loss: 0.0430, G_A_loss: 0.6471, G_B_loss: 0.6354\n",
      "Epoch [24/30], Step [901/1067], D_A_loss: 0.1119, D_B_loss: 0.0381, G_A_loss: 0.6384, G_B_loss: 0.3735\n",
      "Epoch [24/30], Step [911/1067], D_A_loss: 0.1447, D_B_loss: 0.0801, G_A_loss: 0.4986, G_B_loss: 0.2834\n",
      "Epoch [24/30], Step [921/1067], D_A_loss: 0.0784, D_B_loss: 0.0611, G_A_loss: 0.6207, G_B_loss: 0.4254\n",
      "Epoch [24/30], Step [931/1067], D_A_loss: 0.0696, D_B_loss: 0.0180, G_A_loss: 0.6840, G_B_loss: 0.6426\n",
      "Epoch [24/30], Step [941/1067], D_A_loss: 0.0285, D_B_loss: 0.0288, G_A_loss: 0.8321, G_B_loss: 0.5500\n",
      "Epoch [24/30], Step [951/1067], D_A_loss: 0.1283, D_B_loss: 0.0559, G_A_loss: 0.5862, G_B_loss: 0.3443\n",
      "Epoch [24/30], Step [961/1067], D_A_loss: 0.1481, D_B_loss: 0.0456, G_A_loss: 0.7805, G_B_loss: 0.5275\n",
      "Epoch [24/30], Step [971/1067], D_A_loss: 0.1757, D_B_loss: 0.0253, G_A_loss: 0.6088, G_B_loss: 0.2434\n",
      "Epoch [24/30], Step [981/1067], D_A_loss: 0.1178, D_B_loss: 0.1435, G_A_loss: 0.3297, G_B_loss: 0.5477\n",
      "Epoch [24/30], Step [991/1067], D_A_loss: 0.0941, D_B_loss: 0.0257, G_A_loss: 0.7318, G_B_loss: 0.5329\n",
      "Epoch [24/30], Step [1001/1067], D_A_loss: 0.0654, D_B_loss: 0.0267, G_A_loss: 0.7792, G_B_loss: 0.4977\n",
      "Epoch [24/30], Step [1011/1067], D_A_loss: 0.0618, D_B_loss: 0.0425, G_A_loss: 0.4611, G_B_loss: 0.9144\n",
      "Epoch [24/30], Step [1021/1067], D_A_loss: 0.1157, D_B_loss: 0.1423, G_A_loss: 0.9481, G_B_loss: 0.5100\n",
      "Epoch [24/30], Step [1031/1067], D_A_loss: 0.0744, D_B_loss: 0.1030, G_A_loss: 0.4850, G_B_loss: 0.5676\n",
      "Epoch [24/30], Step [1041/1067], D_A_loss: 0.1074, D_B_loss: 0.0595, G_A_loss: 0.6459, G_B_loss: 0.4406\n",
      "Epoch [24/30], Step [1051/1067], D_A_loss: 0.0894, D_B_loss: 0.0828, G_A_loss: 0.7280, G_B_loss: 0.4373\n",
      "Epoch [24/30], Step [1061/1067], D_A_loss: 0.0724, D_B_loss: 0.0309, G_A_loss: 0.7514, G_B_loss: 0.8279\n",
      "Epoch [25/30], Step [1/1067], D_A_loss: 0.0699, D_B_loss: 0.1072, G_A_loss: 0.9432, G_B_loss: 0.5215\n",
      "Epoch [25/30], Step [11/1067], D_A_loss: 0.0537, D_B_loss: 0.3316, G_A_loss: 0.9249, G_B_loss: 0.6215\n",
      "Epoch [25/30], Step [21/1067], D_A_loss: 0.0672, D_B_loss: 0.0963, G_A_loss: 0.4013, G_B_loss: 0.5522\n",
      "Epoch [25/30], Step [31/1067], D_A_loss: 0.0235, D_B_loss: 0.0258, G_A_loss: 1.1358, G_B_loss: 1.0146\n",
      "Epoch [25/30], Step [41/1067], D_A_loss: 0.2098, D_B_loss: 0.1000, G_A_loss: 0.1751, G_B_loss: 0.1710\n",
      "Epoch [25/30], Step [51/1067], D_A_loss: 0.0954, D_B_loss: 0.0586, G_A_loss: 0.9823, G_B_loss: 0.6249\n",
      "Epoch [25/30], Step [61/1067], D_A_loss: 0.0807, D_B_loss: 0.1371, G_A_loss: 0.3916, G_B_loss: 0.8670\n",
      "Epoch [25/30], Step [71/1067], D_A_loss: 0.0635, D_B_loss: 0.0793, G_A_loss: 0.4183, G_B_loss: 0.8561\n",
      "Epoch [25/30], Step [81/1067], D_A_loss: 0.1427, D_B_loss: 0.1736, G_A_loss: 0.4007, G_B_loss: 0.2663\n",
      "Epoch [25/30], Step [91/1067], D_A_loss: 0.0613, D_B_loss: 0.0916, G_A_loss: 0.5265, G_B_loss: 0.5292\n",
      "Epoch [25/30], Step [101/1067], D_A_loss: 0.2428, D_B_loss: 0.0250, G_A_loss: 0.4025, G_B_loss: 0.7064\n",
      "Epoch [25/30], Step [111/1067], D_A_loss: 0.2095, D_B_loss: 0.0487, G_A_loss: 0.6029, G_B_loss: 0.6509\n",
      "Epoch [25/30], Step [121/1067], D_A_loss: 0.0455, D_B_loss: 0.0413, G_A_loss: 1.0452, G_B_loss: 0.7335\n",
      "Epoch [25/30], Step [131/1067], D_A_loss: 0.0779, D_B_loss: 0.1131, G_A_loss: 0.4495, G_B_loss: 0.9618\n",
      "Epoch [25/30], Step [141/1067], D_A_loss: 0.0496, D_B_loss: 0.0219, G_A_loss: 0.2933, G_B_loss: 0.6073\n",
      "Epoch [25/30], Step [151/1067], D_A_loss: 0.1927, D_B_loss: 0.0547, G_A_loss: 0.3328, G_B_loss: 0.3872\n",
      "Epoch [25/30], Step [161/1067], D_A_loss: 0.1402, D_B_loss: 0.0556, G_A_loss: 0.6381, G_B_loss: 0.4824\n",
      "Epoch [25/30], Step [171/1067], D_A_loss: 0.0433, D_B_loss: 0.1321, G_A_loss: 0.4877, G_B_loss: 1.1964\n",
      "Epoch [25/30], Step [181/1067], D_A_loss: 0.2239, D_B_loss: 0.1227, G_A_loss: 0.6570, G_B_loss: 0.1500\n",
      "Epoch [25/30], Step [191/1067], D_A_loss: 0.0215, D_B_loss: 0.1430, G_A_loss: 0.7633, G_B_loss: 0.7233\n",
      "Epoch [25/30], Step [201/1067], D_A_loss: 0.1800, D_B_loss: 0.0725, G_A_loss: 0.6457, G_B_loss: 0.1380\n",
      "Epoch [25/30], Step [211/1067], D_A_loss: 0.0343, D_B_loss: 0.0377, G_A_loss: 0.7479, G_B_loss: 0.3064\n",
      "Epoch [25/30], Step [221/1067], D_A_loss: 0.1407, D_B_loss: 0.0580, G_A_loss: 0.7508, G_B_loss: 0.3881\n",
      "Epoch [25/30], Step [231/1067], D_A_loss: 0.0493, D_B_loss: 0.0996, G_A_loss: 0.8782, G_B_loss: 0.4183\n",
      "Epoch [25/30], Step [241/1067], D_A_loss: 0.2516, D_B_loss: 0.0344, G_A_loss: 0.5696, G_B_loss: 0.7861\n",
      "Epoch [25/30], Step [251/1067], D_A_loss: 0.0178, D_B_loss: 0.1041, G_A_loss: 0.8068, G_B_loss: 0.4988\n",
      "Epoch [25/30], Step [261/1067], D_A_loss: 0.0666, D_B_loss: 0.0741, G_A_loss: 0.9298, G_B_loss: 0.5049\n",
      "Epoch [25/30], Step [271/1067], D_A_loss: 0.0544, D_B_loss: 0.0590, G_A_loss: 1.0485, G_B_loss: 0.4025\n",
      "Epoch [25/30], Step [281/1067], D_A_loss: 0.0386, D_B_loss: 0.0728, G_A_loss: 0.7557, G_B_loss: 0.6554\n",
      "Epoch [25/30], Step [291/1067], D_A_loss: 0.1229, D_B_loss: 0.0546, G_A_loss: 0.7162, G_B_loss: 0.5942\n",
      "Epoch [25/30], Step [301/1067], D_A_loss: 0.2925, D_B_loss: 0.0922, G_A_loss: 0.4276, G_B_loss: 0.4692\n",
      "Epoch [25/30], Step [311/1067], D_A_loss: 0.0799, D_B_loss: 0.0924, G_A_loss: 0.5258, G_B_loss: 0.8584\n",
      "Epoch [25/30], Step [321/1067], D_A_loss: 0.1196, D_B_loss: 0.0511, G_A_loss: 0.5499, G_B_loss: 0.8655\n",
      "Epoch [25/30], Step [331/1067], D_A_loss: 0.2258, D_B_loss: 0.0384, G_A_loss: 0.5305, G_B_loss: 0.3715\n",
      "Epoch [25/30], Step [341/1067], D_A_loss: 0.0427, D_B_loss: 0.0199, G_A_loss: 0.6061, G_B_loss: 0.4582\n",
      "Epoch [25/30], Step [351/1067], D_A_loss: 0.1500, D_B_loss: 0.1182, G_A_loss: 0.3513, G_B_loss: 0.4200\n",
      "Epoch [25/30], Step [361/1067], D_A_loss: 0.0654, D_B_loss: 0.1031, G_A_loss: 0.9608, G_B_loss: 0.3034\n",
      "Epoch [25/30], Step [371/1067], D_A_loss: 0.1186, D_B_loss: 0.0411, G_A_loss: 0.9442, G_B_loss: 0.4319\n",
      "Epoch [25/30], Step [381/1067], D_A_loss: 0.0338, D_B_loss: 0.1070, G_A_loss: 0.7322, G_B_loss: 0.2476\n",
      "Epoch [25/30], Step [391/1067], D_A_loss: 0.0272, D_B_loss: 0.0348, G_A_loss: 0.7845, G_B_loss: 0.8737\n",
      "Epoch [25/30], Step [401/1067], D_A_loss: 0.0727, D_B_loss: 0.0143, G_A_loss: 0.4485, G_B_loss: 0.9045\n",
      "Epoch [25/30], Step [411/1067], D_A_loss: 0.0432, D_B_loss: 0.0614, G_A_loss: 0.8096, G_B_loss: 0.5919\n",
      "Epoch [25/30], Step [421/1067], D_A_loss: 0.1374, D_B_loss: 0.0255, G_A_loss: 0.5381, G_B_loss: 0.3451\n",
      "Epoch [25/30], Step [431/1067], D_A_loss: 0.0599, D_B_loss: 0.0994, G_A_loss: 0.4844, G_B_loss: 0.6339\n",
      "Epoch [25/30], Step [441/1067], D_A_loss: 0.0430, D_B_loss: 0.0477, G_A_loss: 0.5946, G_B_loss: 0.4126\n",
      "Epoch [25/30], Step [451/1067], D_A_loss: 0.0836, D_B_loss: 0.0353, G_A_loss: 0.5592, G_B_loss: 0.4432\n",
      "Epoch [25/30], Step [461/1067], D_A_loss: 0.0749, D_B_loss: 0.0289, G_A_loss: 0.8459, G_B_loss: 0.5485\n",
      "Epoch [25/30], Step [471/1067], D_A_loss: 0.0876, D_B_loss: 0.0641, G_A_loss: 0.8604, G_B_loss: 0.6153\n",
      "Epoch [25/30], Step [481/1067], D_A_loss: 0.1999, D_B_loss: 0.1388, G_A_loss: 1.1670, G_B_loss: 0.2387\n",
      "Epoch [25/30], Step [491/1067], D_A_loss: 0.0707, D_B_loss: 0.1585, G_A_loss: 1.1345, G_B_loss: 0.8832\n",
      "Epoch [25/30], Step [501/1067], D_A_loss: 0.0962, D_B_loss: 0.0390, G_A_loss: 0.6725, G_B_loss: 0.3175\n",
      "Epoch [25/30], Step [511/1067], D_A_loss: 0.1193, D_B_loss: 0.0449, G_A_loss: 0.5922, G_B_loss: 0.3565\n",
      "Epoch [25/30], Step [521/1067], D_A_loss: 0.0839, D_B_loss: 0.0237, G_A_loss: 1.2370, G_B_loss: 0.7430\n",
      "Epoch [25/30], Step [531/1067], D_A_loss: 0.1322, D_B_loss: 0.2075, G_A_loss: 0.3285, G_B_loss: 0.5039\n",
      "Epoch [25/30], Step [541/1067], D_A_loss: 0.1522, D_B_loss: 0.0266, G_A_loss: 1.2556, G_B_loss: 0.3883\n",
      "Epoch [25/30], Step [551/1067], D_A_loss: 0.2539, D_B_loss: 0.1277, G_A_loss: 0.7120, G_B_loss: 0.6016\n",
      "Epoch [25/30], Step [561/1067], D_A_loss: 0.0650, D_B_loss: 0.0414, G_A_loss: 0.5886, G_B_loss: 0.6072\n",
      "Epoch [25/30], Step [571/1067], D_A_loss: 0.1967, D_B_loss: 0.0265, G_A_loss: 0.6910, G_B_loss: 0.3785\n",
      "Epoch [25/30], Step [581/1067], D_A_loss: 0.1126, D_B_loss: 0.1322, G_A_loss: 0.7493, G_B_loss: 0.3911\n",
      "Epoch [25/30], Step [591/1067], D_A_loss: 0.2143, D_B_loss: 0.0276, G_A_loss: 0.8652, G_B_loss: 0.2683\n",
      "Epoch [25/30], Step [601/1067], D_A_loss: 0.0293, D_B_loss: 0.1620, G_A_loss: 0.2246, G_B_loss: 0.4378\n",
      "Epoch [25/30], Step [611/1067], D_A_loss: 0.1803, D_B_loss: 0.0444, G_A_loss: 1.0346, G_B_loss: 0.4582\n",
      "Epoch [25/30], Step [621/1067], D_A_loss: 0.3282, D_B_loss: 0.0304, G_A_loss: 0.7039, G_B_loss: 0.0672\n",
      "Epoch [25/30], Step [631/1067], D_A_loss: 0.1177, D_B_loss: 0.0291, G_A_loss: 0.7227, G_B_loss: 0.2818\n",
      "Epoch [25/30], Step [641/1067], D_A_loss: 0.1503, D_B_loss: 0.2015, G_A_loss: 0.5921, G_B_loss: 0.5968\n",
      "Epoch [25/30], Step [651/1067], D_A_loss: 0.1125, D_B_loss: 0.0516, G_A_loss: 0.8359, G_B_loss: 0.6326\n",
      "Epoch [25/30], Step [661/1067], D_A_loss: 0.1944, D_B_loss: 0.0427, G_A_loss: 0.6515, G_B_loss: 0.3210\n",
      "Epoch [25/30], Step [671/1067], D_A_loss: 0.0395, D_B_loss: 0.0212, G_A_loss: 0.4941, G_B_loss: 1.1355\n",
      "Epoch [25/30], Step [681/1067], D_A_loss: 0.1402, D_B_loss: 0.0599, G_A_loss: 1.1217, G_B_loss: 0.6481\n",
      "Epoch [25/30], Step [691/1067], D_A_loss: 0.1811, D_B_loss: 0.1856, G_A_loss: 0.9971, G_B_loss: 0.2097\n",
      "Epoch [25/30], Step [701/1067], D_A_loss: 0.1419, D_B_loss: 0.0703, G_A_loss: 0.5737, G_B_loss: 0.5965\n",
      "Epoch [25/30], Step [711/1067], D_A_loss: 0.2214, D_B_loss: 0.0815, G_A_loss: 0.4585, G_B_loss: 0.6793\n",
      "Epoch [25/30], Step [721/1067], D_A_loss: 0.0717, D_B_loss: 0.0503, G_A_loss: 0.5614, G_B_loss: 0.6280\n",
      "Epoch [25/30], Step [731/1067], D_A_loss: 0.1711, D_B_loss: 0.0941, G_A_loss: 0.4413, G_B_loss: 0.6538\n",
      "Epoch [25/30], Step [741/1067], D_A_loss: 0.1470, D_B_loss: 0.0257, G_A_loss: 0.8758, G_B_loss: 0.4226\n",
      "Epoch [25/30], Step [751/1067], D_A_loss: 0.1131, D_B_loss: 0.0813, G_A_loss: 0.4924, G_B_loss: 0.5041\n",
      "Epoch [25/30], Step [761/1067], D_A_loss: 0.0975, D_B_loss: 0.0369, G_A_loss: 0.6376, G_B_loss: 0.7385\n",
      "Epoch [25/30], Step [771/1067], D_A_loss: 0.0893, D_B_loss: 0.0225, G_A_loss: 0.8378, G_B_loss: 0.4245\n",
      "Epoch [25/30], Step [781/1067], D_A_loss: 0.1375, D_B_loss: 0.0597, G_A_loss: 0.5183, G_B_loss: 0.4135\n",
      "Epoch [25/30], Step [791/1067], D_A_loss: 0.1843, D_B_loss: 0.0873, G_A_loss: 0.6402, G_B_loss: 0.4135\n",
      "Epoch [25/30], Step [801/1067], D_A_loss: 0.1049, D_B_loss: 0.0787, G_A_loss: 0.5136, G_B_loss: 0.7974\n",
      "Epoch [25/30], Step [811/1067], D_A_loss: 0.1951, D_B_loss: 0.0298, G_A_loss: 0.7259, G_B_loss: 0.3188\n",
      "Epoch [25/30], Step [821/1067], D_A_loss: 0.0877, D_B_loss: 0.0507, G_A_loss: 0.7448, G_B_loss: 0.4810\n",
      "Epoch [25/30], Step [831/1067], D_A_loss: 0.0527, D_B_loss: 0.1679, G_A_loss: 0.2874, G_B_loss: 0.3804\n",
      "Epoch [25/30], Step [841/1067], D_A_loss: 0.0721, D_B_loss: 0.0368, G_A_loss: 0.3984, G_B_loss: 0.6822\n",
      "Epoch [25/30], Step [851/1067], D_A_loss: 0.0452, D_B_loss: 0.0984, G_A_loss: 0.3181, G_B_loss: 0.1969\n",
      "Epoch [25/30], Step [861/1067], D_A_loss: 0.0886, D_B_loss: 0.0675, G_A_loss: 0.5970, G_B_loss: 0.8427\n",
      "Epoch [25/30], Step [871/1067], D_A_loss: 0.1284, D_B_loss: 0.0838, G_A_loss: 0.5030, G_B_loss: 0.7274\n",
      "Epoch [25/30], Step [881/1067], D_A_loss: 0.0416, D_B_loss: 0.0737, G_A_loss: 0.6948, G_B_loss: 0.5214\n",
      "Epoch [25/30], Step [891/1067], D_A_loss: 0.0990, D_B_loss: 0.0601, G_A_loss: 0.9000, G_B_loss: 0.4018\n",
      "Epoch [25/30], Step [901/1067], D_A_loss: 0.2087, D_B_loss: 0.0674, G_A_loss: 0.7989, G_B_loss: 0.1638\n",
      "Epoch [25/30], Step [911/1067], D_A_loss: 0.1195, D_B_loss: 0.0133, G_A_loss: 0.7298, G_B_loss: 0.5751\n",
      "Epoch [25/30], Step [921/1067], D_A_loss: 0.0918, D_B_loss: 0.0985, G_A_loss: 0.3143, G_B_loss: 0.4501\n",
      "Epoch [25/30], Step [931/1067], D_A_loss: 0.1639, D_B_loss: 0.1049, G_A_loss: 0.4354, G_B_loss: 0.3075\n",
      "Epoch [25/30], Step [941/1067], D_A_loss: 0.1019, D_B_loss: 0.0832, G_A_loss: 0.8187, G_B_loss: 0.2958\n",
      "Epoch [25/30], Step [951/1067], D_A_loss: 0.1835, D_B_loss: 0.1754, G_A_loss: 0.2198, G_B_loss: 0.6074\n",
      "Epoch [25/30], Step [961/1067], D_A_loss: 0.1723, D_B_loss: 0.0222, G_A_loss: 0.5063, G_B_loss: 0.3095\n",
      "Epoch [25/30], Step [971/1067], D_A_loss: 0.0745, D_B_loss: 0.0369, G_A_loss: 0.6946, G_B_loss: 0.6202\n",
      "Epoch [25/30], Step [981/1067], D_A_loss: 0.1910, D_B_loss: 0.0631, G_A_loss: 1.1887, G_B_loss: 0.1930\n",
      "Epoch [25/30], Step [991/1067], D_A_loss: 0.0708, D_B_loss: 0.0937, G_A_loss: 0.5159, G_B_loss: 1.0835\n",
      "Epoch [25/30], Step [1001/1067], D_A_loss: 0.0250, D_B_loss: 0.0518, G_A_loss: 0.9252, G_B_loss: 0.5593\n",
      "Epoch [25/30], Step [1011/1067], D_A_loss: 0.1518, D_B_loss: 0.0269, G_A_loss: 0.7212, G_B_loss: 0.2514\n",
      "Epoch [25/30], Step [1021/1067], D_A_loss: 0.1460, D_B_loss: 0.0496, G_A_loss: 0.8546, G_B_loss: 0.3485\n",
      "Epoch [25/30], Step [1031/1067], D_A_loss: 0.2747, D_B_loss: 0.0740, G_A_loss: 0.9262, G_B_loss: 1.2571\n",
      "Epoch [25/30], Step [1041/1067], D_A_loss: 0.0782, D_B_loss: 0.1056, G_A_loss: 0.4258, G_B_loss: 0.3150\n",
      "Epoch [25/30], Step [1051/1067], D_A_loss: 0.1172, D_B_loss: 0.1190, G_A_loss: 0.3856, G_B_loss: 0.3187\n",
      "Epoch [25/30], Step [1061/1067], D_A_loss: 0.0504, D_B_loss: 0.0494, G_A_loss: 0.6507, G_B_loss: 0.6060\n",
      "Epoch [26/30], Step [1/1067], D_A_loss: 0.1037, D_B_loss: 0.2336, G_A_loss: 0.2552, G_B_loss: 0.5887\n",
      "Epoch [26/30], Step [11/1067], D_A_loss: 0.1680, D_B_loss: 0.1001, G_A_loss: 0.3282, G_B_loss: 0.7398\n",
      "Epoch [26/30], Step [21/1067], D_A_loss: 0.1689, D_B_loss: 0.0454, G_A_loss: 0.8769, G_B_loss: 0.2213\n",
      "Epoch [26/30], Step [31/1067], D_A_loss: 0.0530, D_B_loss: 0.0575, G_A_loss: 0.5442, G_B_loss: 0.6444\n",
      "Epoch [26/30], Step [41/1067], D_A_loss: 0.3302, D_B_loss: 0.2166, G_A_loss: 1.0491, G_B_loss: 0.5910\n",
      "Epoch [26/30], Step [51/1067], D_A_loss: 0.1327, D_B_loss: 0.1424, G_A_loss: 0.8605, G_B_loss: 0.8222\n",
      "Epoch [26/30], Step [61/1067], D_A_loss: 0.2218, D_B_loss: 0.0284, G_A_loss: 0.8738, G_B_loss: 0.6765\n",
      "Epoch [26/30], Step [71/1067], D_A_loss: 0.0569, D_B_loss: 0.0535, G_A_loss: 1.0325, G_B_loss: 0.5382\n",
      "Epoch [26/30], Step [81/1067], D_A_loss: 0.1051, D_B_loss: 0.0317, G_A_loss: 0.8193, G_B_loss: 0.5742\n",
      "Epoch [26/30], Step [91/1067], D_A_loss: 0.0875, D_B_loss: 0.0150, G_A_loss: 0.5771, G_B_loss: 0.8381\n",
      "Epoch [26/30], Step [101/1067], D_A_loss: 0.0600, D_B_loss: 0.0361, G_A_loss: 0.3610, G_B_loss: 0.5869\n",
      "Epoch [26/30], Step [111/1067], D_A_loss: 0.2256, D_B_loss: 0.1482, G_A_loss: 0.6816, G_B_loss: 0.2740\n",
      "Epoch [26/30], Step [121/1067], D_A_loss: 0.0657, D_B_loss: 0.0172, G_A_loss: 0.4752, G_B_loss: 0.5039\n",
      "Epoch [26/30], Step [131/1067], D_A_loss: 0.1816, D_B_loss: 0.0323, G_A_loss: 0.6558, G_B_loss: 0.2072\n",
      "Epoch [26/30], Step [141/1067], D_A_loss: 0.3223, D_B_loss: 0.0341, G_A_loss: 0.6849, G_B_loss: 0.5053\n",
      "Epoch [26/30], Step [151/1067], D_A_loss: 0.0547, D_B_loss: 0.0642, G_A_loss: 0.5312, G_B_loss: 0.4523\n",
      "Epoch [26/30], Step [161/1067], D_A_loss: 0.0930, D_B_loss: 0.0581, G_A_loss: 0.5567, G_B_loss: 0.5117\n",
      "Epoch [26/30], Step [171/1067], D_A_loss: 0.0559, D_B_loss: 0.2974, G_A_loss: 0.1837, G_B_loss: 0.5502\n",
      "Epoch [26/30], Step [181/1067], D_A_loss: 0.0318, D_B_loss: 0.0887, G_A_loss: 0.4950, G_B_loss: 0.4105\n",
      "Epoch [26/30], Step [191/1067], D_A_loss: 0.0334, D_B_loss: 0.0228, G_A_loss: 0.9876, G_B_loss: 0.5183\n",
      "Epoch [26/30], Step [201/1067], D_A_loss: 0.0716, D_B_loss: 0.1608, G_A_loss: 0.7773, G_B_loss: 0.5211\n",
      "Epoch [26/30], Step [211/1067], D_A_loss: 0.1389, D_B_loss: 0.0774, G_A_loss: 0.6289, G_B_loss: 0.2966\n",
      "Epoch [26/30], Step [221/1067], D_A_loss: 0.1048, D_B_loss: 0.0755, G_A_loss: 0.5515, G_B_loss: 0.8825\n",
      "Epoch [26/30], Step [231/1067], D_A_loss: 0.0138, D_B_loss: 0.0680, G_A_loss: 0.9087, G_B_loss: 0.4988\n",
      "Epoch [26/30], Step [241/1067], D_A_loss: 0.0790, D_B_loss: 0.0638, G_A_loss: 0.3497, G_B_loss: 0.3718\n",
      "Epoch [26/30], Step [251/1067], D_A_loss: 0.2319, D_B_loss: 0.0647, G_A_loss: 0.2766, G_B_loss: 0.5658\n",
      "Epoch [26/30], Step [261/1067], D_A_loss: 0.0829, D_B_loss: 0.0479, G_A_loss: 1.1060, G_B_loss: 0.1770\n",
      "Epoch [26/30], Step [271/1067], D_A_loss: 0.1667, D_B_loss: 0.0306, G_A_loss: 0.4168, G_B_loss: 0.6064\n",
      "Epoch [26/30], Step [281/1067], D_A_loss: 0.0355, D_B_loss: 0.0339, G_A_loss: 0.9373, G_B_loss: 0.2473\n",
      "Epoch [26/30], Step [291/1067], D_A_loss: 0.2165, D_B_loss: 0.0264, G_A_loss: 0.3866, G_B_loss: 0.9792\n",
      "Epoch [26/30], Step [301/1067], D_A_loss: 0.3473, D_B_loss: 0.0728, G_A_loss: 0.5664, G_B_loss: 0.9335\n",
      "Epoch [26/30], Step [311/1067], D_A_loss: 0.2301, D_B_loss: 0.0240, G_A_loss: 0.5840, G_B_loss: 0.2948\n",
      "Epoch [26/30], Step [321/1067], D_A_loss: 0.0882, D_B_loss: 0.0385, G_A_loss: 0.9767, G_B_loss: 0.4207\n",
      "Epoch [26/30], Step [331/1067], D_A_loss: 0.0603, D_B_loss: 0.2488, G_A_loss: 0.2266, G_B_loss: 0.7845\n",
      "Epoch [26/30], Step [341/1067], D_A_loss: 0.1897, D_B_loss: 0.0553, G_A_loss: 0.8007, G_B_loss: 0.4872\n",
      "Epoch [26/30], Step [351/1067], D_A_loss: 0.0947, D_B_loss: 0.0725, G_A_loss: 0.9829, G_B_loss: 0.4152\n",
      "Epoch [26/30], Step [361/1067], D_A_loss: 0.2646, D_B_loss: 0.0994, G_A_loss: 0.3876, G_B_loss: 0.6032\n",
      "Epoch [26/30], Step [371/1067], D_A_loss: 0.0926, D_B_loss: 0.0583, G_A_loss: 0.6661, G_B_loss: 0.4274\n",
      "Epoch [26/30], Step [381/1067], D_A_loss: 0.1274, D_B_loss: 0.0592, G_A_loss: 0.4256, G_B_loss: 0.1526\n",
      "Epoch [26/30], Step [391/1067], D_A_loss: 0.2861, D_B_loss: 0.0584, G_A_loss: 0.5063, G_B_loss: 0.1190\n",
      "Epoch [26/30], Step [401/1067], D_A_loss: 0.1137, D_B_loss: 0.0785, G_A_loss: 0.5221, G_B_loss: 0.7170\n",
      "Epoch [26/30], Step [411/1067], D_A_loss: 0.0937, D_B_loss: 0.0327, G_A_loss: 0.6891, G_B_loss: 0.8088\n",
      "Epoch [26/30], Step [421/1067], D_A_loss: 0.0477, D_B_loss: 0.0762, G_A_loss: 0.5904, G_B_loss: 0.3733\n",
      "Epoch [26/30], Step [431/1067], D_A_loss: 0.0312, D_B_loss: 0.0414, G_A_loss: 0.7443, G_B_loss: 0.4480\n",
      "Epoch [26/30], Step [441/1067], D_A_loss: 0.0547, D_B_loss: 0.0987, G_A_loss: 0.3215, G_B_loss: 0.6003\n",
      "Epoch [26/30], Step [451/1067], D_A_loss: 0.0570, D_B_loss: 0.1853, G_A_loss: 1.0307, G_B_loss: 0.2596\n",
      "Epoch [26/30], Step [461/1067], D_A_loss: 0.0744, D_B_loss: 0.0330, G_A_loss: 0.2367, G_B_loss: 0.2362\n",
      "Epoch [26/30], Step [471/1067], D_A_loss: 0.3343, D_B_loss: 0.0371, G_A_loss: 0.6680, G_B_loss: 0.5346\n",
      "Epoch [26/30], Step [481/1067], D_A_loss: 0.1158, D_B_loss: 0.0266, G_A_loss: 0.9419, G_B_loss: 0.0832\n",
      "Epoch [26/30], Step [491/1067], D_A_loss: 0.3108, D_B_loss: 0.0363, G_A_loss: 0.9851, G_B_loss: 0.8949\n",
      "Epoch [26/30], Step [501/1067], D_A_loss: 0.2124, D_B_loss: 0.0651, G_A_loss: 0.6729, G_B_loss: 0.3904\n",
      "Epoch [26/30], Step [511/1067], D_A_loss: 0.1203, D_B_loss: 0.0378, G_A_loss: 0.2929, G_B_loss: 0.4972\n",
      "Epoch [26/30], Step [521/1067], D_A_loss: 0.1141, D_B_loss: 0.0456, G_A_loss: 0.8303, G_B_loss: 0.3799\n",
      "Epoch [26/30], Step [531/1067], D_A_loss: 0.2332, D_B_loss: 0.0644, G_A_loss: 0.5067, G_B_loss: 0.2647\n",
      "Epoch [26/30], Step [541/1067], D_A_loss: 0.0906, D_B_loss: 0.0359, G_A_loss: 0.7525, G_B_loss: 0.7633\n",
      "Epoch [26/30], Step [551/1067], D_A_loss: 0.1060, D_B_loss: 0.0631, G_A_loss: 0.5419, G_B_loss: 0.5346\n",
      "Epoch [26/30], Step [561/1067], D_A_loss: 0.2103, D_B_loss: 0.1499, G_A_loss: 0.2889, G_B_loss: 0.5209\n",
      "Epoch [26/30], Step [571/1067], D_A_loss: 0.2441, D_B_loss: 0.0811, G_A_loss: 0.8745, G_B_loss: 0.2015\n",
      "Epoch [26/30], Step [581/1067], D_A_loss: 0.0746, D_B_loss: 0.0955, G_A_loss: 0.4650, G_B_loss: 0.5363\n",
      "Epoch [26/30], Step [591/1067], D_A_loss: 0.0671, D_B_loss: 0.0481, G_A_loss: 0.5535, G_B_loss: 0.1761\n",
      "Epoch [26/30], Step [601/1067], D_A_loss: 0.1105, D_B_loss: 0.0447, G_A_loss: 0.5925, G_B_loss: 0.5052\n",
      "Epoch [26/30], Step [611/1067], D_A_loss: 0.1068, D_B_loss: 0.1083, G_A_loss: 1.1019, G_B_loss: 0.9197\n",
      "Epoch [26/30], Step [621/1067], D_A_loss: 0.1910, D_B_loss: 0.0491, G_A_loss: 0.5242, G_B_loss: 0.5414\n",
      "Epoch [26/30], Step [631/1067], D_A_loss: 0.2476, D_B_loss: 0.0893, G_A_loss: 0.7320, G_B_loss: 0.2574\n",
      "Epoch [26/30], Step [641/1067], D_A_loss: 0.2357, D_B_loss: 0.0170, G_A_loss: 0.7710, G_B_loss: 0.2882\n",
      "Epoch [26/30], Step [651/1067], D_A_loss: 0.2515, D_B_loss: 0.0270, G_A_loss: 0.7185, G_B_loss: 0.1150\n",
      "Epoch [26/30], Step [661/1067], D_A_loss: 0.0848, D_B_loss: 0.0115, G_A_loss: 0.9165, G_B_loss: 0.4023\n",
      "Epoch [26/30], Step [671/1067], D_A_loss: 0.0264, D_B_loss: 0.0743, G_A_loss: 0.8365, G_B_loss: 0.6057\n",
      "Epoch [26/30], Step [681/1067], D_A_loss: 0.1165, D_B_loss: 0.0513, G_A_loss: 0.6321, G_B_loss: 0.4065\n",
      "Epoch [26/30], Step [691/1067], D_A_loss: 0.1874, D_B_loss: 0.1722, G_A_loss: 0.4611, G_B_loss: 0.2802\n",
      "Epoch [26/30], Step [701/1067], D_A_loss: 0.0844, D_B_loss: 0.0355, G_A_loss: 0.5474, G_B_loss: 0.2921\n",
      "Epoch [26/30], Step [711/1067], D_A_loss: 0.1306, D_B_loss: 0.0597, G_A_loss: 0.5471, G_B_loss: 0.8794\n",
      "Epoch [26/30], Step [721/1067], D_A_loss: 0.0685, D_B_loss: 0.0412, G_A_loss: 0.6716, G_B_loss: 0.5143\n",
      "Epoch [26/30], Step [731/1067], D_A_loss: 0.0721, D_B_loss: 0.1291, G_A_loss: 0.3351, G_B_loss: 0.6139\n",
      "Epoch [26/30], Step [741/1067], D_A_loss: 0.2097, D_B_loss: 0.1721, G_A_loss: 0.5647, G_B_loss: 0.2102\n",
      "Epoch [26/30], Step [751/1067], D_A_loss: 0.0383, D_B_loss: 0.2706, G_A_loss: 0.6800, G_B_loss: 0.5420\n",
      "Epoch [26/30], Step [761/1067], D_A_loss: 0.0651, D_B_loss: 0.0519, G_A_loss: 1.2711, G_B_loss: 0.9863\n",
      "Epoch [26/30], Step [771/1067], D_A_loss: 0.1399, D_B_loss: 0.0343, G_A_loss: 0.3638, G_B_loss: 0.4790\n",
      "Epoch [26/30], Step [781/1067], D_A_loss: 0.0937, D_B_loss: 0.0530, G_A_loss: 0.5661, G_B_loss: 0.4991\n",
      "Epoch [26/30], Step [791/1067], D_A_loss: 0.1069, D_B_loss: 0.0712, G_A_loss: 0.7354, G_B_loss: 0.4378\n",
      "Epoch [26/30], Step [801/1067], D_A_loss: 0.1028, D_B_loss: 0.1019, G_A_loss: 0.4060, G_B_loss: 0.4578\n",
      "Epoch [26/30], Step [811/1067], D_A_loss: 0.0773, D_B_loss: 0.0618, G_A_loss: 0.5809, G_B_loss: 0.7247\n",
      "Epoch [26/30], Step [821/1067], D_A_loss: 0.1858, D_B_loss: 0.0749, G_A_loss: 0.6696, G_B_loss: 0.4892\n",
      "Epoch [26/30], Step [831/1067], D_A_loss: 0.0378, D_B_loss: 0.0447, G_A_loss: 0.8879, G_B_loss: 0.4843\n",
      "Epoch [26/30], Step [841/1067], D_A_loss: 0.2279, D_B_loss: 0.1397, G_A_loss: 0.3648, G_B_loss: 0.7097\n",
      "Epoch [26/30], Step [851/1067], D_A_loss: 0.1802, D_B_loss: 0.1375, G_A_loss: 0.6938, G_B_loss: 0.5867\n",
      "Epoch [26/30], Step [861/1067], D_A_loss: 0.1231, D_B_loss: 0.0157, G_A_loss: 0.9753, G_B_loss: 0.3954\n",
      "Epoch [26/30], Step [871/1067], D_A_loss: 0.0935, D_B_loss: 0.0432, G_A_loss: 0.5623, G_B_loss: 0.7566\n",
      "Epoch [26/30], Step [881/1067], D_A_loss: 0.1557, D_B_loss: 0.0446, G_A_loss: 0.9177, G_B_loss: 0.1990\n",
      "Epoch [26/30], Step [891/1067], D_A_loss: 0.0830, D_B_loss: 0.0397, G_A_loss: 0.3982, G_B_loss: 0.6340\n",
      "Epoch [26/30], Step [901/1067], D_A_loss: 0.1664, D_B_loss: 0.0371, G_A_loss: 0.5653, G_B_loss: 0.8588\n",
      "Epoch [26/30], Step [911/1067], D_A_loss: 0.1338, D_B_loss: 0.0456, G_A_loss: 0.6090, G_B_loss: 0.6324\n",
      "Epoch [26/30], Step [921/1067], D_A_loss: 0.1155, D_B_loss: 0.0191, G_A_loss: 0.9441, G_B_loss: 0.3440\n",
      "Epoch [26/30], Step [931/1067], D_A_loss: 0.1352, D_B_loss: 0.2917, G_A_loss: 1.1866, G_B_loss: 0.6622\n",
      "Epoch [26/30], Step [941/1067], D_A_loss: 0.0727, D_B_loss: 0.0396, G_A_loss: 0.6729, G_B_loss: 0.5769\n",
      "Epoch [26/30], Step [951/1067], D_A_loss: 0.1033, D_B_loss: 0.0198, G_A_loss: 0.6375, G_B_loss: 0.6244\n",
      "Epoch [26/30], Step [961/1067], D_A_loss: 0.1008, D_B_loss: 0.1186, G_A_loss: 0.3653, G_B_loss: 0.2485\n",
      "Epoch [26/30], Step [971/1067], D_A_loss: 0.2614, D_B_loss: 0.0593, G_A_loss: 0.5991, G_B_loss: 0.3081\n",
      "Epoch [26/30], Step [981/1067], D_A_loss: 0.1246, D_B_loss: 0.1899, G_A_loss: 0.2854, G_B_loss: 0.0868\n",
      "Epoch [26/30], Step [991/1067], D_A_loss: 0.1311, D_B_loss: 0.1639, G_A_loss: 0.3085, G_B_loss: 0.3940\n",
      "Epoch [26/30], Step [1001/1067], D_A_loss: 0.1636, D_B_loss: 0.1613, G_A_loss: 0.8600, G_B_loss: 0.6440\n",
      "Epoch [26/30], Step [1011/1067], D_A_loss: 0.1417, D_B_loss: 0.0461, G_A_loss: 0.5623, G_B_loss: 0.4887\n",
      "Epoch [26/30], Step [1021/1067], D_A_loss: 0.0359, D_B_loss: 0.3038, G_A_loss: 0.2099, G_B_loss: 0.3134\n",
      "Epoch [26/30], Step [1031/1067], D_A_loss: 0.0915, D_B_loss: 0.0547, G_A_loss: 0.6706, G_B_loss: 0.4142\n",
      "Epoch [26/30], Step [1041/1067], D_A_loss: 0.1006, D_B_loss: 0.0352, G_A_loss: 0.5934, G_B_loss: 0.4206\n",
      "Epoch [26/30], Step [1051/1067], D_A_loss: 0.1082, D_B_loss: 0.0404, G_A_loss: 0.5426, G_B_loss: 0.4827\n",
      "Epoch [26/30], Step [1061/1067], D_A_loss: 0.1151, D_B_loss: 0.0445, G_A_loss: 0.5105, G_B_loss: 0.4948\n",
      "Epoch [27/30], Step [1/1067], D_A_loss: 0.0741, D_B_loss: 0.0449, G_A_loss: 0.6098, G_B_loss: 0.4716\n",
      "Epoch [27/30], Step [11/1067], D_A_loss: 0.1314, D_B_loss: 0.0371, G_A_loss: 0.4453, G_B_loss: 0.7079\n",
      "Epoch [27/30], Step [21/1067], D_A_loss: 0.2035, D_B_loss: 0.0435, G_A_loss: 0.7220, G_B_loss: 0.2358\n",
      "Epoch [27/30], Step [31/1067], D_A_loss: 0.1805, D_B_loss: 0.1166, G_A_loss: 0.6244, G_B_loss: 0.5200\n",
      "Epoch [27/30], Step [41/1067], D_A_loss: 0.1586, D_B_loss: 0.0312, G_A_loss: 0.3424, G_B_loss: 0.4972\n",
      "Epoch [27/30], Step [51/1067], D_A_loss: 0.1916, D_B_loss: 0.0380, G_A_loss: 0.5782, G_B_loss: 0.3581\n",
      "Epoch [27/30], Step [61/1067], D_A_loss: 0.0828, D_B_loss: 0.0187, G_A_loss: 0.7957, G_B_loss: 0.4467\n",
      "Epoch [27/30], Step [71/1067], D_A_loss: 0.0912, D_B_loss: 0.0717, G_A_loss: 0.9750, G_B_loss: 0.4239\n",
      "Epoch [27/30], Step [81/1067], D_A_loss: 0.0864, D_B_loss: 0.0374, G_A_loss: 0.9693, G_B_loss: 0.0768\n",
      "Epoch [27/30], Step [91/1067], D_A_loss: 0.1581, D_B_loss: 0.1017, G_A_loss: 0.7721, G_B_loss: 0.6227\n",
      "Epoch [27/30], Step [101/1067], D_A_loss: 0.0868, D_B_loss: 0.1474, G_A_loss: 0.8690, G_B_loss: 0.4302\n",
      "Epoch [27/30], Step [111/1067], D_A_loss: 0.1092, D_B_loss: 0.0184, G_A_loss: 0.8622, G_B_loss: 0.3454\n",
      "Epoch [27/30], Step [121/1067], D_A_loss: 0.2913, D_B_loss: 0.0580, G_A_loss: 0.6871, G_B_loss: 0.1003\n",
      "Epoch [27/30], Step [131/1067], D_A_loss: 0.0394, D_B_loss: 0.2151, G_A_loss: 0.7658, G_B_loss: 0.3911\n",
      "Epoch [27/30], Step [141/1067], D_A_loss: 0.0603, D_B_loss: 0.0194, G_A_loss: 0.9616, G_B_loss: 0.2955\n",
      "Epoch [27/30], Step [151/1067], D_A_loss: 0.1523, D_B_loss: 0.0634, G_A_loss: 0.7988, G_B_loss: 0.4059\n",
      "Epoch [27/30], Step [161/1067], D_A_loss: 0.0964, D_B_loss: 0.0380, G_A_loss: 0.3579, G_B_loss: 0.8449\n",
      "Epoch [27/30], Step [171/1067], D_A_loss: 0.1047, D_B_loss: 0.0274, G_A_loss: 0.9446, G_B_loss: 0.9562\n",
      "Epoch [27/30], Step [181/1067], D_A_loss: 0.0658, D_B_loss: 0.0622, G_A_loss: 0.6129, G_B_loss: 0.5435\n",
      "Epoch [27/30], Step [191/1067], D_A_loss: 0.1863, D_B_loss: 0.0398, G_A_loss: 1.0930, G_B_loss: 0.5399\n",
      "Epoch [27/30], Step [201/1067], D_A_loss: 0.2945, D_B_loss: 0.1535, G_A_loss: 0.2577, G_B_loss: 1.3739\n",
      "Epoch [27/30], Step [211/1067], D_A_loss: 0.1367, D_B_loss: 0.1631, G_A_loss: 1.5802, G_B_loss: 0.2833\n",
      "Epoch [27/30], Step [221/1067], D_A_loss: 0.0284, D_B_loss: 0.0840, G_A_loss: 0.3340, G_B_loss: 0.3053\n",
      "Epoch [27/30], Step [231/1067], D_A_loss: 0.1491, D_B_loss: 0.0135, G_A_loss: 0.9056, G_B_loss: 0.4211\n",
      "Epoch [27/30], Step [241/1067], D_A_loss: 0.1001, D_B_loss: 0.0325, G_A_loss: 0.6718, G_B_loss: 0.3613\n",
      "Epoch [27/30], Step [251/1067], D_A_loss: 0.1200, D_B_loss: 0.1391, G_A_loss: 1.4494, G_B_loss: 0.4258\n",
      "Epoch [27/30], Step [261/1067], D_A_loss: 0.2028, D_B_loss: 0.0533, G_A_loss: 1.0887, G_B_loss: 0.1605\n",
      "Epoch [27/30], Step [271/1067], D_A_loss: 0.0712, D_B_loss: 0.0314, G_A_loss: 0.8600, G_B_loss: 0.6295\n",
      "Epoch [27/30], Step [281/1067], D_A_loss: 0.0747, D_B_loss: 0.0414, G_A_loss: 0.5714, G_B_loss: 0.4219\n",
      "Epoch [27/30], Step [291/1067], D_A_loss: 0.3095, D_B_loss: 0.1078, G_A_loss: 0.3222, G_B_loss: 0.0778\n",
      "Epoch [27/30], Step [301/1067], D_A_loss: 0.0586, D_B_loss: 0.0393, G_A_loss: 0.9410, G_B_loss: 0.5796\n",
      "Epoch [27/30], Step [311/1067], D_A_loss: 0.1135, D_B_loss: 0.0370, G_A_loss: 0.7152, G_B_loss: 0.7164\n",
      "Epoch [27/30], Step [321/1067], D_A_loss: 0.1826, D_B_loss: 0.0291, G_A_loss: 0.6415, G_B_loss: 0.4843\n",
      "Epoch [27/30], Step [331/1067], D_A_loss: 0.0759, D_B_loss: 0.1050, G_A_loss: 0.4456, G_B_loss: 0.5503\n",
      "Epoch [27/30], Step [341/1067], D_A_loss: 0.1095, D_B_loss: 0.0733, G_A_loss: 0.6401, G_B_loss: 0.3813\n",
      "Epoch [27/30], Step [351/1067], D_A_loss: 0.2046, D_B_loss: 0.0214, G_A_loss: 0.4806, G_B_loss: 0.2267\n",
      "Epoch [27/30], Step [361/1067], D_A_loss: 0.0659, D_B_loss: 0.0271, G_A_loss: 0.8403, G_B_loss: 0.2316\n",
      "Epoch [27/30], Step [371/1067], D_A_loss: 0.3215, D_B_loss: 0.0253, G_A_loss: 0.4109, G_B_loss: 0.3435\n",
      "Epoch [27/30], Step [381/1067], D_A_loss: 0.1834, D_B_loss: 0.0501, G_A_loss: 1.1281, G_B_loss: 0.2156\n",
      "Epoch [27/30], Step [391/1067], D_A_loss: 0.0971, D_B_loss: 0.0929, G_A_loss: 0.2741, G_B_loss: 0.4706\n",
      "Epoch [27/30], Step [401/1067], D_A_loss: 0.1035, D_B_loss: 0.0286, G_A_loss: 0.7363, G_B_loss: 0.7445\n",
      "Epoch [27/30], Step [411/1067], D_A_loss: 0.1199, D_B_loss: 0.0352, G_A_loss: 0.6073, G_B_loss: 0.3269\n",
      "Epoch [27/30], Step [421/1067], D_A_loss: 0.0487, D_B_loss: 0.0275, G_A_loss: 1.0163, G_B_loss: 0.4318\n",
      "Epoch [27/30], Step [431/1067], D_A_loss: 0.0721, D_B_loss: 0.0462, G_A_loss: 1.2526, G_B_loss: 0.8270\n",
      "Epoch [27/30], Step [441/1067], D_A_loss: 0.1707, D_B_loss: 0.1937, G_A_loss: 0.1883, G_B_loss: 0.2549\n",
      "Epoch [27/30], Step [451/1067], D_A_loss: 0.0512, D_B_loss: 0.0351, G_A_loss: 0.9973, G_B_loss: 0.4143\n",
      "Epoch [27/30], Step [461/1067], D_A_loss: 0.0849, D_B_loss: 0.0399, G_A_loss: 1.1015, G_B_loss: 0.4691\n",
      "Epoch [27/30], Step [471/1067], D_A_loss: 0.1318, D_B_loss: 0.0251, G_A_loss: 1.0808, G_B_loss: 0.2932\n",
      "Epoch [27/30], Step [481/1067], D_A_loss: 0.2880, D_B_loss: 0.0657, G_A_loss: 0.4732, G_B_loss: 0.7216\n",
      "Epoch [27/30], Step [491/1067], D_A_loss: 0.0317, D_B_loss: 0.0426, G_A_loss: 0.9172, G_B_loss: 0.6763\n",
      "Epoch [27/30], Step [501/1067], D_A_loss: 0.1218, D_B_loss: 0.0387, G_A_loss: 0.7742, G_B_loss: 0.5387\n",
      "Epoch [27/30], Step [511/1067], D_A_loss: 0.1210, D_B_loss: 0.0491, G_A_loss: 0.8288, G_B_loss: 0.3665\n",
      "Epoch [27/30], Step [521/1067], D_A_loss: 0.1074, D_B_loss: 0.0566, G_A_loss: 0.5145, G_B_loss: 0.6510\n",
      "Epoch [27/30], Step [531/1067], D_A_loss: 0.1589, D_B_loss: 0.1330, G_A_loss: 0.5397, G_B_loss: 0.3405\n",
      "Epoch [27/30], Step [541/1067], D_A_loss: 0.2524, D_B_loss: 0.0751, G_A_loss: 0.7045, G_B_loss: 0.6252\n",
      "Epoch [27/30], Step [551/1067], D_A_loss: 0.1260, D_B_loss: 0.0426, G_A_loss: 0.6108, G_B_loss: 0.1587\n",
      "Epoch [27/30], Step [561/1067], D_A_loss: 0.0473, D_B_loss: 0.0136, G_A_loss: 0.5533, G_B_loss: 0.2330\n",
      "Epoch [27/30], Step [571/1067], D_A_loss: 0.1632, D_B_loss: 0.0277, G_A_loss: 0.9133, G_B_loss: 0.6259\n",
      "Epoch [27/30], Step [581/1067], D_A_loss: 0.0686, D_B_loss: 0.0311, G_A_loss: 0.3934, G_B_loss: 0.8165\n",
      "Epoch [27/30], Step [591/1067], D_A_loss: 0.1742, D_B_loss: 0.1182, G_A_loss: 0.4363, G_B_loss: 0.8057\n",
      "Epoch [27/30], Step [601/1067], D_A_loss: 0.0588, D_B_loss: 0.1195, G_A_loss: 0.6322, G_B_loss: 0.5866\n",
      "Epoch [27/30], Step [611/1067], D_A_loss: 0.0440, D_B_loss: 0.0438, G_A_loss: 0.7175, G_B_loss: 0.5889\n",
      "Epoch [27/30], Step [621/1067], D_A_loss: 0.1251, D_B_loss: 0.0489, G_A_loss: 1.3312, G_B_loss: 0.4834\n",
      "Epoch [27/30], Step [631/1067], D_A_loss: 0.1341, D_B_loss: 0.0462, G_A_loss: 1.0220, G_B_loss: 0.3876\n",
      "Epoch [27/30], Step [641/1067], D_A_loss: 0.0506, D_B_loss: 0.1172, G_A_loss: 0.9150, G_B_loss: 0.6447\n",
      "Epoch [27/30], Step [651/1067], D_A_loss: 0.0748, D_B_loss: 0.0529, G_A_loss: 0.9220, G_B_loss: 0.8335\n",
      "Epoch [27/30], Step [661/1067], D_A_loss: 0.1657, D_B_loss: 0.1324, G_A_loss: 0.3166, G_B_loss: 0.6806\n",
      "Epoch [27/30], Step [671/1067], D_A_loss: 0.1278, D_B_loss: 0.0499, G_A_loss: 1.2031, G_B_loss: 0.5895\n",
      "Epoch [27/30], Step [681/1067], D_A_loss: 0.0583, D_B_loss: 0.0280, G_A_loss: 0.3251, G_B_loss: 0.2973\n",
      "Epoch [27/30], Step [691/1067], D_A_loss: 0.1927, D_B_loss: 0.0192, G_A_loss: 0.6334, G_B_loss: 0.2466\n",
      "Epoch [27/30], Step [701/1067], D_A_loss: 0.0808, D_B_loss: 0.1124, G_A_loss: 0.4189, G_B_loss: 0.6352\n",
      "Epoch [27/30], Step [711/1067], D_A_loss: 0.3613, D_B_loss: 0.2476, G_A_loss: 1.5700, G_B_loss: 0.1369\n",
      "Epoch [27/30], Step [721/1067], D_A_loss: 0.1634, D_B_loss: 0.0316, G_A_loss: 0.4785, G_B_loss: 0.7853\n",
      "Epoch [27/30], Step [731/1067], D_A_loss: 0.0444, D_B_loss: 0.0833, G_A_loss: 0.9850, G_B_loss: 0.2689\n",
      "Epoch [27/30], Step [741/1067], D_A_loss: 0.1949, D_B_loss: 0.0832, G_A_loss: 0.9250, G_B_loss: 0.2889\n",
      "Epoch [27/30], Step [751/1067], D_A_loss: 0.0821, D_B_loss: 0.0594, G_A_loss: 0.5772, G_B_loss: 0.6225\n",
      "Epoch [27/30], Step [761/1067], D_A_loss: 0.1324, D_B_loss: 0.0924, G_A_loss: 0.4287, G_B_loss: 0.4342\n",
      "Epoch [27/30], Step [771/1067], D_A_loss: 0.1044, D_B_loss: 0.1321, G_A_loss: 0.5013, G_B_loss: 0.5113\n",
      "Epoch [27/30], Step [781/1067], D_A_loss: 0.0850, D_B_loss: 0.0281, G_A_loss: 0.8907, G_B_loss: 0.7712\n",
      "Epoch [27/30], Step [791/1067], D_A_loss: 0.0990, D_B_loss: 0.0165, G_A_loss: 0.5910, G_B_loss: 0.5245\n",
      "Epoch [27/30], Step [801/1067], D_A_loss: 0.0758, D_B_loss: 0.1030, G_A_loss: 0.6556, G_B_loss: 0.5018\n",
      "Epoch [27/30], Step [811/1067], D_A_loss: 0.1125, D_B_loss: 0.2009, G_A_loss: 0.7253, G_B_loss: 0.3624\n",
      "Epoch [27/30], Step [821/1067], D_A_loss: 0.0749, D_B_loss: 0.0405, G_A_loss: 0.6621, G_B_loss: 0.4620\n",
      "Epoch [27/30], Step [831/1067], D_A_loss: 0.0306, D_B_loss: 0.0713, G_A_loss: 0.7453, G_B_loss: 0.1926\n",
      "Epoch [27/30], Step [841/1067], D_A_loss: 0.0602, D_B_loss: 0.0152, G_A_loss: 0.9165, G_B_loss: 1.0130\n",
      "Epoch [27/30], Step [851/1067], D_A_loss: 0.0748, D_B_loss: 0.0667, G_A_loss: 0.8938, G_B_loss: 0.6119\n",
      "Epoch [27/30], Step [861/1067], D_A_loss: 0.1117, D_B_loss: 0.0808, G_A_loss: 0.8774, G_B_loss: 0.5106\n",
      "Epoch [27/30], Step [871/1067], D_A_loss: 0.0213, D_B_loss: 0.1379, G_A_loss: 0.4881, G_B_loss: 1.0506\n",
      "Epoch [27/30], Step [881/1067], D_A_loss: 0.2141, D_B_loss: 0.1019, G_A_loss: 0.4032, G_B_loss: 0.4608\n",
      "Epoch [27/30], Step [891/1067], D_A_loss: 0.1205, D_B_loss: 0.0478, G_A_loss: 0.6471, G_B_loss: 0.4503\n",
      "Epoch [27/30], Step [901/1067], D_A_loss: 0.0868, D_B_loss: 0.0621, G_A_loss: 0.8775, G_B_loss: 0.4925\n",
      "Epoch [27/30], Step [911/1067], D_A_loss: 0.1377, D_B_loss: 0.0237, G_A_loss: 0.5574, G_B_loss: 0.6166\n",
      "Epoch [27/30], Step [921/1067], D_A_loss: 0.0662, D_B_loss: 0.0421, G_A_loss: 0.6024, G_B_loss: 0.2679\n",
      "Epoch [27/30], Step [931/1067], D_A_loss: 0.2968, D_B_loss: 0.0413, G_A_loss: 0.7676, G_B_loss: 0.1436\n",
      "Epoch [27/30], Step [941/1067], D_A_loss: 0.1790, D_B_loss: 0.0385, G_A_loss: 0.3687, G_B_loss: 0.5586\n",
      "Epoch [27/30], Step [951/1067], D_A_loss: 0.2020, D_B_loss: 0.0123, G_A_loss: 0.5459, G_B_loss: 0.3554\n",
      "Epoch [27/30], Step [961/1067], D_A_loss: 0.0486, D_B_loss: 0.0368, G_A_loss: 0.7305, G_B_loss: 0.3040\n",
      "Epoch [27/30], Step [971/1067], D_A_loss: 0.0964, D_B_loss: 0.0224, G_A_loss: 0.8800, G_B_loss: 0.5736\n",
      "Epoch [27/30], Step [981/1067], D_A_loss: 0.0270, D_B_loss: 0.0631, G_A_loss: 0.4902, G_B_loss: 0.4943\n",
      "Epoch [27/30], Step [991/1067], D_A_loss: 0.0637, D_B_loss: 0.0252, G_A_loss: 0.4545, G_B_loss: 0.5501\n",
      "Epoch [27/30], Step [1001/1067], D_A_loss: 0.0889, D_B_loss: 0.0376, G_A_loss: 0.8444, G_B_loss: 0.4858\n",
      "Epoch [27/30], Step [1011/1067], D_A_loss: 0.1614, D_B_loss: 0.0795, G_A_loss: 0.9766, G_B_loss: 0.4480\n",
      "Epoch [27/30], Step [1021/1067], D_A_loss: 0.0460, D_B_loss: 0.1146, G_A_loss: 0.3913, G_B_loss: 0.2366\n",
      "Epoch [27/30], Step [1031/1067], D_A_loss: 0.0882, D_B_loss: 0.1269, G_A_loss: 0.7534, G_B_loss: 0.6991\n",
      "Epoch [27/30], Step [1041/1067], D_A_loss: 0.0428, D_B_loss: 0.0790, G_A_loss: 1.1298, G_B_loss: 0.1724\n",
      "Epoch [27/30], Step [1051/1067], D_A_loss: 0.0434, D_B_loss: 0.1468, G_A_loss: 0.7104, G_B_loss: 0.4040\n",
      "Epoch [27/30], Step [1061/1067], D_A_loss: 0.0368, D_B_loss: 0.1602, G_A_loss: 0.5131, G_B_loss: 0.4221\n",
      "Epoch [28/30], Step [1/1067], D_A_loss: 0.3373, D_B_loss: 0.0552, G_A_loss: 1.2743, G_B_loss: 0.0809\n",
      "Epoch [28/30], Step [11/1067], D_A_loss: 0.1012, D_B_loss: 0.0340, G_A_loss: 0.7036, G_B_loss: 0.6947\n",
      "Epoch [28/30], Step [21/1067], D_A_loss: 0.2307, D_B_loss: 0.1760, G_A_loss: 0.5781, G_B_loss: 0.8121\n",
      "Epoch [28/30], Step [31/1067], D_A_loss: 0.3122, D_B_loss: 0.0777, G_A_loss: 0.4970, G_B_loss: 0.7591\n",
      "Epoch [28/30], Step [41/1067], D_A_loss: 0.2417, D_B_loss: 0.0134, G_A_loss: 0.8816, G_B_loss: 0.2799\n",
      "Epoch [28/30], Step [51/1067], D_A_loss: 0.1204, D_B_loss: 0.0647, G_A_loss: 0.4978, G_B_loss: 1.7002\n",
      "Epoch [28/30], Step [61/1067], D_A_loss: 0.3019, D_B_loss: 0.0628, G_A_loss: 0.5375, G_B_loss: 0.3661\n",
      "Epoch [28/30], Step [71/1067], D_A_loss: 0.1253, D_B_loss: 0.0617, G_A_loss: 0.6999, G_B_loss: 0.3700\n",
      "Epoch [28/30], Step [81/1067], D_A_loss: 0.1148, D_B_loss: 0.0391, G_A_loss: 0.7194, G_B_loss: 0.9533\n",
      "Epoch [28/30], Step [91/1067], D_A_loss: 0.2202, D_B_loss: 0.1137, G_A_loss: 0.3843, G_B_loss: 0.4178\n",
      "Epoch [28/30], Step [101/1067], D_A_loss: 0.3327, D_B_loss: 0.0228, G_A_loss: 0.9368, G_B_loss: 0.9658\n",
      "Epoch [28/30], Step [111/1067], D_A_loss: 0.1417, D_B_loss: 0.0847, G_A_loss: 0.7102, G_B_loss: 0.2882\n",
      "Epoch [28/30], Step [121/1067], D_A_loss: 0.0405, D_B_loss: 0.0668, G_A_loss: 0.9198, G_B_loss: 0.7239\n",
      "Epoch [28/30], Step [131/1067], D_A_loss: 0.0386, D_B_loss: 0.0305, G_A_loss: 0.2374, G_B_loss: 0.3826\n",
      "Epoch [28/30], Step [141/1067], D_A_loss: 0.1953, D_B_loss: 0.0200, G_A_loss: 0.8363, G_B_loss: 0.7991\n",
      "Epoch [28/30], Step [151/1067], D_A_loss: 0.0644, D_B_loss: 0.0343, G_A_loss: 0.9543, G_B_loss: 0.3518\n",
      "Epoch [28/30], Step [161/1067], D_A_loss: 0.0803, D_B_loss: 0.0236, G_A_loss: 0.3899, G_B_loss: 0.9720\n",
      "Epoch [28/30], Step [171/1067], D_A_loss: 0.1571, D_B_loss: 0.0993, G_A_loss: 0.7007, G_B_loss: 0.6732\n",
      "Epoch [28/30], Step [181/1067], D_A_loss: 0.2069, D_B_loss: 0.0865, G_A_loss: 0.4211, G_B_loss: 0.2343\n",
      "Epoch [28/30], Step [191/1067], D_A_loss: 0.1211, D_B_loss: 0.2916, G_A_loss: 0.2199, G_B_loss: 0.2064\n",
      "Epoch [28/30], Step [201/1067], D_A_loss: 0.0819, D_B_loss: 0.0651, G_A_loss: 0.5577, G_B_loss: 0.1354\n",
      "Epoch [28/30], Step [211/1067], D_A_loss: 0.0738, D_B_loss: 0.0238, G_A_loss: 0.5427, G_B_loss: 0.1957\n",
      "Epoch [28/30], Step [221/1067], D_A_loss: 0.1302, D_B_loss: 0.0767, G_A_loss: 0.6207, G_B_loss: 0.3569\n",
      "Epoch [28/30], Step [231/1067], D_A_loss: 0.0389, D_B_loss: 0.0286, G_A_loss: 0.7239, G_B_loss: 0.3172\n",
      "Epoch [28/30], Step [241/1067], D_A_loss: 0.1059, D_B_loss: 0.0244, G_A_loss: 0.8115, G_B_loss: 0.3992\n",
      "Epoch [28/30], Step [251/1067], D_A_loss: 0.1628, D_B_loss: 0.0343, G_A_loss: 0.7446, G_B_loss: 0.2791\n",
      "Epoch [28/30], Step [261/1067], D_A_loss: 0.2039, D_B_loss: 0.0791, G_A_loss: 0.7186, G_B_loss: 0.4712\n",
      "Epoch [28/30], Step [271/1067], D_A_loss: 0.1699, D_B_loss: 0.0171, G_A_loss: 0.3833, G_B_loss: 0.2975\n",
      "Epoch [28/30], Step [281/1067], D_A_loss: 0.0763, D_B_loss: 0.0284, G_A_loss: 0.7233, G_B_loss: 0.7698\n",
      "Epoch [28/30], Step [291/1067], D_A_loss: 0.1539, D_B_loss: 0.3882, G_A_loss: 1.2444, G_B_loss: 0.3529\n",
      "Epoch [28/30], Step [301/1067], D_A_loss: 0.1444, D_B_loss: 0.0372, G_A_loss: 0.9276, G_B_loss: 0.3259\n",
      "Epoch [28/30], Step [311/1067], D_A_loss: 0.2686, D_B_loss: 0.0824, G_A_loss: 0.5878, G_B_loss: 0.4477\n",
      "Epoch [28/30], Step [321/1067], D_A_loss: 0.0469, D_B_loss: 0.0169, G_A_loss: 0.6187, G_B_loss: 0.5115\n",
      "Epoch [28/30], Step [331/1067], D_A_loss: 0.1253, D_B_loss: 0.0244, G_A_loss: 0.7973, G_B_loss: 0.3682\n",
      "Epoch [28/30], Step [341/1067], D_A_loss: 0.0555, D_B_loss: 0.0217, G_A_loss: 0.6167, G_B_loss: 0.5207\n",
      "Epoch [28/30], Step [351/1067], D_A_loss: 0.1447, D_B_loss: 0.0245, G_A_loss: 0.5548, G_B_loss: 0.2879\n",
      "Epoch [28/30], Step [361/1067], D_A_loss: 0.1621, D_B_loss: 0.0281, G_A_loss: 0.5645, G_B_loss: 0.3114\n",
      "Epoch [28/30], Step [371/1067], D_A_loss: 0.0282, D_B_loss: 0.0548, G_A_loss: 0.7764, G_B_loss: 0.3650\n",
      "Epoch [28/30], Step [381/1067], D_A_loss: 0.4700, D_B_loss: 0.0125, G_A_loss: 0.2995, G_B_loss: 0.0283\n",
      "Epoch [28/30], Step [391/1067], D_A_loss: 0.0377, D_B_loss: 0.0316, G_A_loss: 0.5182, G_B_loss: 0.7963\n",
      "Epoch [28/30], Step [401/1067], D_A_loss: 0.1011, D_B_loss: 0.0553, G_A_loss: 0.8714, G_B_loss: 0.9566\n",
      "Epoch [28/30], Step [411/1067], D_A_loss: 0.1625, D_B_loss: 0.0576, G_A_loss: 1.0752, G_B_loss: 0.3043\n",
      "Epoch [28/30], Step [421/1067], D_A_loss: 0.0379, D_B_loss: 0.0225, G_A_loss: 0.6358, G_B_loss: 0.6678\n",
      "Epoch [28/30], Step [431/1067], D_A_loss: 0.1263, D_B_loss: 0.0785, G_A_loss: 0.9162, G_B_loss: 0.6769\n",
      "Epoch [28/30], Step [441/1067], D_A_loss: 0.0297, D_B_loss: 0.0419, G_A_loss: 0.6428, G_B_loss: 1.0572\n",
      "Epoch [28/30], Step [451/1067], D_A_loss: 0.0591, D_B_loss: 0.0793, G_A_loss: 0.9178, G_B_loss: 0.3401\n",
      "Epoch [28/30], Step [461/1067], D_A_loss: 0.0326, D_B_loss: 0.0600, G_A_loss: 0.5113, G_B_loss: 0.4714\n",
      "Epoch [28/30], Step [471/1067], D_A_loss: 0.1923, D_B_loss: 0.0570, G_A_loss: 0.6663, G_B_loss: 0.2027\n",
      "Epoch [28/30], Step [481/1067], D_A_loss: 0.1095, D_B_loss: 0.0129, G_A_loss: 0.4466, G_B_loss: 0.9294\n",
      "Epoch [28/30], Step [491/1067], D_A_loss: 0.0621, D_B_loss: 0.0414, G_A_loss: 0.5549, G_B_loss: 0.2803\n",
      "Epoch [28/30], Step [501/1067], D_A_loss: 0.0409, D_B_loss: 0.0959, G_A_loss: 0.3930, G_B_loss: 0.1922\n",
      "Epoch [28/30], Step [511/1067], D_A_loss: 0.1951, D_B_loss: 0.0346, G_A_loss: 1.2767, G_B_loss: 0.3444\n",
      "Epoch [28/30], Step [521/1067], D_A_loss: 0.0658, D_B_loss: 0.0368, G_A_loss: 1.1883, G_B_loss: 0.5153\n",
      "Epoch [28/30], Step [531/1067], D_A_loss: 0.1271, D_B_loss: 0.0899, G_A_loss: 0.5591, G_B_loss: 0.3084\n",
      "Epoch [28/30], Step [541/1067], D_A_loss: 0.0578, D_B_loss: 0.0270, G_A_loss: 0.7286, G_B_loss: 0.8248\n",
      "Epoch [28/30], Step [551/1067], D_A_loss: 0.3533, D_B_loss: 0.0886, G_A_loss: 0.4058, G_B_loss: 1.0806\n",
      "Epoch [28/30], Step [561/1067], D_A_loss: 0.0297, D_B_loss: 0.0135, G_A_loss: 0.5914, G_B_loss: 0.7094\n",
      "Epoch [28/30], Step [571/1067], D_A_loss: 0.0687, D_B_loss: 0.0534, G_A_loss: 0.5338, G_B_loss: 0.6663\n",
      "Epoch [28/30], Step [581/1067], D_A_loss: 0.1048, D_B_loss: 0.0414, G_A_loss: 0.4789, G_B_loss: 0.4849\n",
      "Epoch [28/30], Step [591/1067], D_A_loss: 0.0745, D_B_loss: 0.0335, G_A_loss: 0.7115, G_B_loss: 0.8704\n",
      "Epoch [28/30], Step [601/1067], D_A_loss: 0.1419, D_B_loss: 0.0178, G_A_loss: 0.7997, G_B_loss: 0.4095\n",
      "Epoch [28/30], Step [611/1067], D_A_loss: 0.0639, D_B_loss: 0.0367, G_A_loss: 0.8126, G_B_loss: 0.7041\n",
      "Epoch [28/30], Step [621/1067], D_A_loss: 0.0698, D_B_loss: 0.1127, G_A_loss: 0.3300, G_B_loss: 0.6530\n",
      "Epoch [28/30], Step [631/1067], D_A_loss: 0.1591, D_B_loss: 0.1172, G_A_loss: 0.2450, G_B_loss: 0.2885\n",
      "Epoch [28/30], Step [641/1067], D_A_loss: 0.0616, D_B_loss: 0.1190, G_A_loss: 0.4149, G_B_loss: 0.5995\n",
      "Epoch [28/30], Step [651/1067], D_A_loss: 0.0449, D_B_loss: 0.0151, G_A_loss: 0.5035, G_B_loss: 0.1366\n",
      "Epoch [28/30], Step [661/1067], D_A_loss: 0.0504, D_B_loss: 0.0796, G_A_loss: 0.7652, G_B_loss: 0.3367\n",
      "Epoch [28/30], Step [671/1067], D_A_loss: 0.1604, D_B_loss: 0.0543, G_A_loss: 0.9573, G_B_loss: 0.2668\n",
      "Epoch [28/30], Step [681/1067], D_A_loss: 0.1130, D_B_loss: 0.0478, G_A_loss: 0.6864, G_B_loss: 0.7477\n",
      "Epoch [28/30], Step [691/1067], D_A_loss: 0.1119, D_B_loss: 0.0273, G_A_loss: 0.8058, G_B_loss: 0.5625\n",
      "Epoch [28/30], Step [701/1067], D_A_loss: 0.0516, D_B_loss: 0.0710, G_A_loss: 0.4676, G_B_loss: 0.6442\n",
      "Epoch [28/30], Step [711/1067], D_A_loss: 0.1654, D_B_loss: 0.0336, G_A_loss: 1.0039, G_B_loss: 0.7109\n",
      "Epoch [28/30], Step [721/1067], D_A_loss: 0.0571, D_B_loss: 0.0499, G_A_loss: 0.7783, G_B_loss: 0.5398\n",
      "Epoch [28/30], Step [731/1067], D_A_loss: 0.0893, D_B_loss: 0.0410, G_A_loss: 0.8474, G_B_loss: 0.4892\n",
      "Epoch [28/30], Step [741/1067], D_A_loss: 0.1203, D_B_loss: 0.2114, G_A_loss: 0.5698, G_B_loss: 0.2909\n",
      "Epoch [28/30], Step [751/1067], D_A_loss: 0.1768, D_B_loss: 0.0294, G_A_loss: 0.8011, G_B_loss: 0.6501\n",
      "Epoch [28/30], Step [761/1067], D_A_loss: 0.1187, D_B_loss: 0.0239, G_A_loss: 0.8507, G_B_loss: 0.4290\n",
      "Epoch [28/30], Step [771/1067], D_A_loss: 0.0643, D_B_loss: 0.1462, G_A_loss: 0.6677, G_B_loss: 0.3067\n",
      "Epoch [28/30], Step [781/1067], D_A_loss: 0.0526, D_B_loss: 0.1047, G_A_loss: 0.4890, G_B_loss: 0.1485\n",
      "Epoch [28/30], Step [791/1067], D_A_loss: 0.0805, D_B_loss: 0.0827, G_A_loss: 0.6471, G_B_loss: 0.6091\n",
      "Epoch [28/30], Step [801/1067], D_A_loss: 0.2868, D_B_loss: 0.1163, G_A_loss: 0.5176, G_B_loss: 0.1451\n",
      "Epoch [28/30], Step [811/1067], D_A_loss: 0.0725, D_B_loss: 0.0699, G_A_loss: 0.3225, G_B_loss: 0.4798\n",
      "Epoch [28/30], Step [821/1067], D_A_loss: 0.0478, D_B_loss: 0.0807, G_A_loss: 0.9824, G_B_loss: 0.6849\n",
      "Epoch [28/30], Step [831/1067], D_A_loss: 0.1476, D_B_loss: 0.0239, G_A_loss: 0.7307, G_B_loss: 0.7532\n",
      "Epoch [28/30], Step [841/1067], D_A_loss: 0.1161, D_B_loss: 0.0429, G_A_loss: 0.4172, G_B_loss: 0.5460\n",
      "Epoch [28/30], Step [851/1067], D_A_loss: 0.1808, D_B_loss: 0.1750, G_A_loss: 0.9162, G_B_loss: 0.7146\n",
      "Epoch [28/30], Step [861/1067], D_A_loss: 0.2772, D_B_loss: 0.0992, G_A_loss: 0.5938, G_B_loss: 0.3634\n",
      "Epoch [28/30], Step [871/1067], D_A_loss: 0.0182, D_B_loss: 0.1127, G_A_loss: 1.3603, G_B_loss: 0.4143\n",
      "Epoch [28/30], Step [881/1067], D_A_loss: 0.0723, D_B_loss: 0.0411, G_A_loss: 0.5789, G_B_loss: 0.8722\n",
      "Epoch [28/30], Step [891/1067], D_A_loss: 0.0429, D_B_loss: 0.1266, G_A_loss: 0.7368, G_B_loss: 0.5378\n",
      "Epoch [28/30], Step [901/1067], D_A_loss: 0.0986, D_B_loss: 0.0827, G_A_loss: 0.4345, G_B_loss: 0.3979\n",
      "Epoch [28/30], Step [911/1067], D_A_loss: 0.2980, D_B_loss: 0.0189, G_A_loss: 0.6956, G_B_loss: 0.3783\n",
      "Epoch [28/30], Step [921/1067], D_A_loss: 0.0768, D_B_loss: 0.0619, G_A_loss: 0.5699, G_B_loss: 0.6489\n",
      "Epoch [28/30], Step [931/1067], D_A_loss: 0.0351, D_B_loss: 0.0719, G_A_loss: 1.0737, G_B_loss: 0.4736\n",
      "Epoch [28/30], Step [941/1067], D_A_loss: 0.1251, D_B_loss: 0.1467, G_A_loss: 0.7605, G_B_loss: 1.1226\n",
      "Epoch [28/30], Step [951/1067], D_A_loss: 0.0852, D_B_loss: 0.0493, G_A_loss: 0.5596, G_B_loss: 0.3868\n",
      "Epoch [28/30], Step [961/1067], D_A_loss: 0.0904, D_B_loss: 0.0242, G_A_loss: 0.6177, G_B_loss: 0.4405\n",
      "Epoch [28/30], Step [971/1067], D_A_loss: 0.1972, D_B_loss: 0.0617, G_A_loss: 0.4798, G_B_loss: 0.4558\n",
      "Epoch [28/30], Step [981/1067], D_A_loss: 0.3024, D_B_loss: 0.0273, G_A_loss: 0.7937, G_B_loss: 0.2799\n",
      "Epoch [28/30], Step [991/1067], D_A_loss: 0.0870, D_B_loss: 0.0230, G_A_loss: 0.9340, G_B_loss: 0.4321\n",
      "Epoch [28/30], Step [1001/1067], D_A_loss: 0.0419, D_B_loss: 0.0523, G_A_loss: 0.6146, G_B_loss: 0.5797\n",
      "Epoch [28/30], Step [1011/1067], D_A_loss: 0.1094, D_B_loss: 0.0324, G_A_loss: 0.5572, G_B_loss: 0.5474\n",
      "Epoch [28/30], Step [1021/1067], D_A_loss: 0.1795, D_B_loss: 0.1146, G_A_loss: 0.3924, G_B_loss: 0.2649\n",
      "Epoch [28/30], Step [1031/1067], D_A_loss: 0.3840, D_B_loss: 0.1078, G_A_loss: 0.4583, G_B_loss: 1.1105\n",
      "Epoch [28/30], Step [1041/1067], D_A_loss: 0.1406, D_B_loss: 0.0202, G_A_loss: 0.7295, G_B_loss: 0.5598\n",
      "Epoch [28/30], Step [1051/1067], D_A_loss: 0.0711, D_B_loss: 0.1154, G_A_loss: 1.5647, G_B_loss: 0.4993\n",
      "Epoch [28/30], Step [1061/1067], D_A_loss: 0.1206, D_B_loss: 0.0980, G_A_loss: 0.3912, G_B_loss: 0.5252\n",
      "Epoch [29/30], Step [1/1067], D_A_loss: 0.0483, D_B_loss: 0.0886, G_A_loss: 0.4007, G_B_loss: 0.7373\n",
      "Epoch [29/30], Step [11/1067], D_A_loss: 0.1171, D_B_loss: 0.0665, G_A_loss: 0.4591, G_B_loss: 0.9301\n",
      "Epoch [29/30], Step [21/1067], D_A_loss: 0.0318, D_B_loss: 0.0468, G_A_loss: 0.9641, G_B_loss: 0.4284\n",
      "Epoch [29/30], Step [31/1067], D_A_loss: 0.0951, D_B_loss: 0.0179, G_A_loss: 0.6480, G_B_loss: 0.4119\n",
      "Epoch [29/30], Step [41/1067], D_A_loss: 0.0391, D_B_loss: 0.0279, G_A_loss: 0.7887, G_B_loss: 0.4966\n",
      "Epoch [29/30], Step [51/1067], D_A_loss: 0.0622, D_B_loss: 0.0204, G_A_loss: 0.7699, G_B_loss: 0.2216\n",
      "Epoch [29/30], Step [61/1067], D_A_loss: 0.1580, D_B_loss: 0.0155, G_A_loss: 0.9947, G_B_loss: 0.8713\n",
      "Epoch [29/30], Step [71/1067], D_A_loss: 0.1251, D_B_loss: 0.0559, G_A_loss: 0.5341, G_B_loss: 0.2863\n",
      "Epoch [29/30], Step [81/1067], D_A_loss: 0.1017, D_B_loss: 0.0288, G_A_loss: 0.7057, G_B_loss: 0.4584\n",
      "Epoch [29/30], Step [91/1067], D_A_loss: 0.0930, D_B_loss: 0.0447, G_A_loss: 1.0581, G_B_loss: 0.3227\n",
      "Epoch [29/30], Step [101/1067], D_A_loss: 0.1509, D_B_loss: 0.0329, G_A_loss: 0.8553, G_B_loss: 0.8129\n",
      "Epoch [29/30], Step [111/1067], D_A_loss: 0.0724, D_B_loss: 0.0383, G_A_loss: 0.7106, G_B_loss: 0.5671\n",
      "Epoch [29/30], Step [121/1067], D_A_loss: 0.0300, D_B_loss: 0.2217, G_A_loss: 0.5974, G_B_loss: 0.4031\n",
      "Epoch [29/30], Step [131/1067], D_A_loss: 0.1235, D_B_loss: 0.0294, G_A_loss: 0.3453, G_B_loss: 0.3406\n",
      "Epoch [29/30], Step [141/1067], D_A_loss: 0.1274, D_B_loss: 0.0234, G_A_loss: 0.7927, G_B_loss: 0.8658\n",
      "Epoch [29/30], Step [151/1067], D_A_loss: 0.0733, D_B_loss: 0.3801, G_A_loss: 0.0921, G_B_loss: 0.5730\n",
      "Epoch [29/30], Step [161/1067], D_A_loss: 0.1261, D_B_loss: 0.0331, G_A_loss: 0.6827, G_B_loss: 0.8544\n",
      "Epoch [29/30], Step [171/1067], D_A_loss: 0.1407, D_B_loss: 0.2221, G_A_loss: 0.1759, G_B_loss: 0.3938\n",
      "Epoch [29/30], Step [181/1067], D_A_loss: 0.2351, D_B_loss: 0.1160, G_A_loss: 0.7328, G_B_loss: 0.1672\n",
      "Epoch [29/30], Step [191/1067], D_A_loss: 0.0665, D_B_loss: 0.0415, G_A_loss: 0.3736, G_B_loss: 0.3611\n",
      "Epoch [29/30], Step [201/1067], D_A_loss: 0.0896, D_B_loss: 0.0493, G_A_loss: 0.3917, G_B_loss: 0.3723\n",
      "Epoch [29/30], Step [211/1067], D_A_loss: 0.0455, D_B_loss: 0.0830, G_A_loss: 0.5641, G_B_loss: 0.3239\n",
      "Epoch [29/30], Step [221/1067], D_A_loss: 0.1172, D_B_loss: 0.0823, G_A_loss: 0.6618, G_B_loss: 0.5027\n",
      "Epoch [29/30], Step [231/1067], D_A_loss: 0.2846, D_B_loss: 0.0544, G_A_loss: 0.8867, G_B_loss: 0.6705\n",
      "Epoch [29/30], Step [241/1067], D_A_loss: 0.0649, D_B_loss: 0.0297, G_A_loss: 1.2680, G_B_loss: 0.4693\n",
      "Epoch [29/30], Step [251/1067], D_A_loss: 0.1479, D_B_loss: 0.0949, G_A_loss: 0.8728, G_B_loss: 0.7979\n",
      "Epoch [29/30], Step [261/1067], D_A_loss: 0.1311, D_B_loss: 0.2468, G_A_loss: 0.2117, G_B_loss: 0.6231\n",
      "Epoch [29/30], Step [271/1067], D_A_loss: 0.1036, D_B_loss: 0.0619, G_A_loss: 0.7118, G_B_loss: 0.8145\n",
      "Epoch [29/30], Step [281/1067], D_A_loss: 0.0809, D_B_loss: 0.0428, G_A_loss: 0.8916, G_B_loss: 0.8355\n",
      "Epoch [29/30], Step [291/1067], D_A_loss: 0.1220, D_B_loss: 0.0579, G_A_loss: 0.6120, G_B_loss: 0.7366\n",
      "Epoch [29/30], Step [301/1067], D_A_loss: 0.2546, D_B_loss: 0.0808, G_A_loss: 0.3093, G_B_loss: 1.1024\n",
      "Epoch [29/30], Step [311/1067], D_A_loss: 0.0896, D_B_loss: 0.2413, G_A_loss: 1.0047, G_B_loss: 0.5224\n",
      "Epoch [29/30], Step [321/1067], D_A_loss: 0.0637, D_B_loss: 0.0131, G_A_loss: 1.1205, G_B_loss: 0.7392\n",
      "Epoch [29/30], Step [331/1067], D_A_loss: 0.0412, D_B_loss: 0.0579, G_A_loss: 0.5679, G_B_loss: 0.4346\n",
      "Epoch [29/30], Step [341/1067], D_A_loss: 0.1280, D_B_loss: 0.0967, G_A_loss: 0.3502, G_B_loss: 0.4453\n",
      "Epoch [29/30], Step [351/1067], D_A_loss: 0.2516, D_B_loss: 0.1339, G_A_loss: 0.8226, G_B_loss: 1.2238\n",
      "Epoch [29/30], Step [361/1067], D_A_loss: 0.1758, D_B_loss: 0.0803, G_A_loss: 0.6930, G_B_loss: 0.9495\n",
      "Epoch [29/30], Step [371/1067], D_A_loss: 0.0598, D_B_loss: 0.3405, G_A_loss: 0.1419, G_B_loss: 0.3990\n",
      "Epoch [29/30], Step [381/1067], D_A_loss: 0.3007, D_B_loss: 0.1072, G_A_loss: 0.5503, G_B_loss: 0.1109\n",
      "Epoch [29/30], Step [391/1067], D_A_loss: 0.1780, D_B_loss: 0.0298, G_A_loss: 0.7077, G_B_loss: 0.4441\n",
      "Epoch [29/30], Step [401/1067], D_A_loss: 0.0899, D_B_loss: 0.0188, G_A_loss: 0.9075, G_B_loss: 0.4834\n",
      "Epoch [29/30], Step [411/1067], D_A_loss: 0.0507, D_B_loss: 0.0605, G_A_loss: 0.4752, G_B_loss: 0.5696\n",
      "Epoch [29/30], Step [421/1067], D_A_loss: 0.1199, D_B_loss: 0.0287, G_A_loss: 0.6702, G_B_loss: 0.5222\n",
      "Epoch [29/30], Step [431/1067], D_A_loss: 0.1271, D_B_loss: 0.0492, G_A_loss: 0.9413, G_B_loss: 0.5916\n",
      "Epoch [29/30], Step [441/1067], D_A_loss: 0.1346, D_B_loss: 0.1882, G_A_loss: 0.3155, G_B_loss: 0.5772\n",
      "Epoch [29/30], Step [451/1067], D_A_loss: 0.2678, D_B_loss: 0.0224, G_A_loss: 0.3298, G_B_loss: 0.6888\n",
      "Epoch [29/30], Step [461/1067], D_A_loss: 0.1154, D_B_loss: 0.0559, G_A_loss: 0.5954, G_B_loss: 0.4383\n",
      "Epoch [29/30], Step [471/1067], D_A_loss: 0.2260, D_B_loss: 0.0626, G_A_loss: 0.9600, G_B_loss: 0.6893\n",
      "Epoch [29/30], Step [481/1067], D_A_loss: 0.1496, D_B_loss: 0.3048, G_A_loss: 1.0447, G_B_loss: 0.8021\n",
      "Epoch [29/30], Step [491/1067], D_A_loss: 0.3098, D_B_loss: 0.0816, G_A_loss: 0.5740, G_B_loss: 0.6879\n",
      "Epoch [29/30], Step [501/1067], D_A_loss: 0.1047, D_B_loss: 0.0253, G_A_loss: 0.7503, G_B_loss: 0.3008\n",
      "Epoch [29/30], Step [511/1067], D_A_loss: 0.0918, D_B_loss: 0.1319, G_A_loss: 1.2638, G_B_loss: 0.4111\n",
      "Epoch [29/30], Step [521/1067], D_A_loss: 0.0779, D_B_loss: 0.0329, G_A_loss: 0.8659, G_B_loss: 0.8517\n",
      "Epoch [29/30], Step [531/1067], D_A_loss: 0.2073, D_B_loss: 0.0385, G_A_loss: 0.4748, G_B_loss: 0.1812\n",
      "Epoch [29/30], Step [541/1067], D_A_loss: 0.1316, D_B_loss: 0.0436, G_A_loss: 0.5967, G_B_loss: 0.5255\n",
      "Epoch [29/30], Step [551/1067], D_A_loss: 0.0270, D_B_loss: 0.0878, G_A_loss: 0.9568, G_B_loss: 0.8611\n",
      "Epoch [29/30], Step [561/1067], D_A_loss: 0.0841, D_B_loss: 0.1706, G_A_loss: 1.0404, G_B_loss: 0.2492\n",
      "Epoch [29/30], Step [571/1067], D_A_loss: 0.1770, D_B_loss: 0.0857, G_A_loss: 0.6336, G_B_loss: 0.3433\n",
      "Epoch [29/30], Step [581/1067], D_A_loss: 0.2199, D_B_loss: 0.1485, G_A_loss: 0.3428, G_B_loss: 0.1769\n",
      "Epoch [29/30], Step [591/1067], D_A_loss: 0.0758, D_B_loss: 0.0381, G_A_loss: 0.4680, G_B_loss: 0.5032\n",
      "Epoch [29/30], Step [601/1067], D_A_loss: 0.0449, D_B_loss: 0.1263, G_A_loss: 0.4167, G_B_loss: 0.3802\n",
      "Epoch [29/30], Step [611/1067], D_A_loss: 0.0682, D_B_loss: 0.0168, G_A_loss: 0.4138, G_B_loss: 0.7717\n",
      "Epoch [29/30], Step [621/1067], D_A_loss: 0.1411, D_B_loss: 0.1651, G_A_loss: 0.8585, G_B_loss: 0.1169\n",
      "Epoch [29/30], Step [631/1067], D_A_loss: 0.0292, D_B_loss: 0.0422, G_A_loss: 0.7424, G_B_loss: 0.4256\n",
      "Epoch [29/30], Step [641/1067], D_A_loss: 0.0426, D_B_loss: 0.0601, G_A_loss: 0.8724, G_B_loss: 0.7147\n",
      "Epoch [29/30], Step [651/1067], D_A_loss: 0.3190, D_B_loss: 0.0721, G_A_loss: 0.5478, G_B_loss: 0.0928\n",
      "Epoch [29/30], Step [661/1067], D_A_loss: 0.1950, D_B_loss: 0.0536, G_A_loss: 0.8204, G_B_loss: 0.2095\n",
      "Epoch [29/30], Step [671/1067], D_A_loss: 0.1652, D_B_loss: 0.0170, G_A_loss: 0.6199, G_B_loss: 0.2213\n",
      "Epoch [29/30], Step [681/1067], D_A_loss: 0.1416, D_B_loss: 0.0303, G_A_loss: 0.7203, G_B_loss: 0.4768\n",
      "Epoch [29/30], Step [691/1067], D_A_loss: 0.0784, D_B_loss: 0.0186, G_A_loss: 1.2210, G_B_loss: 0.5359\n",
      "Epoch [29/30], Step [701/1067], D_A_loss: 0.0663, D_B_loss: 0.0842, G_A_loss: 0.7661, G_B_loss: 0.5563\n",
      "Epoch [29/30], Step [711/1067], D_A_loss: 0.0441, D_B_loss: 0.0695, G_A_loss: 0.5677, G_B_loss: 0.6368\n",
      "Epoch [29/30], Step [721/1067], D_A_loss: 0.0781, D_B_loss: 0.0235, G_A_loss: 0.3695, G_B_loss: 0.3217\n",
      "Epoch [29/30], Step [731/1067], D_A_loss: 0.0481, D_B_loss: 0.1211, G_A_loss: 0.6701, G_B_loss: 0.6329\n",
      "Epoch [29/30], Step [741/1067], D_A_loss: 0.1412, D_B_loss: 0.0970, G_A_loss: 0.4964, G_B_loss: 0.3651\n",
      "Epoch [29/30], Step [751/1067], D_A_loss: 0.0968, D_B_loss: 0.1093, G_A_loss: 0.4583, G_B_loss: 0.4742\n",
      "Epoch [29/30], Step [761/1067], D_A_loss: 0.1686, D_B_loss: 0.0627, G_A_loss: 0.8085, G_B_loss: 0.2293\n",
      "Epoch [29/30], Step [771/1067], D_A_loss: 0.0343, D_B_loss: 0.0351, G_A_loss: 0.7059, G_B_loss: 0.1928\n",
      "Epoch [29/30], Step [781/1067], D_A_loss: 0.1141, D_B_loss: 0.1276, G_A_loss: 0.7319, G_B_loss: 0.6304\n",
      "Epoch [29/30], Step [791/1067], D_A_loss: 0.2209, D_B_loss: 0.0698, G_A_loss: 1.0192, G_B_loss: 0.5313\n",
      "Epoch [29/30], Step [801/1067], D_A_loss: 0.0505, D_B_loss: 0.0332, G_A_loss: 0.5564, G_B_loss: 0.4772\n",
      "Epoch [29/30], Step [811/1067], D_A_loss: 0.2051, D_B_loss: 0.0491, G_A_loss: 0.7896, G_B_loss: 0.2338\n",
      "Epoch [29/30], Step [821/1067], D_A_loss: 0.1621, D_B_loss: 0.0811, G_A_loss: 1.3486, G_B_loss: 0.2395\n",
      "Epoch [29/30], Step [831/1067], D_A_loss: 0.2101, D_B_loss: 0.1633, G_A_loss: 0.6274, G_B_loss: 0.2002\n",
      "Epoch [29/30], Step [841/1067], D_A_loss: 0.2366, D_B_loss: 0.0312, G_A_loss: 0.7355, G_B_loss: 0.4387\n",
      "Epoch [29/30], Step [851/1067], D_A_loss: 0.0943, D_B_loss: 0.0165, G_A_loss: 1.1006, G_B_loss: 0.3849\n",
      "Epoch [29/30], Step [861/1067], D_A_loss: 0.1033, D_B_loss: 0.0760, G_A_loss: 0.8853, G_B_loss: 0.8587\n",
      "Epoch [29/30], Step [871/1067], D_A_loss: 0.1804, D_B_loss: 0.0245, G_A_loss: 1.2143, G_B_loss: 0.6903\n",
      "Epoch [29/30], Step [881/1067], D_A_loss: 0.1159, D_B_loss: 0.0199, G_A_loss: 0.6584, G_B_loss: 0.3690\n",
      "Epoch [29/30], Step [891/1067], D_A_loss: 0.0254, D_B_loss: 0.0446, G_A_loss: 0.6142, G_B_loss: 0.9166\n",
      "Epoch [29/30], Step [901/1067], D_A_loss: 0.1382, D_B_loss: 0.0359, G_A_loss: 0.5703, G_B_loss: 0.4458\n",
      "Epoch [29/30], Step [911/1067], D_A_loss: 0.1227, D_B_loss: 0.0217, G_A_loss: 0.6413, G_B_loss: 0.4474\n",
      "Epoch [29/30], Step [921/1067], D_A_loss: 0.0282, D_B_loss: 0.0870, G_A_loss: 0.3987, G_B_loss: 0.9067\n",
      "Epoch [29/30], Step [931/1067], D_A_loss: 0.0172, D_B_loss: 0.0581, G_A_loss: 0.5650, G_B_loss: 0.3750\n",
      "Epoch [29/30], Step [941/1067], D_A_loss: 0.0533, D_B_loss: 0.0266, G_A_loss: 0.5667, G_B_loss: 0.3923\n",
      "Epoch [29/30], Step [951/1067], D_A_loss: 0.0563, D_B_loss: 0.0840, G_A_loss: 1.0075, G_B_loss: 0.2007\n",
      "Epoch [29/30], Step [961/1067], D_A_loss: 0.1671, D_B_loss: 0.0203, G_A_loss: 0.7586, G_B_loss: 0.5312\n",
      "Epoch [29/30], Step [971/1067], D_A_loss: 0.0778, D_B_loss: 0.0485, G_A_loss: 0.7500, G_B_loss: 0.5337\n",
      "Epoch [29/30], Step [981/1067], D_A_loss: 0.1005, D_B_loss: 0.0516, G_A_loss: 1.0384, G_B_loss: 0.5469\n",
      "Epoch [29/30], Step [991/1067], D_A_loss: 0.0377, D_B_loss: 0.0180, G_A_loss: 0.9386, G_B_loss: 0.4368\n",
      "Epoch [29/30], Step [1001/1067], D_A_loss: 0.0320, D_B_loss: 0.1221, G_A_loss: 1.0836, G_B_loss: 0.3186\n",
      "Epoch [29/30], Step [1011/1067], D_A_loss: 0.1807, D_B_loss: 0.0264, G_A_loss: 0.7726, G_B_loss: 0.3589\n",
      "Epoch [29/30], Step [1021/1067], D_A_loss: 0.0477, D_B_loss: 0.0374, G_A_loss: 0.4676, G_B_loss: 0.8289\n",
      "Epoch [29/30], Step [1031/1067], D_A_loss: 0.4705, D_B_loss: 0.0180, G_A_loss: 0.6524, G_B_loss: 0.3221\n",
      "Epoch [29/30], Step [1041/1067], D_A_loss: 0.0558, D_B_loss: 0.0503, G_A_loss: 0.6104, G_B_loss: 0.7103\n",
      "Epoch [29/30], Step [1051/1067], D_A_loss: 0.0666, D_B_loss: 0.1707, G_A_loss: 0.2968, G_B_loss: 0.6163\n",
      "Epoch [29/30], Step [1061/1067], D_A_loss: 0.1099, D_B_loss: 0.0610, G_A_loss: 0.6832, G_B_loss: 0.4891\n",
      "Epoch [30/30], Step [1/1067], D_A_loss: 0.0580, D_B_loss: 0.0435, G_A_loss: 0.5852, G_B_loss: 0.7758\n",
      "Epoch [30/30], Step [11/1067], D_A_loss: 0.2604, D_B_loss: 0.0536, G_A_loss: 0.4919, G_B_loss: 0.2344\n",
      "Epoch [30/30], Step [21/1067], D_A_loss: 0.3532, D_B_loss: 0.0318, G_A_loss: 0.7527, G_B_loss: 0.1447\n",
      "Epoch [30/30], Step [31/1067], D_A_loss: 0.0224, D_B_loss: 0.0632, G_A_loss: 0.8894, G_B_loss: 0.6280\n",
      "Epoch [30/30], Step [41/1067], D_A_loss: 0.1710, D_B_loss: 0.0533, G_A_loss: 0.6099, G_B_loss: 0.4079\n",
      "Epoch [30/30], Step [51/1067], D_A_loss: 0.0947, D_B_loss: 0.0889, G_A_loss: 0.9062, G_B_loss: 0.5058\n",
      "Epoch [30/30], Step [61/1067], D_A_loss: 0.0906, D_B_loss: 0.0202, G_A_loss: 0.9020, G_B_loss: 0.2708\n",
      "Epoch [30/30], Step [71/1067], D_A_loss: 0.0594, D_B_loss: 0.0276, G_A_loss: 1.1138, G_B_loss: 0.5766\n",
      "Epoch [30/30], Step [81/1067], D_A_loss: 0.1833, D_B_loss: 0.0829, G_A_loss: 0.4595, G_B_loss: 0.2439\n",
      "Epoch [30/30], Step [91/1067], D_A_loss: 0.1098, D_B_loss: 0.0384, G_A_loss: 0.6535, G_B_loss: 0.4108\n",
      "Epoch [30/30], Step [101/1067], D_A_loss: 0.0566, D_B_loss: 0.0669, G_A_loss: 0.4185, G_B_loss: 0.5695\n",
      "Epoch [30/30], Step [111/1067], D_A_loss: 0.0454, D_B_loss: 0.1264, G_A_loss: 0.4705, G_B_loss: 0.2743\n",
      "Epoch [30/30], Step [121/1067], D_A_loss: 0.0775, D_B_loss: 0.0158, G_A_loss: 0.6154, G_B_loss: 0.6539\n",
      "Epoch [30/30], Step [131/1067], D_A_loss: 0.1890, D_B_loss: 0.0639, G_A_loss: 0.3825, G_B_loss: 0.2219\n",
      "Epoch [30/30], Step [141/1067], D_A_loss: 0.0607, D_B_loss: 0.2050, G_A_loss: 0.3779, G_B_loss: 0.5166\n",
      "Epoch [30/30], Step [151/1067], D_A_loss: 0.1425, D_B_loss: 0.0266, G_A_loss: 0.9312, G_B_loss: 0.4431\n",
      "Epoch [30/30], Step [161/1067], D_A_loss: 0.1528, D_B_loss: 0.0632, G_A_loss: 0.5426, G_B_loss: 0.5622\n",
      "Epoch [30/30], Step [171/1067], D_A_loss: 0.0387, D_B_loss: 0.0363, G_A_loss: 1.0325, G_B_loss: 0.6846\n",
      "Epoch [30/30], Step [181/1067], D_A_loss: 0.0651, D_B_loss: 0.0458, G_A_loss: 0.7213, G_B_loss: 0.3565\n",
      "Epoch [30/30], Step [191/1067], D_A_loss: 0.0853, D_B_loss: 0.0411, G_A_loss: 0.8250, G_B_loss: 1.0211\n",
      "Epoch [30/30], Step [201/1067], D_A_loss: 0.0462, D_B_loss: 0.0640, G_A_loss: 1.0081, G_B_loss: 0.6407\n",
      "Epoch [30/30], Step [211/1067], D_A_loss: 0.1532, D_B_loss: 0.0423, G_A_loss: 1.3011, G_B_loss: 0.4536\n",
      "Epoch [30/30], Step [221/1067], D_A_loss: 0.0870, D_B_loss: 0.1186, G_A_loss: 1.1415, G_B_loss: 0.2589\n",
      "Epoch [30/30], Step [231/1067], D_A_loss: 0.0510, D_B_loss: 0.0206, G_A_loss: 0.3081, G_B_loss: 0.7576\n",
      "Epoch [30/30], Step [241/1067], D_A_loss: 0.0642, D_B_loss: 0.0756, G_A_loss: 0.6008, G_B_loss: 0.7659\n",
      "Epoch [30/30], Step [251/1067], D_A_loss: 0.0602, D_B_loss: 0.0209, G_A_loss: 0.7720, G_B_loss: 0.5621\n",
      "Epoch [30/30], Step [261/1067], D_A_loss: 0.5167, D_B_loss: 0.2128, G_A_loss: 0.1833, G_B_loss: 0.9716\n",
      "Epoch [30/30], Step [271/1067], D_A_loss: 0.2257, D_B_loss: 0.0974, G_A_loss: 0.4105, G_B_loss: 0.7567\n",
      "Epoch [30/30], Step [281/1067], D_A_loss: 0.1759, D_B_loss: 0.0693, G_A_loss: 0.5357, G_B_loss: 0.2963\n",
      "Epoch [30/30], Step [291/1067], D_A_loss: 0.1169, D_B_loss: 0.1158, G_A_loss: 0.9574, G_B_loss: 0.3697\n",
      "Epoch [30/30], Step [301/1067], D_A_loss: 0.1029, D_B_loss: 0.0354, G_A_loss: 0.7722, G_B_loss: 0.2452\n",
      "Epoch [30/30], Step [311/1067], D_A_loss: 0.0883, D_B_loss: 0.0379, G_A_loss: 1.0679, G_B_loss: 0.4154\n",
      "Epoch [30/30], Step [321/1067], D_A_loss: 0.1772, D_B_loss: 0.0374, G_A_loss: 0.5323, G_B_loss: 0.3056\n",
      "Epoch [30/30], Step [331/1067], D_A_loss: 0.0841, D_B_loss: 0.0396, G_A_loss: 0.9897, G_B_loss: 0.4845\n",
      "Epoch [30/30], Step [341/1067], D_A_loss: 0.1193, D_B_loss: 0.0279, G_A_loss: 0.7705, G_B_loss: 0.3748\n",
      "Epoch [30/30], Step [351/1067], D_A_loss: 0.0797, D_B_loss: 0.0200, G_A_loss: 0.9415, G_B_loss: 0.3326\n",
      "Epoch [30/30], Step [361/1067], D_A_loss: 0.0646, D_B_loss: 0.0202, G_A_loss: 1.0512, G_B_loss: 0.6797\n",
      "Epoch [30/30], Step [371/1067], D_A_loss: 0.1165, D_B_loss: 0.1412, G_A_loss: 0.2941, G_B_loss: 0.4059\n",
      "Epoch [30/30], Step [381/1067], D_A_loss: 0.0360, D_B_loss: 0.0417, G_A_loss: 0.9489, G_B_loss: 0.7251\n",
      "Epoch [30/30], Step [391/1067], D_A_loss: 0.0530, D_B_loss: 0.2219, G_A_loss: 0.2483, G_B_loss: 0.3989\n",
      "Epoch [30/30], Step [401/1067], D_A_loss: 0.1424, D_B_loss: 0.0545, G_A_loss: 0.8249, G_B_loss: 0.4225\n",
      "Epoch [30/30], Step [411/1067], D_A_loss: 0.2393, D_B_loss: 0.0160, G_A_loss: 0.5105, G_B_loss: 0.2551\n",
      "Epoch [30/30], Step [421/1067], D_A_loss: 0.1878, D_B_loss: 0.0806, G_A_loss: 0.9485, G_B_loss: 0.7970\n",
      "Epoch [30/30], Step [431/1067], D_A_loss: 0.0337, D_B_loss: 0.0689, G_A_loss: 0.4611, G_B_loss: 0.4865\n",
      "Epoch [30/30], Step [441/1067], D_A_loss: 0.0639, D_B_loss: 0.1999, G_A_loss: 0.2329, G_B_loss: 0.3113\n",
      "Epoch [30/30], Step [451/1067], D_A_loss: 0.1736, D_B_loss: 0.0450, G_A_loss: 0.6330, G_B_loss: 0.6386\n",
      "Epoch [30/30], Step [461/1067], D_A_loss: 0.2571, D_B_loss: 0.2983, G_A_loss: 1.5426, G_B_loss: 0.2949\n",
      "Epoch [30/30], Step [471/1067], D_A_loss: 0.3796, D_B_loss: 0.1564, G_A_loss: 0.4602, G_B_loss: 0.0813\n",
      "Epoch [30/30], Step [481/1067], D_A_loss: 0.1525, D_B_loss: 0.0820, G_A_loss: 1.5552, G_B_loss: 0.2665\n",
      "Epoch [30/30], Step [491/1067], D_A_loss: 0.0674, D_B_loss: 0.0442, G_A_loss: 0.4100, G_B_loss: 0.5613\n",
      "Epoch [30/30], Step [501/1067], D_A_loss: 0.0407, D_B_loss: 0.0873, G_A_loss: 0.7981, G_B_loss: 0.6981\n",
      "Epoch [30/30], Step [511/1067], D_A_loss: 0.1099, D_B_loss: 0.2413, G_A_loss: 0.2365, G_B_loss: 0.3726\n",
      "Epoch [30/30], Step [521/1067], D_A_loss: 0.2254, D_B_loss: 0.0316, G_A_loss: 0.4376, G_B_loss: 0.4348\n",
      "Epoch [30/30], Step [531/1067], D_A_loss: 0.1349, D_B_loss: 0.0717, G_A_loss: 0.5064, G_B_loss: 0.3082\n",
      "Epoch [30/30], Step [541/1067], D_A_loss: 0.0295, D_B_loss: 0.1273, G_A_loss: 0.6331, G_B_loss: 0.2345\n",
      "Epoch [30/30], Step [551/1067], D_A_loss: 0.0906, D_B_loss: 0.0134, G_A_loss: 0.8846, G_B_loss: 0.1913\n",
      "Epoch [30/30], Step [561/1067], D_A_loss: 0.0717, D_B_loss: 0.0648, G_A_loss: 0.5702, G_B_loss: 0.4177\n",
      "Epoch [30/30], Step [571/1067], D_A_loss: 0.1869, D_B_loss: 0.0271, G_A_loss: 0.7650, G_B_loss: 0.2363\n",
      "Epoch [30/30], Step [581/1067], D_A_loss: 0.0752, D_B_loss: 0.0613, G_A_loss: 1.0353, G_B_loss: 0.1240\n",
      "Epoch [30/30], Step [591/1067], D_A_loss: 0.0401, D_B_loss: 0.0461, G_A_loss: 1.2812, G_B_loss: 0.8398\n",
      "Epoch [30/30], Step [601/1067], D_A_loss: 0.0752, D_B_loss: 0.0654, G_A_loss: 0.5925, G_B_loss: 0.4097\n",
      "Epoch [30/30], Step [611/1067], D_A_loss: 0.1629, D_B_loss: 0.0115, G_A_loss: 0.7422, G_B_loss: 0.9335\n",
      "Epoch [30/30], Step [621/1067], D_A_loss: 0.1473, D_B_loss: 0.0513, G_A_loss: 0.7489, G_B_loss: 0.8596\n",
      "Epoch [30/30], Step [631/1067], D_A_loss: 0.1221, D_B_loss: 0.0749, G_A_loss: 0.6945, G_B_loss: 0.4579\n",
      "Epoch [30/30], Step [641/1067], D_A_loss: 0.0222, D_B_loss: 0.1171, G_A_loss: 0.4494, G_B_loss: 0.8331\n",
      "Epoch [30/30], Step [651/1067], D_A_loss: 0.0849, D_B_loss: 0.0622, G_A_loss: 0.5327, G_B_loss: 0.0916\n",
      "Epoch [30/30], Step [661/1067], D_A_loss: 0.0342, D_B_loss: 0.1656, G_A_loss: 0.4656, G_B_loss: 0.1944\n",
      "Epoch [30/30], Step [671/1067], D_A_loss: 0.1308, D_B_loss: 0.0814, G_A_loss: 0.6100, G_B_loss: 0.9491\n",
      "Epoch [30/30], Step [681/1067], D_A_loss: 0.1246, D_B_loss: 0.0336, G_A_loss: 0.6235, G_B_loss: 0.6894\n",
      "Epoch [30/30], Step [691/1067], D_A_loss: 0.2095, D_B_loss: 0.0217, G_A_loss: 1.3720, G_B_loss: 0.7558\n",
      "Epoch [30/30], Step [701/1067], D_A_loss: 0.1430, D_B_loss: 0.1862, G_A_loss: 0.1868, G_B_loss: 0.3834\n",
      "Epoch [30/30], Step [711/1067], D_A_loss: 0.0273, D_B_loss: 0.0356, G_A_loss: 0.7028, G_B_loss: 0.7533\n",
      "Epoch [30/30], Step [721/1067], D_A_loss: 0.0717, D_B_loss: 0.2451, G_A_loss: 0.6690, G_B_loss: 0.1861\n",
      "Epoch [30/30], Step [731/1067], D_A_loss: 0.0191, D_B_loss: 0.0668, G_A_loss: 0.9562, G_B_loss: 0.7041\n",
      "Epoch [30/30], Step [741/1067], D_A_loss: 0.1487, D_B_loss: 0.0450, G_A_loss: 0.5647, G_B_loss: 0.2858\n",
      "Epoch [30/30], Step [751/1067], D_A_loss: 0.1372, D_B_loss: 0.1377, G_A_loss: 1.0759, G_B_loss: 0.3518\n",
      "Epoch [30/30], Step [761/1067], D_A_loss: 0.0420, D_B_loss: 0.0905, G_A_loss: 0.7013, G_B_loss: 0.9074\n",
      "Epoch [30/30], Step [771/1067], D_A_loss: 0.0275, D_B_loss: 0.0671, G_A_loss: 0.5246, G_B_loss: 0.2982\n",
      "Epoch [30/30], Step [781/1067], D_A_loss: 0.0411, D_B_loss: 0.0224, G_A_loss: 0.9494, G_B_loss: 0.5252\n",
      "Epoch [30/30], Step [791/1067], D_A_loss: 0.0952, D_B_loss: 0.2267, G_A_loss: 1.0944, G_B_loss: 0.4375\n",
      "Epoch [30/30], Step [801/1067], D_A_loss: 0.2391, D_B_loss: 0.0271, G_A_loss: 0.8721, G_B_loss: 0.2064\n",
      "Epoch [30/30], Step [811/1067], D_A_loss: 0.1659, D_B_loss: 0.0615, G_A_loss: 0.5555, G_B_loss: 0.8149\n",
      "Epoch [30/30], Step [821/1067], D_A_loss: 0.2315, D_B_loss: 0.0639, G_A_loss: 0.5348, G_B_loss: 0.2827\n",
      "Epoch [30/30], Step [831/1067], D_A_loss: 0.0753, D_B_loss: 0.0179, G_A_loss: 0.8906, G_B_loss: 0.4892\n",
      "Epoch [30/30], Step [841/1067], D_A_loss: 0.1246, D_B_loss: 0.1247, G_A_loss: 0.4017, G_B_loss: 0.1988\n",
      "Epoch [30/30], Step [851/1067], D_A_loss: 0.0520, D_B_loss: 0.0755, G_A_loss: 0.4186, G_B_loss: 0.4479\n",
      "Epoch [30/30], Step [861/1067], D_A_loss: 0.1317, D_B_loss: 0.1018, G_A_loss: 0.2472, G_B_loss: 0.4737\n",
      "Epoch [30/30], Step [871/1067], D_A_loss: 0.2031, D_B_loss: 0.0502, G_A_loss: 0.2520, G_B_loss: 0.2504\n",
      "Epoch [30/30], Step [881/1067], D_A_loss: 0.1076, D_B_loss: 0.0360, G_A_loss: 0.9949, G_B_loss: 0.2900\n",
      "Epoch [30/30], Step [891/1067], D_A_loss: 0.1121, D_B_loss: 0.0296, G_A_loss: 0.3183, G_B_loss: 0.3802\n",
      "Epoch [30/30], Step [901/1067], D_A_loss: 0.1259, D_B_loss: 0.0489, G_A_loss: 0.9588, G_B_loss: 0.3231\n",
      "Epoch [30/30], Step [911/1067], D_A_loss: 0.1482, D_B_loss: 0.0446, G_A_loss: 1.0466, G_B_loss: 0.7663\n",
      "Epoch [30/30], Step [921/1067], D_A_loss: 0.0424, D_B_loss: 0.0470, G_A_loss: 0.7118, G_B_loss: 0.6954\n",
      "Epoch [30/30], Step [931/1067], D_A_loss: 0.0592, D_B_loss: 0.1464, G_A_loss: 1.0199, G_B_loss: 0.6034\n",
      "Epoch [30/30], Step [941/1067], D_A_loss: 0.1051, D_B_loss: 0.0223, G_A_loss: 0.7392, G_B_loss: 0.4198\n",
      "Epoch [30/30], Step [951/1067], D_A_loss: 0.1873, D_B_loss: 0.0384, G_A_loss: 0.5992, G_B_loss: 0.4567\n",
      "Epoch [30/30], Step [961/1067], D_A_loss: 0.0427, D_B_loss: 0.0601, G_A_loss: 0.2821, G_B_loss: 0.5248\n",
      "Epoch [30/30], Step [971/1067], D_A_loss: 0.1851, D_B_loss: 0.0301, G_A_loss: 0.7089, G_B_loss: 0.5902\n",
      "Epoch [30/30], Step [981/1067], D_A_loss: 0.0581, D_B_loss: 0.0706, G_A_loss: 0.4626, G_B_loss: 0.6060\n",
      "Epoch [30/30], Step [991/1067], D_A_loss: 0.1287, D_B_loss: 0.0674, G_A_loss: 0.4656, G_B_loss: 0.9564\n",
      "Epoch [30/30], Step [1001/1067], D_A_loss: 0.0429, D_B_loss: 0.0194, G_A_loss: 0.6907, G_B_loss: 0.3458\n",
      "Epoch [30/30], Step [1011/1067], D_A_loss: 0.1584, D_B_loss: 0.0196, G_A_loss: 0.8089, G_B_loss: 0.5556\n",
      "Epoch [30/30], Step [1021/1067], D_A_loss: 0.2185, D_B_loss: 0.0179, G_A_loss: 0.6578, G_B_loss: 0.1821\n",
      "Epoch [30/30], Step [1031/1067], D_A_loss: 0.0329, D_B_loss: 0.0512, G_A_loss: 0.6206, G_B_loss: 0.6925\n",
      "Epoch [30/30], Step [1041/1067], D_A_loss: 0.0714, D_B_loss: 0.0239, G_A_loss: 1.2386, G_B_loss: 0.6041\n",
      "Epoch [30/30], Step [1051/1067], D_A_loss: 0.0380, D_B_loss: 0.0703, G_A_loss: 1.1507, G_B_loss: 0.9399\n",
      "Epoch [30/30], Step [1061/1067], D_A_loss: 0.0640, D_B_loss: 0.0550, G_A_loss: 0.9767, G_B_loss: 0.6208\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "for epoch in range(params.num_epochs):\n",
    "    D_A_losses = []\n",
    "    D_B_losses = []\n",
    "    G_A_losses = []\n",
    "    G_B_losses = []\n",
    "    cycle_A_losses = []\n",
    "    cycle_B_losses = []\n",
    "\n",
    "    # learning rate decay\n",
    "    if (epoch + 1) > params.decay_epoch:\n",
    "        D_A_optimizer.param_groups[0]['lr'] -= params.lrD / (params.num_epochs - params.decay_epoch)\n",
    "        D_B_optimizer.param_groups[0]['lr'] -= params.lrD / (params.num_epochs - params.decay_epoch)\n",
    "        G_optimizer.param_groups[0]['lr'] -= params.lrG / (params.num_epochs - params.decay_epoch)\n",
    "\n",
    "    # training\n",
    "    for i, (real_A, real_B) in enumerate(zip(train_data_loader_A, train_data_loader_B)):\n",
    "\n",
    "        # input image data\n",
    "        real_A = real_A.cuda()\n",
    "        real_B = real_B.cuda()\n",
    "\n",
    "        # ------------------------ Train generator G ----------------------------\n",
    "        # A -> B\n",
    "        fake_B = G_A(real_A)\n",
    "        D_B_fake_decision = D_B(fake_B)\n",
    "        G_A_loss = MSE_loss(D_B_fake_decision, torch.ones(D_B_fake_decision.size()).cuda())\n",
    "\n",
    "        # forward cycle loss\n",
    "        recon_A = G_B(fake_B)\n",
    "        cycle_A_loss = L1_loss(recon_A, real_A) * params.lambdaA\n",
    "\n",
    "        # B -> A\n",
    "        fake_A = G_B(real_B)\n",
    "        D_A_fake_decision = D_A(fake_A)\n",
    "        G_B_loss = MSE_loss(D_A_fake_decision, torch.ones(D_A_fake_decision.size()).cuda())\n",
    "\n",
    "        # backward cycle loss\n",
    "        recon_B = G_A(fake_A)\n",
    "        cycle_B_loss = L1_loss(recon_B, real_B) * params.lambdaB\n",
    "\n",
    "        # Back propagation\n",
    "        G_loss = G_A_loss + G_B_loss + cycle_A_loss + cycle_B_loss\n",
    "        G_optimizer.zero_grad()\n",
    "        G_loss.backward()\n",
    "        G_optimizer.step()\n",
    "        \n",
    "        # ------------------------ Train discriminator D_A ------------------------\n",
    "        D_A_real_decision = D_A(real_A)\n",
    "        D_A_real_loss = MSE_loss(D_A_real_decision, torch.ones(D_A_real_decision.size()).cuda())\n",
    "        \n",
    "        fake_A = fake_A_pool.query(fake_A)\n",
    "        \n",
    "        D_A_fake_decision = D_A(fake_A)\n",
    "        D_A_fake_loss = MSE_loss(D_A_fake_decision, torch.zeros(D_A_fake_decision.size()).cuda())\n",
    "\n",
    "        # Back propagation\n",
    "        D_A_loss = (D_A_real_loss + D_A_fake_loss) * 0.5\n",
    "        D_A_optimizer.zero_grad()\n",
    "        D_A_loss.backward()\n",
    "        D_A_optimizer.step()\n",
    "\n",
    "        # ------------------------ Train discriminator D_B ------------------------\n",
    "        D_B_real_decision = D_B(real_B)\n",
    "        D_B_real_loss = MSE_loss(D_B_real_decision, torch.ones(D_B_real_decision.size()).cuda())\n",
    "        fake_B = fake_B_pool.query(fake_B)\n",
    "        D_B_fake_decision = D_B(fake_B)\n",
    "        D_B_fake_loss = MSE_loss(D_B_fake_decision, torch.zeros(D_B_fake_decision.size()).cuda())\n",
    "\n",
    "        # Back propagation\n",
    "        D_B_loss = (D_B_real_loss + D_B_fake_loss) * 0.5\n",
    "        D_B_optimizer.zero_grad()\n",
    "        D_B_loss.backward()\n",
    "        D_B_optimizer.step()\n",
    "\n",
    "        # ------------------------ Print -----------------------------\n",
    "        # loss values\n",
    "        D_A_losses.append(D_A_loss.item())\n",
    "        D_B_losses.append(D_B_loss.item())\n",
    "        G_A_losses.append(G_A_loss.item())\n",
    "        G_B_losses.append(G_B_loss.item())\n",
    "        cycle_A_losses.append(cycle_A_loss.item())\n",
    "        cycle_B_losses.append(cycle_B_loss.item())\n",
    "\n",
    "        if i%10 == 0:\n",
    "            print('Epoch [%d/%d], Step [%d/%d], D_A_loss: %.4f, D_B_loss: %.4f, G_A_loss: %.4f, G_B_loss: %.4f'\n",
    "                  % (epoch+1, params.num_epochs, i+1, len(train_data_loader_A), D_A_loss.item(), D_B_loss.item(), G_A_loss.item(), G_B_loss.item()))\n",
    "\n",
    "        # ============ TensorBoard logging ============#\n",
    "        D_A_logger.scalar_summary('losses', D_A_loss.item(), step + 1)\n",
    "        D_B_logger.scalar_summary('losses', D_B_loss.item(), step + 1)\n",
    "        G_A_logger.scalar_summary('losses', G_A_loss.item(), step + 1)\n",
    "        G_B_logger.scalar_summary('losses', G_B_loss.item(), step + 1)\n",
    "        cycle_A_logger.scalar_summary('losses', cycle_A_loss.item(), step + 1)\n",
    "        cycle_B_logger.scalar_summary('losses', cycle_B_loss.item(), step + 1)\n",
    "        step += 1\n",
    "\n",
    "    D_A_avg_loss = torch.mean(torch.FloatTensor(D_A_losses))\n",
    "    D_B_avg_loss = torch.mean(torch.FloatTensor(D_B_losses))\n",
    "    G_A_avg_loss = torch.mean(torch.FloatTensor(G_A_losses))\n",
    "    G_B_avg_loss = torch.mean(torch.FloatTensor(G_B_losses))\n",
    "    cycle_A_avg_loss = torch.mean(torch.FloatTensor(cycle_A_losses))\n",
    "    cycle_B_avg_loss = torch.mean(torch.FloatTensor(cycle_B_losses))\n",
    "\n",
    "    # avg loss values for plot\n",
    "    D_A_avg_losses.append(D_A_avg_loss)\n",
    "    D_B_avg_losses.append(D_B_avg_loss)\n",
    "    G_A_avg_losses.append(G_A_avg_loss)\n",
    "    G_B_avg_losses.append(G_B_avg_loss)\n",
    "    cycle_A_avg_losses.append(cycle_A_avg_loss)\n",
    "    cycle_B_avg_losses.append(cycle_B_avg_loss)\n",
    "\n",
    "    # Show result for test image\n",
    "    test_real_A = test_real_A_data.cuda()\n",
    "    test_fake_B = G_A(test_real_A)\n",
    "    test_recon_A = G_B(test_fake_B)\n",
    "\n",
    "    test_real_B = test_real_B_data.cuda()\n",
    "    test_fake_A = G_B(test_real_B)\n",
    "    test_recon_B = G_A(test_fake_A)\n",
    "\n",
    "    utils.plot_train_result([test_real_A, test_real_B], [test_fake_B, test_fake_A], [test_recon_A, test_recon_B],\n",
    "                            epoch, save=True, save_dir=save_dir)\n",
    "\n",
    "    # log the images\n",
    "    result_AtoB = np.concatenate((utils.to_np(test_real_A), utils.to_np(test_fake_B), utils.to_np(test_recon_A)), axis=3)\n",
    "    result_BtoA = np.concatenate((utils.to_np(test_real_B), utils.to_np(test_fake_A), utils.to_np(test_recon_B)), axis=3)\n",
    "\n",
    "    info = { 'result_AtoB': result_AtoB.transpose(0, 2, 3, 1),  # convert to BxHxWxC\n",
    "             'result_BtoA': result_BtoA.transpose(0, 2, 3, 1) }\n",
    "\n",
    "    for tag, images in info.items():\n",
    "        img_logger.image_summary(tag, images, epoch + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot average losses\n",
    "avg_losses = []\n",
    "avg_losses.append(D_A_avg_losses)\n",
    "avg_losses.append(D_B_avg_losses)\n",
    "avg_losses.append(G_A_avg_losses)\n",
    "avg_losses.append(G_B_avg_losses)\n",
    "avg_losses.append(cycle_A_avg_losses)\n",
    "avg_losses.append(cycle_B_avg_losses)\n",
    "utils.plot_loss(avg_losses, params.num_epochs, save=True, save_dir=save_dir)\n",
    "\n",
    "# Make gif\n",
    "utils.make_gif(params.dataset, params.num_epochs, save_dir=save_dir)\n",
    "# Save trained parameters of model\n",
    "torch.save(G_A.state_dict(), model_dir + 'generator_A_param.pkl')\n",
    "torch.save(G_B.state_dict(), model_dir + 'generator_B_param.pkl')\n",
    "torch.save(D_A.state_dict(), model_dir + 'discriminator_A_param.pkl')\n",
    "torch.save(D_B.state_dict(), model_dir + 'discriminator_B_param.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
