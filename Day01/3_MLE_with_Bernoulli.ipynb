{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.distributions import normal\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = normal.Normal(4.0, 0.5)\n",
    "sample = m.sample((100,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.9052, 3.8018, 4.7267, 3.2874, 3.8692, 4.8902, 5.2769, 3.9898, 4.0905,\n",
       "        4.3270, 4.3348, 3.8455, 4.4163, 3.2587, 4.4511, 4.1145, 4.2852, 2.7311,\n",
       "        3.4649, 3.9961, 4.0940, 4.0473, 3.2010, 4.3445, 3.5395, 3.4755, 4.6836,\n",
       "        3.9888, 3.4280, 4.4842, 4.3357, 3.0576, 3.9690, 4.3254, 4.6122, 4.1205,\n",
       "        4.3750, 3.3612, 4.6894, 3.3088, 3.9086, 3.6418, 4.2421, 3.8228, 4.2386,\n",
       "        3.4394, 3.3265, 4.0155, 4.3897, 4.4121, 4.1101, 3.8423, 4.3691, 4.1557,\n",
       "        4.1370, 4.2630, 4.0064, 3.1609, 3.8725, 4.0313, 4.4246, 4.5667, 4.2082,\n",
       "        4.4227, 3.8969, 4.0618, 2.9900, 3.9165, 4.4664, 4.5635, 3.8917, 3.0974,\n",
       "        3.3490, 4.0303, 4.1494, 4.0518, 4.2698, 3.6119, 4.5849, 4.2388, 4.1246,\n",
       "        3.7975, 4.6549, 4.7445, 3.7502, 2.9897, 3.9085, 3.7969, 3.1440, 3.8353,\n",
       "        4.3073, 3.5629, 3.6275, 3.1284, 4.4866, 3.8777, 4.4852, 4.3004, 3.9777,\n",
       "        4.5596])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_norm_constant = -0.5 * np.log(2 * np.pi)\n",
    "\n",
    "def log_gaussian(x, mean=0, logvar=0.):\n",
    "  \"\"\"\n",
    "  Returns the density of x under the supplied gaussian. Defaults to\n",
    "  standard gaussian N(0, I)\n",
    "  :param x: (*) torch.Tensor\n",
    "  :param mean: float or torch.FloatTensor with dimensions (*)\n",
    "  :param logvar: float or torch.FloatTensor with dimensions (*)\n",
    "  :return: (*) elementwise log density\n",
    "  \"\"\"\n",
    "  if type(logvar) == 'float':\n",
    "      logvar = x.new(1).fill_(logvar)\n",
    "\n",
    "  a = (x - mean) ** 2\n",
    "  log_p = -0.5 * (logvar + a / logvar.exp())\n",
    "  log_p = log_p + log_norm_constant\n",
    "\n",
    "  return log_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_likelihoods(X, mu, logvar, log=True):\n",
    "  \"\"\"\n",
    "  :param X: design matrix (examples, features)\n",
    "  :param mu: the component means (K, features)\n",
    "  :param logvar: the component log-variances (K, features)\n",
    "  :param log: return value in log domain?\n",
    "      Note: exponentiating can be unstable in high dimensions.\n",
    "  :return likelihoods: (K, examples)\n",
    "  \"\"\"\n",
    "\n",
    "  # get feature-wise log-likelihoods (K, examples, features)\n",
    "  log_likelihoods = log_gaussian(\n",
    "      X[None, :, :], # (1, examples, features)\n",
    "      mu[:, None, :], # (K, 1, features)\n",
    "      logvar[:, None, :] # (K, 1, features)\n",
    "  )\n",
    "\n",
    "  # sum over the feature dimension\n",
    "  log_likelihoods = log_likelihoods.sum(-1)\n",
    "\n",
    "  if not log:\n",
    "      log_likelihoods.exp_()\n",
    "\n",
    "  return log_likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.00002\n",
    "\n",
    "for t in range(1000):\n",
    "    NLL = gte_likelihoods(X,)\n",
    "    NLL.backward()\n",
    "\n",
    "    if t % 100 == 0:\n",
    "        print(\"loglikelihood=\", NLL.data.numpy(), \"p=\", p.data.numpy(), \"dL/dp= \", p.grad.data.numpy())\n",
    "\n",
    "    \n",
    "    p.data -= learning_rate * p.grad.data\n",
    "    p.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood for Bernoulli with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that we have 100 samples from a Bernoulli distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "sample = np.array([ 1.,  1.,  0.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
    "        1.,  0.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
    "        1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
    "        1.,  1.,  1.,  0.,  1.,  1.,  1.,  0.,  1.,  1.,  0.,  1.,  1.,\n",
    "        1.,  0.,  1.,  0.,  1.,  1.,  0.,  1.,  1.,  0.,  1.,  1.,  1.,\n",
    "        1.,  1.,  1.,  1.,  0.,  0.,  1.,  1.,  1.,  1.,  0.,  0.,  1.,\n",
    "        0.,  1.,  1.,  1.,  1.,  0.,  0.,  1.,  1.,  1.,  1.,  0.,  1.,\n",
    "        0.,  1.,  1.,  0.,  1.,  0.,  1.,  0.,  0.,  1.,  1.,  1.,  0.,\n",
    "        0.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,\n",
    "        0.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,  0.,  0.,  1.,  1.,  1.,\n",
    "        0.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  0.,  0.,  1.,  1.,  1.,\n",
    "        1.,  1.,  0.,  0.,  1.,  1.,  0.,  1.,  1.,  0.,  1.,  0.,  1.,\n",
    "        1.,  1.,  1.,  0.,  1.,  0.,  1.,  1.,  1.,  1.,  0.,  0.,  1.,\n",
    "        0.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  0.,  1.,\n",
    "        1.,  1.,  1.,  0.,  1.,  1.,  1.,  0.,  0.,  0.,  1.,  1.,  1.,\n",
    "        1.,  0.,  1.,  0.,  1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.725"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define the probability p of generating 1, and put the sample into a PyTorch Variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.from_numpy(sample).float()\n",
    "p = torch.rand(1).float().requires_grad_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to learn the model using maximum likelihood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loglikelihood= 133.97046 p= [0.527544] dL/dp=  [-158.44556]\n",
      "loglikelihood= 118.19812 p= [0.6906524] dL/dp=  [-32.15291]\n",
      "loglikelihood= 117.64539 p= [0.72016996] dL/dp=  [-4.7935333]\n",
      "loglikelihood= 117.63397 p= [0.72435737] dL/dp=  [-0.64375305]\n",
      "loglikelihood= 117.63374 p= [0.72491515] dL/dp=  [-0.08511353]\n",
      "loglikelihood= 117.63378 p= [0.7249888] dL/dp=  [-0.01123047]\n",
      "loglikelihood= 117.63376 p= [0.7249986] dL/dp=  [-0.00141907]\n",
      "loglikelihood= 117.63376 p= [0.7249986] dL/dp=  [-0.00141907]\n",
      "loglikelihood= 117.63376 p= [0.7249986] dL/dp=  [-0.00141907]\n",
      "loglikelihood= 117.63376 p= [0.7249986] dL/dp=  [-0.00141907]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.00002\n",
    "for t in range(1000):\n",
    "    NLL = -torch.sum(torch.log(x*p + (1-x)*(1-p)) )\n",
    "    NLL.backward()\n",
    "\n",
    "    if t % 100 == 0:\n",
    "        print(\"loglikelihood=\", NLL.data.numpy(), \"p=\", p.data.numpy(), \"dL/dp= \", p.grad.data.numpy())\n",
    "\n",
    "    \n",
    "    p.data -= learning_rate * p.grad.data\n",
    "    p.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
