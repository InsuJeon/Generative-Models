{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ea492c38abcc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'..'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'notebooks'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'data'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib inline\n",
    "import os\n",
    "from IPython import display\n",
    "os.chdir('..')\n",
    "import data\n",
    "os.chdir('notebooks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "#shutil.rmtree('octa/pred_mean//')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 128\n",
    "batch = 128\n",
    "max_step = 50001\n",
    "summary_step = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._net = nn.Sequential(\n",
    "            nn.Linear(z_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.mean_fc = nn.Linear(128, 2)\n",
    "        self.var_fc = nn.Linear(128, 2)\n",
    "    \n",
    "    def forward(self, z):\n",
    "        h = self._net(z)\n",
    "        return self.mean_fc(h), self.var_fc(h)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._net = nn.Sequential(\n",
    "            nn.Linear(2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self._net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data():\n",
    "    num_data = 50000\n",
    "    noise_std = 0.1\n",
    "    radius = 1\n",
    "    rad = np.random.choice([\n",
    "        0, np.pi / 4, np.pi / 2, 3 * np.pi / 4,\n",
    "        np.pi, 5 * np.pi / 4, 3 * np.pi / 2, 7 * np.pi / 4\n",
    "    ], [num_data])\n",
    "    data = np.stack([radius * np.cos(rad), radius * np.sin(rad)], axis=1)\n",
    "    noise = np.random.normal(0, noise_std, data.shape)\n",
    "    return data + noise\n",
    "    \n",
    "raw_data = generate_data()\n",
    "dataset = torch.utils.data.TensorDataset(torch.Tensor(raw_data))\n",
    "dataset.vector_preprocess = lambda x: x\n",
    "#data_iterator = data.DataIterator(dataset, batch_size=batch, sampler=data.InfiniteRandomSampler(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.12963152, -1.1354435 ],\n",
       "       [-0.0690444 ,  0.87052675],\n",
       "       [-0.05483682,  1.04465169],\n",
       "       ...,\n",
       "       [ 0.87101206, -0.1739673 ],\n",
       "       [-0.7568259 ,  0.72366955],\n",
       "       [ 1.09713994,  0.14930306]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vis_limit = 2.0\n",
    "\n",
    "def visualize_samples(samples, save_path=None):\n",
    "    fig = plt.figure(figsize=(2,2), dpi=100)\n",
    "    plt.plot(samples[:,0], samples[:,1], 'b.', markersize=1, alpha=0.2)\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([-vis_limit, vis_limit])\n",
    "    axes.set_ylim([-vis_limit, vis_limit])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()\n",
    "    \n",
    "    if save_path:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        fig.savefig(save_path, bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "    return fig\n",
    "\n",
    "grid_size = 64\n",
    "grid = np.tile(np.reshape(np.linspace(-vis_limit, vis_limit, grid_size), (grid_size, 1)), (1, grid_size))\n",
    "coord = np.stack([np.transpose(grid), grid], axis=2)\n",
    "coord = coord[::-1, :]\n",
    "coord = np.reshape(coord, [grid_size ** 2, 2])\n",
    "coord = torch.Tensor(coord).to(device)\n",
    "\n",
    "def visualize_discriminator(d):\n",
    "    with torch.no_grad():\n",
    "        verdicts = d(coord).cpu().numpy()\n",
    "    verdicts = np.reshape(verdicts, [grid_size, grid_size])\n",
    "    fig = plt.figure(figsize=(2,2), dpi=100)\n",
    "    plt.imshow(verdicts)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "fig = visualize_samples(raw_data[:1000], save_path='octa/data.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_iterator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-949525873247>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary_step\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Train D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     z = torch.normal(torch.zeros((x.size(0), z_dim)),\n\u001b[1;32m     16\u001b[0m                      torch.ones((x.size(0), z_dim))).to(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_iterator' is not defined"
     ]
    }
   ],
   "source": [
    "G = Generator().to(device)\n",
    "D = Discriminator().to(device)\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=0.0001)\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=0.0001)\n",
    "criterion = nn.BCELoss()\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "summary_step = 10\n",
    "\n",
    "# Train\n",
    "training_loss_sum = 0.0\n",
    "for step in range(summary_step * 200):\n",
    "    # Train D\n",
    "    x = next(data_iterator).to(device)\n",
    "    z = torch.normal(torch.zeros((x.size(0), z_dim)),\n",
    "                     torch.ones((x.size(0), z_dim))).to(device)\n",
    "\n",
    "    D.zero_grad()\n",
    "    label = torch.full((x.size(0),), real_label, device=device)\n",
    "    fake, _ = G(z)\n",
    "    real_loss = criterion(D(x), label)\n",
    "    real_loss.backward()\n",
    "\n",
    "    label.fill_(fake_label)\n",
    "    fake_loss = criterion(D(fake.detach()), label)\n",
    "    fake_loss.backward()\n",
    "    D_optimizer.step()\n",
    "\n",
    "    # Train G\n",
    "    G.zero_grad()\n",
    "    label.fill_(real_label)\n",
    "    G_loss = criterion(D(fake), label)\n",
    "    G_loss.backward()\n",
    "    G_optimizer.step()\n",
    "\n",
    "    # Summary\n",
    "    if step % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            z = torch.normal(torch.zeros((1000, z_dim)), 1.0).to(device)\n",
    "            samples, _ = G(z)\n",
    "            samples = samples.cpu().numpy()\n",
    "        display.clear_output(wait=True)\n",
    "        print(\"Step %d\" % step)\n",
    "        fig = visualize_samples(samples, 'octa/gan/%07d.png' % step)\n",
    "\n",
    "        # Visualize D's view\n",
    "        visualize_discriminator(D)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Gaussian Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_gaussian = Generator().to(device)\n",
    "P_optimizer = torch.optim.Adam(P_gaussian.parameters(), lr=0.0001)\n",
    "mse_loss = nn.MSELoss()\n",
    "summary_step = 50  # for MM\n",
    "summary_step = 5\n",
    "\n",
    "# Train\n",
    "training_loss_sum = 0.0\n",
    "for step in range(summary_step * 200):\n",
    "    # Train G\n",
    "    x = next(data_iterator).to(device)\n",
    "    z = torch.normal(torch.zeros((x.size(0), z_dim)),\n",
    "                     torch.ones((x.size(0), z_dim))).to(device)\n",
    "    P_gaussian.zero_grad()\n",
    "    mean, log_var = P_gaussian(z)\n",
    "    P_loss = ((mean - x) ** 2) / (2 * torch.exp(log_var)) + log_var / 2\n",
    "    P_loss.sum(dim=1).mean(dim=0).backward()\n",
    "    P_optimizer.step()\n",
    "\n",
    "    # Summary\n",
    "    if step % summary_step == 0:\n",
    "        with torch.no_grad():\n",
    "            z = torch.normal(torch.zeros((1000, z_dim)), 1.0).to(device)\n",
    "            mean, log_var = P_gaussian(z)\n",
    "            samples = torch.normal(mean.repeat(2, 1), log_var.exp().sqrt().repeat(2, 1))\n",
    "            mean = mean.cpu().numpy()\n",
    "            samples = samples.cpu().numpy()\n",
    "        display.clear_output(wait=True)\n",
    "        print(\"Step %d\" % step)\n",
    "        fig = visualize_samples(mean, 'octa/gaussian_pred_mean/%07d.png' % step)\n",
    "        fig = visualize_samples(samples, 'octa/gaussian_pred/%07d.png' % step)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Laplace Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_laplace = Generator().to(device)\n",
    "P_optimizer = torch.optim.Adam(P_laplace.parameters(), lr=0.0001)\n",
    "mse_loss = nn.MSELoss()\n",
    "summary_step = 50  # for MM\n",
    "summary_step = 5\n",
    "\n",
    "# Train\n",
    "training_loss_sum = 0.0\n",
    "for step in range(summary_step * 200):\n",
    "    # Train G\n",
    "    x = next(data_iterator).to(device)\n",
    "    z = torch.normal(torch.zeros((x.size(0), z_dim)),\n",
    "                     torch.ones((x.size(0), z_dim))).to(device)\n",
    "    P_laplace.zero_grad()\n",
    "    mean, log_var = P_laplace(z)\n",
    "    P_loss = (mean - x).abs() / torch.exp(log_var) + log_var\n",
    "    P_loss.sum(dim=1).mean(dim=0).backward()\n",
    "    P_optimizer.step()\n",
    "\n",
    "    # Summary\n",
    "    if step % summary_step == 0:\n",
    "        with torch.no_grad():\n",
    "            z = torch.normal(torch.zeros((1000, z_dim)), 1.0).to(device)\n",
    "            mean, log_var = P_laplace(z)\n",
    "            laplace = torch.distributions.laplace.Laplace(mean.repeat(2, 1), log_var.exp().repeat(2, 1))\n",
    "            samples = laplace.sample()\n",
    "            mean = mean.cpu().numpy()\n",
    "            samples = samples.cpu().numpy()\n",
    "        display.clear_output(wait=True)\n",
    "        print(\"Step %d\" % step)\n",
    "        fig = visualize_samples(mean, 'octa/laplace_pred_median/%07d.png' % step)\n",
    "        fig = visualize_samples(samples, 'octa/laplace_pred/%07d.png' % step)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train GAN with L2 Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Generator().to(device)\n",
    "D = Discriminator().to(device)\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=0.0001)\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=0.0001)\n",
    "criterion = nn.BCELoss()\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "summary_step = 10\n",
    "\n",
    "# Train\n",
    "training_loss_sum = 0.0\n",
    "for step in range(summary_step * 200):\n",
    "    # Train D\n",
    "    x = next(data_iterator).to(device)\n",
    "    z = torch.normal(torch.zeros((x.size(0), z_dim)),\n",
    "                     torch.ones((x.size(0), z_dim))).to(device)\n",
    "\n",
    "    D.zero_grad()\n",
    "    label = torch.full((x.size(0),), real_label, device=device)\n",
    "    fake, _ = G(z)\n",
    "    real_loss = criterion(D(x), label)\n",
    "    real_loss.backward()\n",
    "\n",
    "    label.fill_(fake_label)\n",
    "    fake_loss = criterion(D(fake.detach()), label)\n",
    "    fake_loss.backward()\n",
    "    D_optimizer.step()\n",
    "\n",
    "    # Train G\n",
    "    G.zero_grad()\n",
    "    label.fill_(real_label)\n",
    "    G_loss = criterion(D(fake), label)\n",
    "    l2_loss = mse_loss(fake, x)\n",
    "    (G_loss + l2_loss).backward()\n",
    "    G_optimizer.step()\n",
    "\n",
    "    # Summary\n",
    "    if step % summary_step == 0:\n",
    "        with torch.no_grad():\n",
    "            z = torch.normal(torch.zeros((1000, z_dim)), 1.0).to(device)\n",
    "            samples, _ = G(z)\n",
    "            samples = samples.cpu().numpy()\n",
    "        display.clear_output(wait=True)\n",
    "        print(\"[Step %d] L2 Loss: %.3f | GAN Loss: %.3f\" % (step, l2_loss, G_loss))\n",
    "        fig = visualize_samples(samples, 'octa/gan+l2/%07d.png' % step)\n",
    "        fig = visualize_discriminator(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train GAN with Gaussian Moment Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_harmony(method, dist, order):\n",
    "    G = Generator().to(device)\n",
    "    D = Discriminator().to(device)\n",
    "    G_optimizer = torch.optim.Adam(G.parameters(), lr=0.0001)\n",
    "    D_optimizer = torch.optim.Adam(D.parameters(), lr=0.0001)\n",
    "    criterion = nn.BCELoss()\n",
    "    real_label = 1\n",
    "    fake_label = 0\n",
    "    summary_step = 75\n",
    "\n",
    "    # Train\n",
    "    training_loss_sum = 0.0\n",
    "    for step in range(summary_step * 200):\n",
    "        # Train D\n",
    "        x = next(data_iterator).to(device)\n",
    "        z = torch.normal(torch.zeros((x.size(0), z_dim)),\n",
    "                         torch.ones((x.size(0), z_dim))).to(device)\n",
    "\n",
    "        D.zero_grad()\n",
    "        label = torch.full((x.size(0),), real_label, device=device)\n",
    "        fake, _ = G(z)\n",
    "        real_loss = criterion(D(x), label)\n",
    "        real_loss.backward()\n",
    "\n",
    "        label.fill_(fake_label)\n",
    "        fake_loss = criterion(D(fake.detach()), label)\n",
    "        fake_loss.backward()\n",
    "        D_optimizer.step()\n",
    "\n",
    "        # Train G\n",
    "        with torch.no_grad():\n",
    "            p_mean, p_log_var = P(z)\n",
    "            p_var = torch.exp(p_log_var)\n",
    "        G.zero_grad()\n",
    "        label.fill_(real_label)\n",
    "        G_loss = criterion(D(fake), label)\n",
    "        gen_mean = fake.mean(dim=0)\n",
    "        gen_var = fake.std(dim=0) ** 2\n",
    "        mm_loss = (p_mean[0] - gen_mean) ** 2 + (p_var[0] - gen_var) ** 2\n",
    "        mm_loss = mm_loss.mean(dim=0)\n",
    "        (G_loss + mm_loss).backward()\n",
    "        G_optimizer.step()\n",
    "\n",
    "        # Summary\n",
    "        if step % summary_step == 0:\n",
    "            with torch.no_grad():\n",
    "                z = torch.normal(torch.zeros((1000, z_dim)), 1.0).to(device)\n",
    "                samples, _ = G(z)\n",
    "                samples = samples.cpu().numpy()\n",
    "            display.clear_output(wait=True)\n",
    "            print(\"[Step %d] MM Loss: %.3f | GAN Loss: %.3f\" % (step, mm_loss, G_loss))\n",
    "            fig = visualize_samples(samples, 'octa/mm/%07d.png' % step)\n",
    "            fig = visualize_discriminator(D)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train GAN with L1 Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_iterator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-dcccf3325d8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Train D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     z = torch.normal(torch.zeros((x.size(0), z_dim)),\n\u001b[1;32m     14\u001b[0m                      torch.ones((x.size(0), z_dim))).to(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_iterator' is not defined"
     ]
    }
   ],
   "source": [
    "G = Generator().to(device)\n",
    "D = Discriminator().to(device)\n",
    "G_optimizer = torch.optim.SGD(G.parameters(), lr=0.001)\n",
    "D_optimizer = torch.optim.SGD(D.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "# Train\n",
    "training_loss_sum = 0.0\n",
    "for step in range(max_step):\n",
    "    # Train D\n",
    "    x = next(data_iterator).to(device)\n",
    "    z = torch.normal(torch.zeros((x.size(0), z_dim)),\n",
    "                     torch.ones((x.size(0), z_dim))).to(device)\n",
    "\n",
    "    D.zero_grad()\n",
    "    label = torch.full((x.size(0),), real_label, device=device)\n",
    "    fake, _ = G(z)\n",
    "    real_loss = criterion(D(x), label)\n",
    "    real_loss.backward()\n",
    "\n",
    "    label.fill_(fake_label)\n",
    "    fake_loss = criterion(D(fake.detach()), label)\n",
    "    fake_loss.backward()\n",
    "    D_optimizer.step()\n",
    "\n",
    "    # Train G\n",
    "    G.zero_grad()\n",
    "    label.fill_(real_label)\n",
    "    G_loss = criterion(D(fake), label)\n",
    "    l1_loss = (fake - x).abs().mean()\n",
    "    (G_loss + l1_loss).backward()\n",
    "    G_optimizer.step()\n",
    "\n",
    "    # Summary\n",
    "    if step % summary_step == 0:\n",
    "        with torch.no_grad():\n",
    "            z = torch.normal(torch.zeros((1000, z_dim)), 1.0).to(device)\n",
    "            samples, _ = G(z)\n",
    "            samples = samples.cpu().numpy()\n",
    "        display.clear_output(wait=True)\n",
    "        print(\"[Step %d] L1 Loss: %.3f | GAN Loss: %.3f\" % (step, l1_loss, G_loss))\n",
    "        os.makedirs('notebooks/toy_data_summary/gan+l1', exist_ok=True)\n",
    "        fig = visualize_samples(samples)\n",
    "        fig.savefig('notebooks/toy_data_summary/gan+l1/%07d.pdf' % step, bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "        # Visualize D's view\n",
    "        fig = visualize_discriminator(D)\n",
    "        fig.savefig('notebooks/toy_data_summary/gan+l1/d_%07d.pdf' % step, bbox_inches='tight', pad_inches=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Laplace Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_iterator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-c13199198052>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Train G\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     z = torch.normal(torch.zeros((x.size(0), z_dim)),\n\u001b[1;32m     10\u001b[0m                      torch.ones((x.size(0), z_dim))).to(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_iterator' is not defined"
     ]
    }
   ],
   "source": [
    "P_laplace = Generator().to(device)\n",
    "P_optimizer = torch.optim.SGD(P_laplace.parameters(), lr=0.001)\n",
    "mse_loss = nn.MSELoss()\n",
    "# Train\n",
    "training_loss_sum = 0.0\n",
    "for step in range(max_step):\n",
    "    # Train G\n",
    "    x = next(data_iterator).to(device)\n",
    "    z = torch.normal(torch.zeros((x.size(0), z_dim)),\n",
    "                     torch.ones((x.size(0), z_dim))).to(device)\n",
    "    P_laplace.zero_grad()\n",
    "    mean, log_var = P_laplace(z)\n",
    "    P_loss = (mean - x).abs() / torch.exp(log_var) + log_var\n",
    "    P_loss.sum(dim=1).mean(dim=0).backward()\n",
    "    P_optimizer.step()\n",
    "\n",
    "    # Summary\n",
    "    if step % summary_step == 0:\n",
    "        with torch.no_grad():\n",
    "            z = torch.normal(torch.zeros((1000, z_dim)), 1.0).to(device)\n",
    "            mean, log_var = P_laplace(z)\n",
    "            dist = torch.distributions.Laplace(mean, log_var.exp())\n",
    "            samples = dist.sample()\n",
    "            mean = mean.cpu().numpy()\n",
    "            samples = samples.cpu().numpy()\n",
    "        display.clear_output(wait=True)\n",
    "        print(\"Step %d\" % step)\n",
    "        os.makedirs('notebooks/toy_data_summary/predictor_laplace_median', exist_ok=True)\n",
    "        fig = visualize_samples(mean)\n",
    "        fig.savefig('notebooks/toy_data_summary/predictor_laplace_median/%07d.pdf' % step, bbox_inches='tight', pad_inches=0)\n",
    "        fig = visualize_samples(samples)\n",
    "        fig.savefig('notebooks/toy_data_summary/predictor_laplace_sample/%07d.pdf' % step, bbox_inches='tight', pad_inches=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train GAN with Laplace Moment Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_iterator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-749a5ce1d334>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_step\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Train D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     z = torch.normal(torch.zeros((x.size(0), z_dim)),\n\u001b[1;32m     15\u001b[0m                      torch.ones((x.size(0), z_dim))).to(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_iterator' is not defined"
     ]
    }
   ],
   "source": [
    "G = Generator().to(device)\n",
    "D = Discriminator().to(device)\n",
    "G_optimizer = torch.optim.SGD(G.parameters(), lr=0.001)\n",
    "D_optimizer = torch.optim.SGD(D.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# Train\n",
    "training_loss_sum = 0.0\n",
    "for step in range(max_step * 5):\n",
    "    # Train D\n",
    "    x = next(data_iterator).to(device)\n",
    "    z = torch.normal(torch.zeros((x.size(0), z_dim)),\n",
    "                     torch.ones((x.size(0), z_dim))).to(device)\n",
    "\n",
    "    D.zero_grad()\n",
    "    label = torch.full((x.size(0),), real_label, device=device)\n",
    "    fake, _ = G(z)\n",
    "    real_loss = criterion(D(x), label)\n",
    "    real_loss.backward()\n",
    "\n",
    "    label.fill_(fake_label)\n",
    "    fake_loss = criterion(D(fake.detach()), label)\n",
    "    fake_loss.backward()\n",
    "    D_optimizer.step()\n",
    "\n",
    "    # Train G\n",
    "    with torch.no_grad():\n",
    "        p_mean, p_log_var = P_laplace(z)\n",
    "        p_var = torch.exp(p_log_var)\n",
    "    G.zero_grad()\n",
    "    label.fill_(real_label)\n",
    "    G_loss = criterion(D(fake), label)\n",
    "    gen_median = fake.median(dim=0, keepdim=True)[0]\n",
    "    gen_mad = (fake - gen_median).abs().mean(0)\n",
    "    gen_median = gen_median.squeeze(0)\n",
    "    mm_loss = (p_mean[0] - gen_median) ** 2 + (p_var[0] - gen_mad) ** 2\n",
    "    mm_loss = mm_loss.mean(dim=0)\n",
    "    (G_loss + mm_loss).backward()\n",
    "    G_optimizer.step()\n",
    "\n",
    "    # Summary\n",
    "    if step % summary_step == 0:\n",
    "        with torch.no_grad():\n",
    "            z = torch.normal(torch.zeros((1000, z_dim)), 1.0).to(device)\n",
    "            samples, _ = G(z)\n",
    "            samples = samples.cpu().numpy()\n",
    "        display.clear_output(wait=True)\n",
    "        print(\"[Step %d] MM Loss: %.3f | GAN Loss: %.3f\" % (step, mm_loss, G_loss))\n",
    "        fig = visualize_samples(samples)\n",
    "        os.makedirs('notebooks/toy_data_summary/gan+laplace_mm', exist_ok=True)\n",
    "        fig.savefig('notebooks/toy_data_summary/gan+laplace_mm/%07d.pdf' % step, bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "        # Visualize D's view\n",
    "        fig = visualize_discriminator(D)\n",
    "        fig.savefig('notebooks/toy_data_summary/gan+laplace_mm/d_%07d.pdf' % step, bbox_inches='tight', pad_inches=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
